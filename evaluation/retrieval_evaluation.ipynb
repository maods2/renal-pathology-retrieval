{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity,euclidean_distances\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"..\")\n",
    "QUERIES = 47\n",
    "K_LIST = [1 , 5, 10, 15, 30, 50]\n",
    "PRECISION = 2  # number of decimal digits of precision\n",
    "MODELS = ['autoenconder', 'triplet', 'swav', 'semant_att', 'semant_autoenconder', 'semant_triplet', 'semant_swav']\n",
    "METRICS = ['mAP@', 'mAR@', 'mAF1@']\n",
    "DISTANCES = ['euclidian', 'cosine_similarity']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(pickle_file_path):\n",
    "    with open(pickle_file_path, 'rb') as pickle_file:\n",
    "        loaded_data_dict = pickle.load(pickle_file)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    data = loaded_data_dict[\"embedding\"]\n",
    "    labels = np.array(loaded_data_dict[\"target\"])\n",
    "    return data, labels, loaded_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],

   "source": [
    "# Dados fictÃ­cios\n",
    "\n",
    "def run_queries(query_embedding, data_embeddings, similarity_function):\n",
    "    similarity_scores = similarity_function(query_embedding.reshape(1, -1), data_embeddings).flatten()\n",
    "    sorted_indices = np.argsort(similarity_scores)\n",
    "    return sorted_indices, similarity_scores\n",
    "\n",
    "\n",
    "def calculate_ap_at_k(relevant_labels, k_value):\n",
    "    ap_num = 0\n",
    "    tp = np.sum(relevant_labels[:k_value])\n",
    "    for k in range(1, k_value + 1):\n",
    "        tp_at_k = np.sum(relevant_labels[:k])\n",
    "        precision_at_k =  tp_at_k / k\n",
    "        # calculate numerator value for ap\n",
    "        ap_num += precision_at_k * relevant_labels[k - 1]\n",
    "        # print(f\"P@{k+1}_{i+1} = {round(precision_at_k,2)}\")\n",
    "\n",
    "    ap_q = ap_num / tp if tp > 0 else 0\n",
    "    return ap_q\n",
    "\n",
    "def calculate_ar_at_k(relevant_labels, k_value, positives):\n",
    "    ar_num = 0\n",
    "    tp = np.sum(relevant_labels[:k_value])\n",
    "    for k in range(1, k_value + 1):\n",
    "        tp_at_k = np.sum(relevant_labels[:k])\n",
    "        recall_at_k = tp_at_k / positives if positives > 0 else 0\n",
    "        \n",
    "        ar_num += recall_at_k * relevant_labels[k - 1]\n",
    "\n",
    "    ar_q = ar_num / tp if tp > 0 else 0\n",
    "    return ar_q\n",
    "\n",
    "def calculate_f1_at_k(relevant_labels, k_value, positives):\n",
    "    f1_num = 0\n",
    "    tp = np.sum(relevant_labels[:k_value])\n",
    "    for k in range(1, k_value + 1):\n",
    "        tp_at_k = np.sum(relevant_labels[:k])\n",
    "        precision_at_k =  tp_at_k / k\n",
    "        recall_at_k = tp_at_k / positives if positives > 0 else 0\n",
    "        f1_score_at_k = 2 * (precision_at_k * recall_at_k) / (precision_at_k + recall_at_k) \\\n",
    "            if (precision_at_k + recall_at_k) > 0 else 0\n",
    "        \n",
    "        f1_num += f1_score_at_k * relevant_labels[k - 1]\n",
    "\n",
    "    af1_q = f1_num / tp if tp > 0 else 0\n",
    "    return af1_q\n",
    "\n",
    "def evaluate_retrieval_pipeline(\n",
    "        data_embeddings,\n",
    "        data_labels, \n",
    "        query_embeddings, \n",
    "        query_labels, \n",
    "        k,\n",
    "        similarity_function\n",
    "        ):\n",
    "    \n",
    "    ap_at_k_list = []\n",
    "    ar_at_k_list = []\n",
    "    af1_at_k_list = []\n",
    "    for q in range(len(query_embeddings)):\n",
    "        query_label = query_labels[q] \n",
    "        sorted_indices, _ = run_queries(query_embeddings[q], data_embeddings, similarity_function)\n",
    "        sorted_labels = data_labels[sorted_indices]\n",
    "        # [1,0,1,1,1,0] relevant label = 1\n",
    "        relevant_labels = sorted_labels == np.full((len(sorted_labels),), query_label)\n",
    "        total_positives = np.sum(data_labels == query_label)\n",
    "        average_precision = calculate_ap_at_k(relevant_labels, k)\n",
    "        average_recall = calculate_ar_at_k(relevant_labels, k, positives=total_positives)\n",
    "        average_f1score = calculate_f1_at_k(relevant_labels, k, positives=total_positives)\n",
    "        ap_at_k_list.append(average_precision)\n",
    "        ar_at_k_list.append(average_recall)\n",
    "        af1_at_k_list.append(average_f1score)\n",
    "\n",
    "\n",
    "    map_at_k = sum(ap_at_k_list) / len(query_embeddings)\n",
    "    mar_at_k = sum(ar_at_k_list) / len(query_embeddings)\n",
    "    maf1_at_k = sum(af1_at_k_list) / len(query_embeddings)\n",
    "    \n",
    "    print(f\"mAP@{k} = {round(map_at_k, PRECISION)}\")\n",
    "    print(f\"mAR@{k} = {round(mar_at_k, PRECISION)}\")\n",
    "    print(f\"mAF1@{k} = {round(maf1_at_k, PRECISION)}\")\n",
    "\n",
    "    return round(map_at_k, PRECISION), round(mar_at_k, PRECISION), round(maf1_at_k, PRECISION)\n",
    "\n",
    "def dot_product(query_embedding: np.ndarray, data_embeddings: np.ndarray) -> np.ndarray:\n",
    "    distances = np.dot(data_embeddings, np.transpose(query_embedding)).flatten()\n",
    "    return distances\n",
    "\n",
    "def euclidian(query_embedding, data_embeddings):\n",
    "    distances = euclidean_distances(query_embedding, data_embeddings).flatten()\n",
    "    return distances\n",
    "\n",
    "def cosine_similarity_func(query_embedding, data_embeddings):\n",
    "    similarity = cosine_similarity(query_embedding, data_embeddings).flatten()\n",
    "    distances = 1 - similarity\n",
    "    return distances\n",
    "\n",
    "\n",
    "def normalize_with_L2_norm(vector):\n",
    "    \"\"\"\n",
    "    Function to normalize a vector using the L2 (Euclidean) norm.\n",
    "\n",
    "    Parameters:\n",
    "        vector (numpy.ndarray): The vector to be normalized.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The normalized vector.\n",
    "    \"\"\"\n",
    "    norm = np.linalg.norm(vector)\n",
    "    normalized_vector = vector / norm\n",
    "    return normalized_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = DATA_DIR / 'efficientnetb0_4096_autoencoder_test.pickle'\n",
    "path_train = DATA_DIR / 'efficientnetb0_4096_autoencoder_train.pickle'\n",
    "encod_data_test, encod_labels_test, _ = load_embeddings(path_test)\n",
    "encod_data_train, encod_labels_train, _ = load_embeddings(path_train)\n",
    "\n",
    "path_test = DATA_DIR / 'efficientnetb0_4096_pretrained_test.pickle'\n",
    "path_train = DATA_DIR / 'efficientnetb0_4096_pretrained_train.pickle'\n",
    "triplet_data_test, triplet_labels_test, _ = load_embeddings(path_test)\n",
    "triplet_data_train, triplet_labels_train, _ = load_embeddings(path_train)\n",
    "\n",
    "path_test = DATA_DIR / 'efficientnet_SwaV_test.pickle'\n",
    "path_train = DATA_DIR / 'efficientnet_SwaV_train.pickle'\n",
    "swav_data_test, swav_labels_test, _ = load_embeddings(path_test)\n",
    "swav_data_train, swav_labels_train, _ = load_embeddings(path_train)\n",
    "\n",
    "path_test = DATA_DIR / 'semantic_test.pickle'\n",
    "path_train = DATA_DIR / 'semantic_train.pickle'\n",
    "semant_data_test, semant_labels_test, _ = load_embeddings(path_test)\n",
    "semant_data_train, semant_labels_train, _ = load_embeddings(path_train)\n",
    "\n",
    "#############################################################\n",
    "\n",
    "path_test = DATA_DIR / 'semantic_att_efficientnetb0_encoder_test.pickle'\n",
    "path_train = DATA_DIR / 'semantic_att_efficientnetb0_encoder_train.pickle'\n",
    "semant_encod_data_test, semant_encod_labels_test, _ = load_embeddings(path_test)\n",
    "semant_encod_data_train, semant_encod_labels_train, _ = load_embeddings(path_train)\n",
    "\n",
    "path_test = DATA_DIR / 'semantic_att_efficientnetb0_test.pickle'\n",
    "path_train = DATA_DIR / 'semantic_att_efficientnetb0_train.pickle'\n",
    "semant_triplet_data_test, semant_triplet_labels_test, _ = load_embeddings(path_test)\n",
    "semant_triplet_data_train, semant_triplet_labels_train, _ = load_embeddings(path_train)\n",
    "\n",
    "path_test = DATA_DIR / 'semantic_att_efficientnet_SwaV_test.pickle'\n",
    "path_train = DATA_DIR / 'semantic_att_efficientnet_SwaV_train.pickle'\n",
    "semant_swav_data_test, semant_swav_labels_test, _ = load_embeddings(path_test)\n",
    "semant_swav_data_train, semant_swav_labels_train, _ = load_embeddings(path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encod_data_train = normalize_with_L2_norm(encod_data_train)\n",
    "encod_data_test = normalize_with_L2_norm(encod_data_test)\n",
    "\n",
    "triplet_data_train = normalize_with_L2_norm(triplet_data_train)\n",
    "triplet_data_test = normalize_with_L2_norm(triplet_data_test)\n",
    "\n",
    "swav_data_train = normalize_with_L2_norm(swav_data_train)\n",
    "swav_data_test = normalize_with_L2_norm(swav_data_test)\n",
    "\n",
    "semant_data_train = normalize_with_L2_norm(semant_data_train)\n",
    "semant_data_test = normalize_with_L2_norm(semant_data_test)\n",
    "\n",
    "\n",
    "semant_encod_data_train = normalize_with_L2_norm(semant_encod_data_train)\n",
    "semant_encod_data_test = normalize_with_L2_norm(semant_encod_data_test)\n",
    "\n",
    "semant_triplet_data_train = normalize_with_L2_norm(semant_triplet_data_train)\n",
    "semant_triplet_data_test = normalize_with_L2_norm(semant_triplet_data_test)\n",
    "\n",
    "semant_swav_data_train = normalize_with_L2_norm(semant_swav_data_train)\n",
    "semant_swav_data_test = normalize_with_L2_norm(semant_swav_data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_bundle = [\n",
    "    ('autoenconder', encod_data_train, encod_labels_train, encod_data_test, encod_labels_test),\n",
    "    ('triplet', triplet_data_train, triplet_labels_train, triplet_data_test, triplet_labels_test),\n",
    "    ('swav', swav_data_train, swav_labels_train, swav_data_test, swav_labels_test),\n",
    "    ('semant_att', semant_data_train, semant_labels_train, semant_data_test, semant_labels_test),\n",
    "    \n",
    "    ('semant_autoenconder', semant_encod_data_train, semant_encod_labels_train, semant_encod_data_test, semant_encod_labels_test),\n",
    "    ('semant_triplet', semant_triplet_data_train, semant_triplet_labels_train, semant_triplet_data_test, semant_triplet_labels_test),\n",
    "    ('semant_swav', semant_swav_data_train, semant_swav_labels_train, semant_swav_data_test, semant_swav_labels_test),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encod_labels_test.shape)\n",
    "print(triplet_labels_test.shape)\n",
    "print(swav_labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_labels=semant_labels_train\n",
    "query_embeddings = semant_data_train\n",
    "data_embeddings = semant_data_train\n",
    "data_labels = semant_labels_train\n",
    "similarity_function=euclidian \n",
    "k = 50\n",
    "q = 5009\n",
    "\n",
    "query_label = query_labels[q] \n",
    "sorted_indices, _ = run_queries(query_embeddings[q], data_embeddings, similarity_function)\n",
    "sorted_true_labels = data_labels[sorted_indices]\n",
    "# [1,0,1,1,1,0] relevant label = 1\n",
    "relevant_labels = sorted_true_labels == np.full(len(sorted_true_labels), query_label)\n",
    "average_precision = calculate_ap_at_k(relevant_labels, k)\n",
    "average_precision\n",
    "\n",
    "# dot_product(encod_data_train[0].reshape(1, -1), encod_data_train).flatten()\n",
    "# similarity_function(query_embedding.reshape(1, -1), data_embeddings).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {\n",
    "        \"model\":[],\n",
    "        \"distance\":[],\n",
    "        \"mAP@\":[],\n",
    "        \"mAR@\":[],\n",
    "        \"mAF1@\":[],\n",
    "        \"k\":[],\n",
    "        }\n",
    "\n",
    "\n",
    "for m_name, x_train, y_train, x_test, y_test in emb_bundle:\n",
    "        print(f\"For embedding '{m_name}'\".center(80, '-'))\n",
    "        for k in K_LIST:\n",
    "                print(\"\\nCosine Similarity \")\n",
    "                map, mar, maf1 = evaluate_retrieval_pipeline(\n",
    "                        data_embeddings=x_train,\n",
    "                        data_labels=y_train, \n",
    "                        query_embeddings=x_test[:QUERIES], \n",
    "                        query_labels=y_test[:QUERIES], \n",
    "                        k=k,\n",
    "                        similarity_function=cosine_similarity_func\n",
    "                        )\n",
    "                res[\"model\"].append(m_name)\n",
    "                res[\"distance\"].append(\"cosine similarity\")\n",
    "                res[\"mAP@\"].append(map)\n",
    "                res[\"mAR@\"].append(mar)\n",
    "                res[\"mAF1@\"].append(maf1)\n",
    "                res[\"k\"].append(k)\n",
    "\n",
    "                print(\"\\nEuclidian Distance \")\n",
    "                map, mar, maf1 = evaluate_retrieval_pipeline(\n",
    "                        data_embeddings=x_train,\n",
    "                        data_labels=y_train, \n",
    "                        query_embeddings=x_test[:QUERIES], \n",
    "                        query_labels=y_test[:QUERIES], \n",
    "                        k=k,\n",
    "                        similarity_function=euclidian\n",
    "                )\n",
    "                res[\"model\"].append(m_name)\n",
    "                res[\"distance\"].append(\"euclidian\")\n",
    "                res[\"mAP@\"].append(map)\n",
    "                res[\"mAR@\"].append(mar)\n",
    "                res[\"mAF1@\"].append(maf1)\n",
    "                res[\"k\"].append(k)\n",
    "                print('-'*80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(res)\n",
    "df = df.sort_values(by=['model','distance','k','mAP@'], ascending=[False,False,True,False]).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# function from https://stackoverflow.com/questions/14270391/how-to-plot-multiple-bars-grouped\n",
    "# (ADAPTED)\n",
    "def bar_plot(ax: plt.Axes, \n",
    "             data: pd.DataFrame, \n",
    "             metric: str, \n",
    "             group_names: list[str],\n",
    "             xlabel: str | None = None,\n",
    "             xticks: list | None = None,\n",
    "             colors=None, \n",
    "             total_width=0.8, \n",
    "             single_width=1, \n",
    "             legend=True):\n",
    "    \"\"\"Draws a bar plot with multiple bars per data point.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : matplotlib.pyplot.axis\n",
    "        The axis we want to draw our plot on.\n",
    "\n",
    "    data: pandas.DataFrame\n",
    "\n",
    "    metric: metric to be considered\n",
    "        will plot the data found in column metric.\n",
    "\n",
    "    group_names: names for each element of the group\n",
    "        group_names[i] is the name on the legend for the i-th element of each group \n",
    "\n",
    "    colors : array-like, optional\n",
    "        A list of colors which are used for the bars. If None, the colors\n",
    "        will be the standard matplotlib color cyle. (default: None)\n",
    "\n",
    "    total_width : float, optional, default: 0.8\n",
    "        The width of a bar group. 0.8 means that 80% of the x-axis is covered\n",
    "        by bars and 20% will be spaces between the bars.\n",
    "\n",
    "    single_width: float, optional, default: 1\n",
    "        The relative width of a single bar within a group. 1 means the bars\n",
    "        will touch eachother within a group, values less than 1 will make\n",
    "        these bars thinner.\n",
    "\n",
    "    legend: bool, optional, default: True\n",
    "        If this is set to true, a legend will be added to the axis.\n",
    "    \"\"\"\n",
    "    # we want to build a dict-like object\n",
    "    # such that d[model] = [metric@k1 for model, metric@k2 for model, ...]\n",
    "\n",
    "    # Check if colors where provided, otherwhise use the default color cycle\n",
    "    if colors is None:\n",
    "        colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "    # Number of bars per group\n",
    "    n_bars = len(group_names)\n",
    "    \n",
    "    # The width of a single bar\n",
    "    bar_width = total_width / n_bars\n",
    "    bar_handles = []\n",
    "\n",
    "    # Iterate over all data\n",
    "    for i, group_name in enumerate(group_names):\n",
    "        # The offset in x direction of that bar\n",
    "        x_offset = (i - n_bars / 2) * bar_width + bar_width / 2\n",
    "        group_data = data[data['model'] == group_name]\n",
    "\n",
    "        # Draw a bar for every value of that type\n",
    "        for x, (value, k) in enumerate(zip(group_data[metric], group_data['k'])):\n",
    "            bar = ax.bar(x + x_offset, value, width=bar_width * single_width, color=colors[i % len(colors)])\n",
    "            bar.set_label(k)\n",
    "            if x + 1 == len(group_data[metric]):\n",
    "                bar_handles.append(bar[0])\n",
    "    \n",
    "    if legend:\n",
    "        ax.legend(bar_handles, group_names)\n",
    "\n",
    "    if xticks is not None:\n",
    "        ax.set_xticks(range(len(xticks)), xticks)\n",
    "        \n",
    "    ax.set_yticks(np.arange(0.0, 1.1, 0.1))\n",
    "\n",
    "    if xlabel is not None:\n",
    "        ax.set_xlabel(xlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric(distance: str, title: str):\n",
    "    fig, axs = plt.subplots(3, figsize=(10, 10))\n",
    "    for i, metric in enumerate(METRICS):\n",
    "        data = df[df['distance'] == distance]\n",
    "        bar_plot(axs[i], data, metric, \n",
    "                group_names=MODELS,\n",
    "                xticks=K_LIST,\n",
    "                xlabel='k', \n",
    "                total_width=0.8)\n",
    "\n",
    "        axs[i].set_title(metric)\n",
    "        axs[i].set_xlabel('k')\n",
    "        axs[i].set_ylabel(metric)\n",
    "        axs[i].set_ylim(0.0, 1.0)\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(\"euclidian\", \"Euclidean distances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric('cosine similarity', \"Cosine distances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.model=='semant_att']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('query_same_dataset.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "renal-pathology-retrieval-P_udDvkW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
