{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity,euclidean_distances\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_embeddings(pickle_file_path):\n",
    "    with open(pickle_file_path, 'rb') as pickle_file:\n",
    "        loaded_data_dict = pickle.load(pickle_file)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    data = loaded_data_dict[\"embedding\"]\n",
    "    labels = np.array(loaded_data_dict[\"target\"])\n",
    "    return data, labels, loaded_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados fictÃ­cios\n",
    "\n",
    "def run_queries(query_embedding, data_embeddings, similarity_function):\n",
    "    similarity_scores = similarity_function(query_embedding.reshape(1, -1), data_embeddings).flatten()\n",
    "    sorted_indices = np.argsort(similarity_scores)\n",
    "    return sorted_indices, similarity_scores\n",
    "\n",
    "\n",
    "def calculate_ap_at_k(relevant_labels, k_value):\n",
    "    ap_num = 0\n",
    "    tp = np.sum(relevant_labels[:k_value])\n",
    "    for k in range(1, k_value + 1):\n",
    "        tp_at_k = np.sum(relevant_labels[:k])\n",
    "        precision_at_k =  tp_at_k / k\n",
    "        # calculate numerator value for ap\n",
    "        ap_num += precision_at_k * relevant_labels[k - 1]\n",
    "        # print(f\"P@{k+1}_{i+1} = {round(precision_at_k,2)}\")\n",
    "\n",
    "    ap_q = ap_num / tp\n",
    "\n",
    "    return ap_q\n",
    "\n",
    "def calculate_ar_at_k(relevant_labels, k_value, positives):\n",
    "    ar_num = 0\n",
    "    tp = np.sum(relevant_labels[:k_value])\n",
    "    for k in range(1, k_value + 1):\n",
    "        tp_at_k = np.sum(relevant_labels[:k])\n",
    "        recall_at_k = tp_at_k / positives if positives > 0 else 0\n",
    "        \n",
    "        ar_num += recall_at_k * relevant_labels[k - 1]\n",
    "\n",
    "    ar_q = ar_num / tp    \n",
    "    return ar_q\n",
    "\n",
    "def calculate_f1_at_k(relevant_labels, k_value, positives):\n",
    "    f1_num = 0\n",
    "    tp = np.sum(relevant_labels[:k_value])\n",
    "    for k in range(1, k_value + 1):\n",
    "        tp_at_k = np.sum(relevant_labels[:k])\n",
    "        precision_at_k =  tp_at_k / k\n",
    "        recall_at_k = tp_at_k / positives if positives > 0 else 0\n",
    "        f1_score_at_k = 2 * (precision_at_k * recall_at_k) / (precision_at_k + recall_at_k) \\\n",
    "            if (precision_at_k + recall_at_k) > 0 else 0\n",
    "        \n",
    "        f1_num += f1_score_at_k * relevant_labels[k - 1]\n",
    "\n",
    "    af1_q = f1_num / tp    \n",
    "    return af1_q\n",
    "\n",
    "def evaluate_retrieval_pipeline(\n",
    "        data_embeddings,\n",
    "        data_labels, \n",
    "        query_embeddings, \n",
    "        query_labels, \n",
    "        k,\n",
    "        similarity_function\n",
    "        ):\n",
    "    \n",
    "    ap_at_k_list = []\n",
    "    ar_at_k_list = []\n",
    "    af1_at_k_list = []\n",
    "    for q in range(len(query_embeddings)):\n",
    "        query_label = query_labels[q] \n",
    "        sorted_indices, _ = run_queries(query_embeddings[q], data_embeddings, similarity_function)\n",
    "        sorted_labels = data_labels[sorted_indices]\n",
    "        # [1,0,1,1,1,0] relevant label = 1\n",
    "        relevant_labels = sorted_labels == np.full((len(sorted_labels),), query_label)\n",
    "        total_positives = np.sum(data_labels == query_label)\n",
    "        average_precision = calculate_ap_at_k(relevant_labels, k)\n",
    "        average_recall = calculate_ar_at_k(relevant_labels, k, positives=total_positives)\n",
    "        average_f1score = calculate_f1_at_k(relevant_labels, k, positives=total_positives)\n",
    "        ap_at_k_list.append(average_precision)\n",
    "        ar_at_k_list.append(average_recall)\n",
    "        af1_at_k_list.append(average_f1score)\n",
    "\n",
    "\n",
    "    map_at_k = sum(ap_at_k_list) / len(query_embeddings)\n",
    "    mar_at_k = sum(ar_at_k_list) / len(query_embeddings)\n",
    "    maf1_at_k = sum(af1_at_k_list) / len(query_embeddings)\n",
    "    \n",
    "    print(f\"mAP@{k} = {round(map_at_k, 2)}\")\n",
    "    print(f\"mAR@{k} = {round(mar_at_k, 2)}\")\n",
    "    print(f\"mAF1@{k} = {round(maf1_at_k, 2)}\")\n",
    "\n",
    "    return round(map_at_k, 2), round(mar_at_k, 2), round(maf1_at_k, 2)\n",
    "\n",
    "def dot_product(query_embedding: np.ndarray, data_embeddings: np.ndarray) -> np.ndarray:\n",
    "    distances = np.dot(data_embeddings, np.transpose(query_embedding)).flatten()\n",
    "    return distances\n",
    "\n",
    "def euclidian(query_embedding, data_embeddings):\n",
    "    distances = euclidean_distances(query_embedding, data_embeddings).flatten()\n",
    "    return distances\n",
    "\n",
    "def cosine_similarity_func(query_embedding, data_embeddings):\n",
    "    similarity = cosine_similarity(query_embedding, data_embeddings).flatten()\n",
    "    distances = 1 - similarity\n",
    "    return distances\n",
    "\n",
    "\n",
    "def normalize_with_L2_norm(vector):\n",
    "    \"\"\"\n",
    "    Function to normalize a vector using the L2 (Euclidean) norm.\n",
    "\n",
    "    Parameters:\n",
    "        vector (numpy.ndarray): The vector to be normalized.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The normalized vector.\n",
    "    \"\"\"\n",
    "    norm = np.linalg.norm(vector)\n",
    "    normalized_vector = vector / norm\n",
    "    return normalized_vector\n",
    "\n",
    "# Example of using the function\n",
    "vector = np.array([3, 4])\n",
    "normalize_with_L2_norm(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = './efficientnetb0_4096_autoencoder_test.pickle'\n",
    "path_train = './efficientnetb0_4096_autoencoder_train.pickle'\n",
    "encod_data_test, encod_labels_test, _ = load_embeddings(path_test)\n",
    "encod_data_train, encod_labels_train, _ = load_embeddings(path_train)\n",
    "\n",
    "path_test = './efficientnetb0_4096_pretrained_test.pickle'\n",
    "path_train = './efficientnetb0_4096_pretrained_train.pickle'\n",
    "triplet_data_test, triplet_labels_test, _ = load_embeddings(path_test)\n",
    "triplet_data_train, triplet_labels_train, _ = load_embeddings(path_train)\n",
    "\n",
    "path_test = 'efficientnet_SwaV_test.pickle'\n",
    "path_train = './efficientnet_SwaV_train.pickle'\n",
    "swav_data_test, swav_labels_test, _ = load_embeddings(path_test)\n",
    "swav_data_train, swav_labels_train, _ = load_embeddings(path_train)\n",
    "\n",
    "path_test = '../semantic/semantic_test.pickle'\n",
    "path_train = '../semantic/semantic_train.pickle'\n",
    "semant_data_test, semant_labels_test, _ = load_embeddings(path_test)\n",
    "semant_data_train, semant_labels_train, _ = load_embeddings(path_train)\n",
    "\n",
    "#############################################################\n",
    "\n",
    "path_test = '../semantic/semantic_att_efficientnetb0_encoder_test.pickle'\n",
    "path_train = '../semantic/semantic_att_efficientnetb0_encoder_train.pickle'\n",
    "semant_encod_data_test, semant_encod_labels_test, _ = load_embeddings(path_test)\n",
    "semant_encod_data_train, semant_encod_labels_train, _ = load_embeddings(path_train)\n",
    "\n",
    "path_test = '../semantic/semantic_att_efficientnetb0_test.pickle'\n",
    "path_train = '../semantic/semantic_att_efficientnetb0_train.pickle'\n",
    "semant_triplet_data_test, semant_triplet_labels_test, _ = load_embeddings(path_test)\n",
    "semant_triplet_data_train, semant_triplet_labels_train, _ = load_embeddings(path_train)\n",
    "\n",
    "path_test = '../semantic/semantic_att_efficientnet_SwaV_test.pickle'\n",
    "path_train = '../semantic/semantic_att_efficientnet_SwaV_train.pickle'\n",
    "semant_swav_data_test, semant_swav_labels_test, _ = load_embeddings(path_test)\n",
    "semant_swav_data_train, semant_swav_labels_train, _ = load_embeddings(path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encod_data_train = normalize_with_L2_norm(encod_data_train)\n",
    "encod_data_test = normalize_with_L2_norm(encod_data_test)\n",
    "\n",
    "triplet_data_train = normalize_with_L2_norm(triplet_data_train)\n",
    "triplet_data_test = normalize_with_L2_norm(triplet_data_test)\n",
    "\n",
    "swav_data_train = normalize_with_L2_norm(swav_data_train)\n",
    "swav_data_test = normalize_with_L2_norm(swav_data_test)\n",
    "\n",
    "semant_data_train = normalize_with_L2_norm(semant_data_train)\n",
    "semant_data_test = normalize_with_L2_norm(semant_data_test)\n",
    "\n",
    "\n",
    "semant_encod_data_train = normalize_with_L2_norm(semant_encod_data_train)\n",
    "semant_encod_data_test = normalize_with_L2_norm(semant_encod_data_test)\n",
    "\n",
    "semant_triplet_data_train = normalize_with_L2_norm(semant_triplet_data_train)\n",
    "semant_triplet_data_test = normalize_with_L2_norm(semant_triplet_data_test)\n",
    "\n",
    "semant_swav_data_train = normalize_with_L2_norm(semant_swav_data_train)\n",
    "semant_swav_data_test = normalize_with_L2_norm(semant_swav_data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_bundle = [\n",
    "    ('autoenconder', encod_data_train, encod_labels_train, encod_data_test, encod_labels_test),\n",
    "    ('triplet', triplet_data_train, triplet_labels_train, triplet_data_test, triplet_labels_test),\n",
    "    ('swav', swav_data_train, swav_labels_train, swav_data_test, swav_labels_test),\n",
    "    ('semant_att', semant_data_train, semant_labels_train, semant_data_test, semant_labels_test),\n",
    "    \n",
    "    ('semant_autoenconder', semant_encod_data_train, semant_encod_labels_train, semant_encod_data_test, semant_encod_labels_test),\n",
    "    ('semant_triplet', semant_triplet_data_train, semant_triplet_labels_train, semant_triplet_data_test, semant_triplet_labels_test),\n",
    "    ('semant_swav', semant_swav_data_train, semant_swav_labels_train, semant_swav_data_test, semant_swav_labels_test),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encod_labels_test.shape)\n",
    "print(triplet_labels_test.shape)\n",
    "print(swav_labels_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_labels=semant_labels_train\n",
    "query_embeddings = semant_data_train\n",
    "data_embeddings = semant_data_train\n",
    "data_labels = semant_labels_train\n",
    "similarity_function=euclidian \n",
    "k = 50\n",
    "q = 5009\n",
    "\n",
    "query_label = query_labels[q] \n",
    "sorted_indices, _ = run_queries(query_embeddings[q], data_embeddings, similarity_function)\n",
    "sorted_true_labels = data_labels[sorted_indices]\n",
    "# [1,0,1,1,1,0] relevant label = 1\n",
    "relevant_labels = sorted_true_labels == np.full(len(sorted_true_labels), query_label)\n",
    "average_precision = calculate_ap_at_k(relevant_labels, k)\n",
    "average_precision\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dot_product(encod_data_train[0].reshape(1, -1), encod_data_train).flatten()\n",
    "# similarity_function(query_embedding.reshape(1, -1), data_embeddings).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_true_labels[:50]\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "queries = 47\n",
    "k_list = [1 , 5, 10, 15, 30, 50]\n",
    "\n",
    "res = {\n",
    "        \"model\":[],\n",
    "        \"distance\":[],\n",
    "        \"mAP@\":[],\n",
    "        \"mAR@\":[],\n",
    "        \"mAF1@\":[],\n",
    "        \"k\":[],\n",
    "        }\n",
    "\n",
    "\n",
    "for m_name, x_train, y_train, x_test, y_test in emb_bundle:\n",
    "\n",
    "        \n",
    "        for k in k_list:\n",
    "                print(\"\\nCosine Similarity \")\n",
    "                map, mar, maf1 = evaluate_retrieval_pipeline(\n",
    "                        data_embeddings=x_train,\n",
    "                        data_labels=y_train, \n",
    "                        query_embeddings=x_test[:queries], \n",
    "                        query_labels=y_test[:queries], \n",
    "                        k=k,\n",
    "                        similarity_function=cosine_similarity_func\n",
    "                        )\n",
    "                res[\"model\"].append(m_name)\n",
    "                res[\"distance\"].append(\"cosine similarity\")\n",
    "                res[\"mAP@\"].append(map)\n",
    "                res[\"mAR@\"].append(mar)\n",
    "                res[\"mAF1@\"].append(maf1)\n",
    "                res[\"k\"].append(k)\n",
    "\n",
    "                print(\"\\nEuclidian Distance \")\n",
    "                map, mar, maf1 = evaluate_retrieval_pipeline(\n",
    "                        data_embeddings=x_train,\n",
    "                        data_labels=y_train, \n",
    "                        query_embeddings=x_test[:queries], \n",
    "                        query_labels=y_test[:queries], \n",
    "                        k=k,\n",
    "                        similarity_function=euclidian\n",
    "                )\n",
    "                res[\"model\"].append(m_name)\n",
    "                res[\"distance\"].append(\"euclidian\")\n",
    "                res[\"mAP@\"].append(map)\n",
    "                res[\"mAR@\"].append(mar)\n",
    "                res[\"mAF1@\"].append(maf1)\n",
    "                res[\"k\"].append(k)\n",
    "\n",
    "\n",
    "                \n",
    "                print('-'*30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(res)\n",
    "df = df.sort_values(by=['model','distance','k','mAP@'], ascending=[False,False,True,False]).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Plot\n",
    "fig, axs = plt.subplots(3, figsize=(10, 10))\n",
    "\n",
    "metrics = ['mAP@', 'mAR@', 'mAF1@']\n",
    "colors = ['blue', 'green', 'orange']\n",
    "df_ = df[df['distance'] == 'euclidian']\n",
    "for i, metric in enumerate(metrics):\n",
    "    for model in df_['model'].unique():\n",
    "        df_filtered = df_[df_['model'] == model]\n",
    "        axs[i].plot(df_filtered['k'], df_filtered[metric], marker='o', label=model)\n",
    "\n",
    "    axs[i].set_title(metric)\n",
    "    axs[i].set_xlabel('k')\n",
    "    axs[i].set_ylabel(metric)\n",
    "    axs[i].legend()\n",
    "\n",
    "plt.suptitle(\"Euclidian Distance\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Plot\n",
    "fig, axs = plt.subplots(3, figsize=(10, 10))\n",
    "\n",
    "metrics = ['mAP@', 'mAR@', 'mAF1@']\n",
    "colors = ['blue', 'green', 'orange']\n",
    "df_ = df[df['distance'] == 'euclidian']\n",
    "for i, metric in enumerate(metrics):\n",
    "    for model in df_['model'].unique():\n",
    "        df_filtered = df_[df_['model'] == model]\n",
    "        axs[i].plot(df_filtered['k'], df_filtered[metric], marker='o', label=model)\n",
    "\n",
    "    axs[i].set_title(metric)\n",
    "    axs[i].set_xlabel('k')\n",
    "    axs[i].set_ylabel(metric)\n",
    "    axs[i].legend()\n",
    "\n",
    "plt.suptitle(\"cosine similarity\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.model=='semant_att']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('query_same_dataset.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "renal-pathology-retrieval-P_udDvkW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
