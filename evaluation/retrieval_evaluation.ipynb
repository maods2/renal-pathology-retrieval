{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rótulos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consulta 1 - Precisão: 0.4, Recall: 0.08163265306122448\n",
      "Consulta 2 - Precisão: 0.3, Recall: 0.05263157894736842\n",
      "Consulta 3 - Precisão: 0.4, Recall: 0.10256410256410256\n",
      "Consulta 4 - Precisão: 0.3, Recall: 0.061224489795918366\n",
      "Consulta 5 - Precisão: 0.3, Recall: 0.06382978723404255\n",
      "Consulta 6 - Precisão: 0.3, Recall: 0.06521739130434782\n",
      "Consulta 7 - Precisão: 0.4, Recall: 0.08333333333333333\n",
      "Consulta 8 - Precisão: 0.5, Recall: 0.1\n",
      "Consulta 9 - Precisão: 0.3, Recall: 0.06521739130434782\n",
      "Consulta 10 - Precisão: 0.1, Recall: 0.022222222222222223\n",
      "Média de Precisão: 0.32999999999999996\n",
      "Média de Recall: 0.06978729497669076\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Suponha que você tenha embeddings de consulta e embeddings de imagens de referência\n",
    "# Substitua esses dados com seus próprios embeddings\n",
    "embeddings_de_consulta = np.random.rand(10, 256)  # 10 embeddings de consulta de exemplo (256 dimensões)\n",
    "embeddings_de_referencia = np.random.rand(100, 256)  # 100 embeddings de referência de exemplo (256 dimensões)\n",
    "\n",
    "# Suponha que você tenha rótulos de relevância para cada consulta\n",
    "# Substitua esses dados com seus próprios rótulos de relevância\n",
    "rótulos_de_relevância = np.random.randint(2, size=(10, 100))  # Matriz de relevância (0 ou 1)\n",
    "\n",
    "# Número de imagens a serem recuperadas para cada consulta\n",
    "k = 10\n",
    "\n",
    "# Recuperação de Imagens\n",
    "imagens_recuperadas = []\n",
    "for embedding_de_consulta, rótulos in zip(embeddings_de_consulta, rótulos_de_relevância):\n",
    "    # Calcula a similaridade entre o embedding de consulta e todos os embeddings de referência\n",
    "    similaridades = np.dot(embeddings_de_referencia, embedding_de_consulta)\n",
    "\n",
    "    # Classifica os índices das imagens de referência com base na similaridade\n",
    "    índices_ordenados = np.argsort(similaridades)[::-1]\n",
    "\n",
    "    # Recupera as top-k imagens mais similares\n",
    "    top_k_imagens = índices_ordenados[:k]\n",
    "\n",
    "    imagens_recuperadas.append(top_k_imagens)\n",
    "\n",
    "# Validando os Resultados\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "for i, (imagens_relevantes, rótulos) in enumerate(zip(imagens_recuperadas, rótulos_de_relevância)):\n",
    "    # Calcula as métricas de precisão e recall para cada consulta\n",
    "    precision = precision_score(rótulos, [1 if j in imagens_relevantes else 0 for j in range(len(embeddings_de_referencia))])\n",
    "    recall = recall_score(rótulos, [1 if j in imagens_relevantes else 0 for j in range(len(embeddings_de_referencia))])\n",
    "\n",
    "    print(f\"Consulta {i+1} - Precisão: {precision}, Recall: {recall}\")\n",
    "    \n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "\n",
    "# Calcula a média das métricas de precisão e recall para todas as consultas\n",
    "mean_precision = np.mean(precision_scores)\n",
    "mean_recall = np.mean(recall_scores)\n",
    "\n",
    "print(f\"Média de Precisão: {mean_precision}\")\n",
    "print(f\"Média de Recall: {mean_recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision@10: 0.032\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Função para calcular Mean Average Precision@K\n",
    "def calculate_map_at_k(true_labels, predicted_scores, k):\n",
    "    # Ordenar as previsões pelos maiores scores\n",
    "    sorted_indices = np.argsort(predicted_scores)[::-1]\n",
    "    sorted_labels = true_labels[sorted_indices]\n",
    "\n",
    "    # Calcular a Precisão Média nos primeiros k elementos\n",
    "    precision_at_k = np.cumsum(sorted_labels[:k]) / np.arange(1, k + 1)\n",
    "    map_at_k = np.mean(precision_at_k * sorted_labels[:k])\n",
    "\n",
    "    return map_at_k\n",
    "\n",
    "# Função para criar embeddings (substitua isso pela sua lógica real)\n",
    "def create_embeddings(data):\n",
    "    # Aqui, use seu modelo de embedding para obter embeddings para os dados\n",
    "    return np.random.rand(len(data), 64)\n",
    "\n",
    "# Função para executar consultas e obter scores de similaridade\n",
    "def run_queries(query_embedding, data_embeddings):\n",
    "    # Calcular similaridade de cosseno entre a consulta e todos os embeddings\n",
    "    similarity_scores = cosine_similarity(query_embedding.reshape(1, -1), data_embeddings).flatten()\n",
    "    return similarity_scores\n",
    "\n",
    "# Função principal do pipeline de avaliação\n",
    "def evaluate_retrieval_pipeline(data, queries, k):\n",
    "    # Criar embeddings para o conjunto de dados\n",
    "    data_embeddings = create_embeddings(data)\n",
    "\n",
    "    # Inicializar uma lista para armazenar os resultados MAP@K de cada consulta\n",
    "    map_at_k_list = []\n",
    "\n",
    "    # Iterar sobre cada consulta\n",
    "    for query in queries:\n",
    "        # Criar embeddings para a consulta\n",
    "        query_embedding = create_embeddings([query])[0]\n",
    "\n",
    "        # Executar a consulta para obter scores de similaridade\n",
    "        similarity_scores = run_queries(query_embedding, data_embeddings)\n",
    "\n",
    "        # Suponha que as relevâncias verdadeiras são aleatórias para este exemplo\n",
    "        true_labels = np.random.choice([0, 1], size=len(data), p=[0.9, 0.1])\n",
    "\n",
    "        # Calcular MAP@K para a consulta atual\n",
    "        map_at_k = calculate_map_at_k(true_labels, similarity_scores, k)\n",
    "        map_at_k_list.append(map_at_k)\n",
    "\n",
    "    # Calcular a média dos resultados MAP@K de todas as consultas\n",
    "    mean_map_at_k = np.mean(map_at_k_list)\n",
    "\n",
    "    return mean_map_at_k\n",
    "\n",
    "# Exemplo de uso\n",
    "data = np.random.rand(100, 64)  # Dados fictícios\n",
    "queries = np.random.rand(5, 64)  # Consultas fictícias\n",
    "k = 10  # Valor de K para MAP@K\n",
    "\n",
    "result = evaluate_retrieval_pipeline(data, queries, k)\n",
    "print(f\"Mean Average Precision@{k}: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado da comparação: [1 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_true_labels(array1, array2):\n",
    "    \"\"\"\n",
    "    Compara dois arrays e retorna um novo array de 0s e 1s, indicando se os elementos correspondentes são iguais.\n",
    "\n",
    "    Parameters:\n",
    "    - array1: Primeiro array a ser comparado (NumPy array).\n",
    "    - array2: Segundo array a ser comparado (NumPy array).\n",
    "\n",
    "    Returns:\n",
    "    - Novo array contendo 0s e 1s.\n",
    "    \"\"\"\n",
    "    if array1.shape != array2.shape:\n",
    "        raise ValueError(\"Os arrays devem ter o mesmo tamanho\")\n",
    "\n",
    "    resultado = np.where(array1 == array2, 1, 0)\n",
    "    return resultado\n",
    "\n",
    "# Exemplo de uso com arrays NumPy\n",
    "array1 = np.array([0, 3, 3, 0, 5, 6])\n",
    "array2 = np.array([0, 0, 0, 0, 0, 0])\n",
    "\n",
    "resultado_comparacao = get_true_labels(array1, array2)\n",
    "print(\"Resultado da comparação:\", resultado_comparacao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_embeddings(pickle_file_path):\n",
    "    with open(pickle_file_path, 'rb') as pickle_file:\n",
    "        loaded_data_dict = pickle.load(pickle_file)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    data = loaded_data_dict[\"embedding\"]\n",
    "    labels = np.array(loaded_data_dict[\"target\"])\n",
    "    return data, labels, loaded_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = './efficientnet_SwaV_test.pickle'\n",
    "path_train = '../notebooks/efficientnet_SwaV.pickle'\n",
    "\n",
    "swav_data_test, swav_labels_test, swav_result_test = load_embeddings(path_test)\n",
    "swav_data_train, swav_labels_train, swav_result_train = load_embeddings(path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12131, 1280)\n",
      "(496, 1280)\n",
      "(496,)\n"
     ]
    }
   ],
   "source": [
    "print(swav_data_train.shape)\n",
    "print(swav_data_test.shape)\n",
    "print(swav_labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swav_labels_test[40:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP@15 = 0.28\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Dados fictícios\n",
    "\n",
    "def run_queries(query_embedding, data_embeddings, similarity_function):\n",
    "    similarity_scores = similarity_function(query_embedding.reshape(1, -1), data_embeddings).flatten()\n",
    "    sorted_indices = np.argsort(similarity_scores)[::-1]\n",
    "    return sorted_indices, similarity_scores\n",
    "\n",
    "\n",
    "def map_relevant_labels(array1, array2):\n",
    "    \"\"\"\n",
    "    Compara dois arrays e retorna um novo array de 0s e 1s, indicando se os elementos correspondentes são iguais.\n",
    "\n",
    "    Parameters:\n",
    "    - array1: Primeiro array a ser comparado (NumPy array).\n",
    "    - array2: Segundo array a ser comparado (NumPy array).\n",
    "\n",
    "    Returns:\n",
    "    - Novo array contendo 0s e 1s.\n",
    "    \"\"\"\n",
    "    if array1.shape != array2.shape:\n",
    "        raise ValueError(\"Os arrays devem ter o mesmo tamanho\")\n",
    "\n",
    "    result = np.where(array1 == array2, 1, 0)\n",
    "    return result\n",
    "\n",
    "def calculate_map_at_k(relevant_labels, k_value):\n",
    "    ap_num = 0\n",
    "    for k in range(k_value):\n",
    "        precision_at_k = sum(relevant_labels[:k+1]) / (k+1)\n",
    "        # calculate numerator value for ap\n",
    "        ap_num += precision_at_k * relevant_labels[k]\n",
    "        # print(f\"P@{k+1}_{i+1} = {round(precision_at_k,2)}\")\n",
    "\n",
    "    ap_q = ap_num / k_value\n",
    "\n",
    "    return ap_q\n",
    "\n",
    "def evaluate_retrieval_pipeline(\n",
    "        data_embeddings,\n",
    "        data_labels, \n",
    "        query_embeddings, \n",
    "        query_labels, \n",
    "        k,\n",
    "        similarity_function\n",
    "        ):\n",
    "    \n",
    "    map_at_k_list = []\n",
    "    for q in range(len(query_embeddings)):\n",
    "        query_label = query_labels[q] \n",
    "        sorted_indices, _ = run_queries(query_embeddings[q], data_embeddings, similarity_function)\n",
    "        sorted_true_labels = data_labels[sorted_indices]\n",
    "        # [1,0,1,1,1,0] relevant label = 1\n",
    "        relevant_labels = map_relevant_labels(sorted_true_labels, np.full(len(sorted_true_labels), query_label))\n",
    "        average_precision = calculate_map_at_k(relevant_labels, k)\n",
    "        map_at_k_list.append(average_precision)\n",
    "\n",
    "\n",
    "k_value = 15\n",
    "queries = 30\n",
    "\n",
    "map_at_k_list = []\n",
    "\n",
    "\n",
    "for i in range(queries):\n",
    "\n",
    "    query_embedding = swav_data_test[i] \n",
    "    query_label = swav_labels_test[i] \n",
    "\n",
    "    similarity_scores = cosine_similarity(query_embedding.reshape(1, -1), swav_data_train).flatten()\n",
    "    sorted_indices = np.argsort(similarity_scores)[::-1]\n",
    "    sorted_true_labels = swav_labels_train[sorted_indices]\n",
    "    relevant_labels = get_true_labels(sorted_true_labels, np.full(len(sorted_true_labels), query_label))\n",
    "\n",
    "    ap_num = 0\n",
    "    for k in range(k_value):\n",
    "\n",
    "        precision_at_k = sum(relevant_labels[:k+1]) / (k+1)\n",
    "        # calculate numerator value for ap\n",
    "        ap_num += precision_at_k * relevant_labels[k]\n",
    "        # print(f\"P@{k+1}_{i+1} = {round(precision_at_k,2)}\")\n",
    "\n",
    "    ap_q = ap_num / k_value\n",
    "    # print(f\"AP@{k+1}: {round(ap_q,2)}\") \n",
    "    map_at_k_list.append(ap_q)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "map_at_k = sum(map_at_k_list) / queries\n",
    "\n",
    "# generate results\n",
    "print(f\"mAP@{k+1} = {round(map_at_k, 2)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_true_labels \n",
    "relevant_label = swav_labels_test[0]\n",
    "get_true_labels(sorted_true_labels, np.full(len(sorted_true_labels), relevant_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k =1\n",
    "sum(relevant_labels[:k]) / (k+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_labels[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_labels[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 1, 3, 2, 0, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_true_labels[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP@8_1 = 0.54\n",
      "AP@8_2 = 0.67\n",
      "AP@8_3 = 0.23\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m     ap_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m precision_at_k \u001b[38;5;241m*\u001b[39m rel_k\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# now we calculate the AP value as the average of AP\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# numerator values\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m ap_q \u001b[38;5;241m=\u001b[39m \u001b[43map_num\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mactual\u001b[49m\u001b[43m[\u001b[49m\u001b[43mq\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAP@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(ap_q,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m ap\u001b[38;5;241m.\u001b[39mappend(ap_q)\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "# initialize variables\n",
    "actual = [\n",
    "    [2, 4, 5, 7],\n",
    "    [1, 4, 5, 7],\n",
    "    [5, 8],\n",
    "    []\n",
    "]\n",
    "Q = len(actual)\n",
    "predicted = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "k = 8\n",
    "ap = []\n",
    "\n",
    "# loop through and calculate AP for each query q\n",
    "for q in range(Q):\n",
    "    ap_num = 0\n",
    "    # loop through k values\n",
    "    for x in range(k):\n",
    "        # calculate precision@k\n",
    "        act_set = set(actual[q])                                                                                                                                   \n",
    "        pred_set = set(predicted[:x+1])\n",
    "        precision_at_k = len(act_set & pred_set) / (x+1)\n",
    "        # calculate rel_k values\n",
    "        if predicted[x] in actual[q]:\n",
    "            rel_k = 1\n",
    "        else:\n",
    "            rel_k = 0\n",
    "        # calculate numerator value for ap\n",
    "        ap_num += precision_at_k * rel_k\n",
    "    # now we calculate the AP value as the average of AP\n",
    "    # numerator values\n",
    "    ap_q = ap_num / len(actual[q])\n",
    "    print(f\"AP@{k}_{q+1} = {round(ap_q,2)}\")\n",
    "    ap.append(ap_q)\n",
    "\n",
    "# now we take the mean of all ap values to get mAP\n",
    "map_at_k = sum(ap) / Q\n",
    "\n",
    "# generate results\n",
    "print(f\"mAP@{k} = {round(map_at_k, 2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "renal-pathology-retrieval-P_udDvkW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
