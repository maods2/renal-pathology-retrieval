{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "from lightly.loss import SwaVLoss\n",
    "from lightly.models.modules import SwaVProjectionHead, SwaVPrototypes\n",
    "from lightly.transforms.swav_transform import SwaVTransform\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "\n",
    "class SwaV(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projection_head = SwaVProjectionHead(512, 512, 128)\n",
    "        self.prototypes = SwaVPrototypes(128, n_prototypes=512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        x = self.projection_head(x)\n",
    "        x = nn.functional.normalize(x, dim=1, p=2)\n",
    "        p = self.prototypes(x)\n",
    "        return p\n",
    "\n",
    "\n",
    "resnet = torchvision.models.resnet18()\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "model = SwaV(backbone)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "backbone = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "# Ignore the classification head as we only want the features.\n",
    "backbone._fc = torch.nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing -c...\n",
      "Resolving -c...\n",
      "[    ] Installing...\n",
      "[    ] Installing...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Courtesy Notice: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set PIPENV_IGNORE_VIRTUALENVS=1 to force pipenv to ignore that environment and create its own instead. You can set PIPENV_VERBOSITY=-1 to suppress this warning.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\pipenv\\patched\\pip\\_vendor\\packaging\\requirements.py\", line 102, in __init__\n",
      "    req = REQUIREMENT.parseString(requirement_string)\n",
      "  File \"C:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\pipenv\\patched\\pip\\_vendor\\pyparsing\\core.py\", line 1141, in parse_string\n",
      "    raise exc.with_traceback(None)\n",
      "pipenv.patched.pip._vendor.pyparsing.exceptions.ParseException: Expected W:(0-9A-Za-z), found '-'  (at char 0), (line:1, col:1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\pipenv\\patched\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3101, in __init__\n",
      "    super(Requirement, self).__init__(requirement_string)\n",
      "  File \"C:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\pipenv\\patched\\pip\\_vendor\\packaging\\requirements.py\", line 104, in __init__\n",
      "    raise InvalidRequirement(\n",
      "pipenv.patched.pip._vendor.packaging.requirements.InvalidRequirement: Parse error at \"'-c'\": Expected W:(0-9A-Za-z)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\pipenv\\vendor\\requirementslib\\models\\requirements.py\", line 964, in _parse_name_from_line\n",
      "    self._requirement = init_requirement(self.line)\n",
      "  File \"C:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\pipenv\\vendor\\requirementslib\\models\\utils.py\", line 191, in init_requirement\n",
      "    req = Requirement.parse(name)\n",
      "  File \"C:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\pipenv\\patched\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3148, in parse\n",
      "    req, = parse_requirements(s)\n",
      "  File \"C:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\pipenv\\patched\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3094, in parse_requirements\n",
      "    yield Requirement(line)\n",
      "  File \"C:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\pipenv\\patched\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3103, in __init__\n",
      "    raise RequirementParseError(str(e))\n",
      "pipenv.patched.pip._vendor.pkg_resources.RequirementParseError: Parse error at \"'-c'\": Expected W:(0-9A-Za-z)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\Scripts\\pipenv.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\pipenv\\vendor\\click\\core.py\", line 1128, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"C:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\pipenv\\cli\\options.py\", line 58, in main\n",
      "    return super().main(*args, **kwargs, windows_expand_args=False)\n",
      "  File \"C:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\pipenv\\vendor\\click\\core.py\", line 1053, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"C:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\pipenv\\vendor\\click\\core.py\", line 1659, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"C:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\pipenv\\vendor\\click\\core.py\", line 1395, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"C:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\pipenv\\vendor\\click\\core.py\", line 754, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"C:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\pipenv\\vendor\\click\\decorators.py\", line 84, in new_func\n",
      "    return ctx.invoke(f, obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\pipenv\\vendor\\click\\core.py\", line 754, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"C:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\pipenv\\cli\\command.py\", line 235, in install\n",
      "    do_install(\n",
      "  File \"C:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\pipenv\\routines\\install.py\", line 246, in do_install\n",
      "    pkg_requirement = Requirement.from_line(pkg_line)\n",
      "  File \"C:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\pipenv\\vendor\\requirementslib\\models\\requirements.py\", line 2677, in from_line\n",
      "    parsed_line = Line(line)\n",
      "  File \"C:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\pipenv\\vendor\\requirementslib\\models\\requirements.py\", line 171, in __init__\n",
      "    self.parse()\n",
      "  File \"C:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\pipenv\\vendor\\requirementslib\\models\\requirements.py\", line 1300, in parse\n",
      "    self.parse_name()\n",
      "  File \"C:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\pipenv\\vendor\\requirementslib\\models\\requirements.py\", line 1025, in parse_name\n",
      "    name = self._parse_name_from_line()\n",
      "  File \"C:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\pipenv\\vendor\\requirementslib\\models\\requirements.py\", line 966, in _parse_name_from_line\n",
      "    raise RequirementError(\n",
      "pipenv.vendor.requirementslib.exceptions.RequirementError: Failed parsing requirement from '-c'\n"
     ]
    }
   ],
   "source": [
    "!pipenv install huggingface transformers==4.14.1 tokenizers==0.10.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "# Create a PyTorch module for the SimCLR model.\n",
    "class SimCLR(torch.nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projection_head = heads.SimCLRProjectionHead(\n",
    "            input_dim=512,  # Resnet18 features have 512 dimensions.\n",
    "            hidden_dim=512,\n",
    "            output_dim=128,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(features)\n",
    "        return z\n",
    "\n",
    "\n",
    "# Use a resnet backbone.\n",
    "backbone = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "# Ignore the classification head as we only want the features.\n",
    "backbone._fc = torch.nn.Identity()\n",
    "\n",
    "# Build the SimCLR model.\n",
    "model = SimCLR(backbone)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "loss: 5.50475\n",
      "loss: 5.51909\n",
      "loss: 5.52904\n",
      "loss: 5.51790\n",
      "loss: 5.53173\n",
      "loss: 5.49575\n",
      "loss: 5.51858\n",
      "loss: 5.53091\n",
      "loss: 5.48558\n",
      "loss: 5.46340\n",
      "loss: 5.45240\n",
      "loss: 5.47274\n",
      "loss: 5.50354\n",
      "loss: 5.50478\n",
      "loss: 5.49191\n",
      "loss: 5.47412\n",
      "loss: 5.44785\n",
      "loss: 5.42565\n",
      "loss: 5.51401\n",
      "loss: 5.45893\n",
      "loss: 5.44463\n",
      "loss: 5.43077\n",
      "loss: 5.41673\n",
      "loss: 5.47465\n",
      "loss: 5.43301\n",
      "loss: 5.41797\n",
      "loss: 5.43376\n",
      "loss: 5.45916\n",
      "loss: 5.38749\n",
      "loss: 5.41056\n",
      "loss: 5.35334\n",
      "loss: 5.35617\n",
      "loss: 5.42437\n",
      "loss: 5.40366\n",
      "loss: 5.38953\n",
      "loss: 5.35424\n",
      "loss: 5.41400\n",
      "loss: 5.35691\n",
      "loss: 5.41279\n",
      "loss: 5.42426\n",
      "loss: 5.45534\n",
      "loss: 5.37404\n",
      "loss: 5.36493\n",
      "loss: 5.34767\n",
      "loss: 5.40132\n",
      "loss: 5.31602\n",
      "loss: 5.37155\n",
      "loss: 5.38717\n",
      "loss: 5.34566\n",
      "loss: 5.36895\n",
      "loss: 5.35160\n",
      "loss: 5.42077\n",
      "loss: 5.33988\n",
      "loss: 5.32502\n",
      "loss: 5.31049\n",
      "loss: 5.33960\n",
      "loss: 5.36135\n",
      "loss: 5.29119\n",
      "loss: 5.35251\n",
      "loss: 5.29370\n",
      "loss: 5.27821\n",
      "loss: 5.31126\n",
      "loss: 5.33855\n",
      "loss: 5.30880\n",
      "loss: 5.26585\n",
      "loss: 5.25315\n",
      "loss: 5.31130\n",
      "loss: 5.26712\n",
      "loss: 5.32240\n",
      "loss: 5.29432\n",
      "loss: 5.21763\n",
      "loss: 5.23851\n",
      "loss: 5.34727\n",
      "loss: 5.34732\n",
      "loss: 5.25144\n",
      "loss: 5.23767\n",
      "loss: 5.29244\n",
      "loss: 5.36769\n",
      "loss: 5.27898\n",
      "loss: 5.25829\n",
      "loss: 5.25916\n",
      "loss: 5.26426\n",
      "loss: 5.26545\n",
      "loss: 5.31731\n",
      "loss: 5.26851\n",
      "loss: 5.23805\n",
      "loss: 5.33766\n",
      "loss: 5.26882\n",
      "loss: 5.30104\n",
      "loss: 5.25088\n",
      "loss: 5.27816\n",
      "loss: 5.36790\n",
      "loss: 5.16281\n",
      "loss: 5.28304\n",
      "loss: 4.96840\n",
      "loss: 5.23495\n",
      "loss: 5.23375\n",
      "loss: 5.19603\n",
      "loss: 5.27680\n",
      "loss: 5.22345\n",
      "loss: 5.19739\n",
      "loss: 5.21859\n",
      "loss: 5.21448\n",
      "loss: 5.33250\n",
      "loss: 5.24118\n",
      "loss: 5.23815\n",
      "loss: 5.24163\n",
      "loss: 5.34775\n",
      "loss: 5.24289\n",
      "loss: 5.10970\n",
      "loss: 5.24979\n",
      "loss: 5.25785\n",
      "loss: 5.20410\n",
      "loss: 5.23827\n",
      "loss: 5.18586\n",
      "loss: 5.22474\n",
      "loss: 5.19527\n",
      "loss: 5.24346\n",
      "loss: 5.19151\n",
      "loss: 5.25406\n",
      "loss: 5.27880\n",
      "loss: 5.21557\n",
      "loss: 5.20987\n",
      "loss: 5.24981\n",
      "loss: 5.12666\n",
      "loss: 5.29347\n",
      "loss: 5.25444\n",
      "loss: 5.17906\n",
      "loss: 5.15016\n",
      "loss: 5.21453\n",
      "loss: 5.26425\n",
      "loss: 5.13081\n",
      "loss: 5.20321\n",
      "loss: 5.10347\n",
      "loss: 5.27999\n",
      "loss: 5.11571\n",
      "loss: 5.16805\n",
      "loss: 5.14394\n",
      "loss: 5.24983\n",
      "loss: 5.12932\n",
      "loss: 5.14470\n",
      "loss: 5.22811\n",
      "loss: 5.17189\n",
      "loss: 5.16156\n",
      "loss: 5.07418\n",
      "loss: 5.15989\n",
      "loss: 5.10964\n",
      "loss: 5.16882\n",
      "loss: 5.11898\n",
      "loss: 5.14390\n",
      "loss: 5.28333\n",
      "loss: 5.24870\n",
      "loss: 5.29389\n",
      "loss: 5.13892\n",
      "loss: 5.21731\n",
      "loss: 5.13492\n",
      "loss: 5.17589\n",
      "loss: 5.22327\n",
      "loss: 5.17909\n",
      "loss: 5.14335\n",
      "loss: 5.13465\n",
      "loss: 5.17674\n",
      "loss: 5.13974\n",
      "loss: 5.06595\n",
      "loss: 5.10100\n",
      "loss: 5.07383\n",
      "loss: 5.16168\n",
      "loss: 5.15439\n",
      "loss: 5.21358\n",
      "loss: 5.06204\n",
      "loss: 5.11681\n",
      "loss: 5.12439\n",
      "loss: 5.12796\n",
      "loss: 5.14495\n",
      "loss: 5.17582\n",
      "loss: 5.10242\n",
      "loss: 5.26125\n",
      "loss: 5.10602\n",
      "loss: 5.15298\n",
      "loss: 5.24217\n",
      "loss: 5.14628\n",
      "loss: 5.14695\n",
      "loss: 5.16913\n",
      "loss: 5.16762\n",
      "loss: 5.12792\n",
      "loss: 5.18101\n",
      "loss: 5.08265\n",
      "loss: 5.19598\n",
      "loss: 5.17135\n",
      "loss: 4.83310\n",
      "loss: 5.10971\n",
      "loss: 5.03502\n",
      "loss: 5.21438\n",
      "loss: 5.24844\n",
      "loss: 5.14442\n",
      "loss: 5.17030\n",
      "loss: 5.08980\n",
      "loss: 5.04122\n",
      "loss: 5.08592\n",
      "loss: 5.09316\n",
      "loss: 5.13400\n",
      "loss: 5.05797\n",
      "loss: 5.03685\n",
      "loss: 5.04014\n",
      "loss: 5.16148\n",
      "loss: 5.19618\n",
      "loss: 5.12134\n",
      "loss: 5.06220\n",
      "loss: 5.03197\n",
      "loss: 5.19392\n",
      "loss: 5.06072\n",
      "loss: 5.14831\n",
      "loss: 5.02791\n",
      "loss: 5.01339\n",
      "loss: 5.03206\n",
      "loss: 5.17566\n",
      "loss: 5.14780\n",
      "loss: 5.13394\n",
      "loss: 5.06963\n",
      "loss: 5.08400\n",
      "loss: 5.17003\n",
      "loss: 5.12685\n",
      "loss: 5.00350\n",
      "loss: 5.09927\n",
      "loss: 5.06999\n",
      "loss: 5.08304\n",
      "loss: 5.01288\n",
      "loss: 5.06628\n",
      "loss: 4.97768\n",
      "loss: 4.96063\n",
      "loss: 5.15714\n",
      "loss: 5.04582\n",
      "loss: 5.12169\n",
      "loss: 5.11022\n",
      "loss: 5.03539\n",
      "loss: 5.09079\n",
      "loss: 5.10705\n",
      "loss: 5.08486\n",
      "loss: 5.16416\n",
      "loss: 5.22446\n",
      "loss: 5.09624\n",
      "loss: 4.97817\n",
      "loss: 5.03234\n",
      "loss: 5.04695\n",
      "loss: 5.04241\n",
      "loss: 5.08484\n",
      "loss: 4.94865\n",
      "loss: 4.98726\n",
      "loss: 5.04014\n",
      "loss: 4.98956\n",
      "loss: 4.98183\n",
      "loss: 4.96491\n",
      "loss: 5.09516\n",
      "loss: 5.01324\n",
      "loss: 5.00578\n",
      "loss: 4.96213\n",
      "loss: 5.13053\n",
      "loss: 5.10208\n",
      "loss: 4.99560\n",
      "loss: 4.97163\n",
      "loss: 5.03118\n",
      "loss: 4.96109\n",
      "loss: 4.98113\n",
      "loss: 5.19919\n",
      "loss: 5.04561\n",
      "loss: 5.12345\n",
      "loss: 5.02705\n",
      "loss: 4.99046\n",
      "loss: 4.96573\n",
      "loss: 4.96487\n",
      "loss: 5.07607\n",
      "loss: 5.02365\n",
      "loss: 5.06355\n",
      "loss: 5.07012\n",
      "loss: 4.99777\n",
      "loss: 5.02578\n",
      "loss: 4.98071\n",
      "loss: 4.95491\n",
      "loss: 5.03798\n",
      "loss: 4.95212\n",
      "loss: 4.90078\n",
      "loss: 4.99162\n",
      "loss: 4.96810\n",
      "loss: 5.03965\n",
      "loss: 4.81200\n",
      "loss: 4.96027\n",
      "loss: 5.01895\n",
      "loss: 5.08426\n",
      "loss: 5.04482\n",
      "loss: 4.99471\n",
      "loss: 5.14309\n",
      "loss: 4.94260\n",
      "loss: 4.96117\n",
      "loss: 4.97786\n",
      "loss: 5.03771\n",
      "loss: 5.00207\n",
      "loss: 4.93534\n",
      "loss: 4.95269\n",
      "loss: 4.97276\n",
      "loss: 5.04707\n",
      "loss: 4.94894\n",
      "loss: 5.06124\n",
      "loss: 4.96404\n",
      "loss: 4.97379\n",
      "loss: 5.03096\n",
      "loss: 4.97558\n",
      "loss: 5.05808\n",
      "loss: 4.95282\n",
      "loss: 4.98456\n",
      "loss: 5.06032\n",
      "loss: 5.05690\n",
      "loss: 4.97901\n",
      "loss: 4.96513\n",
      "loss: 5.13352\n",
      "loss: 5.01180\n",
      "loss: 5.06327\n",
      "loss: 5.04388\n",
      "loss: 5.02709\n",
      "loss: 5.01733\n",
      "loss: 4.97671\n",
      "loss: 5.01009\n",
      "loss: 5.02370\n",
      "loss: 5.00706\n",
      "loss: 5.01539\n",
      "loss: 4.92529\n",
      "loss: 5.02799\n",
      "loss: 4.86484\n",
      "loss: 4.94006\n",
      "loss: 4.90147\n",
      "loss: 5.06353\n",
      "loss: 4.95351\n",
      "loss: 5.00948\n",
      "loss: 4.93084\n",
      "loss: 4.92124\n",
      "loss: 4.96263\n",
      "loss: 4.94364\n",
      "loss: 5.01356\n",
      "loss: 5.00428\n",
      "loss: 5.13046\n",
      "loss: 4.96380\n",
      "loss: 5.08814\n",
      "loss: 4.93371\n",
      "loss: 4.92380\n",
      "loss: 5.01051\n",
      "loss: 4.92390\n",
      "loss: 4.90644\n",
      "loss: 4.97434\n",
      "loss: 4.90848\n",
      "loss: 4.99039\n",
      "loss: 5.04546\n",
      "loss: 5.00504\n",
      "loss: 5.00052\n",
      "loss: 4.92300\n",
      "loss: 4.99870\n",
      "loss: 4.93117\n",
      "loss: 4.96215\n",
      "loss: 4.96242\n",
      "loss: 5.02202\n",
      "loss: 4.88916\n",
      "loss: 4.96407\n",
      "loss: 4.87256\n",
      "loss: 4.94917\n",
      "loss: 4.90012\n",
      "loss: 4.90401\n",
      "loss: 4.88087\n",
      "loss: 4.96594\n",
      "loss: 4.95936\n",
      "loss: 5.02597\n",
      "loss: 4.96759\n",
      "loss: 4.98626\n",
      "loss: 4.95577\n",
      "loss: 4.87848\n",
      "loss: 5.03481\n",
      "loss: 4.87706\n",
      "loss: 5.06924\n",
      "loss: 4.98723\n",
      "loss: 4.90171\n",
      "loss: 4.96741\n",
      "loss: 4.81600\n",
      "loss: 4.68567\n",
      "loss: 4.94042\n",
      "loss: 5.05268\n",
      "loss: 5.06205\n",
      "loss: 4.87801\n",
      "loss: 4.94815\n",
      "loss: 4.92989\n",
      "loss: 4.93663\n",
      "loss: 4.91907\n",
      "loss: 4.99931\n",
      "loss: 4.87814\n",
      "loss: 4.97530\n",
      "loss: 5.02453\n",
      "loss: 4.84724\n",
      "loss: 5.05004\n",
      "loss: 4.89889\n",
      "loss: 4.88360\n",
      "loss: 4.82376\n",
      "loss: 4.93732\n",
      "loss: 5.02023\n",
      "loss: 5.02548\n",
      "loss: 4.90383\n",
      "loss: 4.94252\n",
      "loss: 4.92204\n",
      "loss: 4.88648\n",
      "loss: 4.89192\n",
      "loss: 4.92876\n",
      "loss: 4.87786\n",
      "loss: 4.95573\n",
      "loss: 4.86046\n",
      "loss: 4.91077\n",
      "loss: 4.89840\n",
      "loss: 4.94123\n",
      "loss: 4.93150\n",
      "loss: 4.98169\n",
      "loss: 4.88539\n",
      "loss: 4.94753\n",
      "loss: 4.96854\n",
      "loss: 4.92188\n",
      "loss: 4.95599\n",
      "loss: 4.97869\n",
      "loss: 4.84495\n",
      "loss: 4.86723\n",
      "loss: 4.86528\n",
      "loss: 4.93045\n",
      "loss: 4.95840\n",
      "loss: 4.86095\n",
      "loss: 4.95334\n",
      "loss: 4.82483\n",
      "loss: 4.92875\n",
      "loss: 4.94509\n",
      "loss: 4.93120\n",
      "loss: 4.90524\n",
      "loss: 4.96771\n",
      "loss: 4.96217\n",
      "loss: 4.91711\n",
      "loss: 4.89088\n",
      "loss: 4.88785\n",
      "loss: 4.92089\n",
      "loss: 5.03769\n",
      "loss: 4.94036\n",
      "loss: 4.89980\n",
      "loss: 5.03200\n",
      "loss: 4.89572\n",
      "loss: 4.80702\n",
      "loss: 4.96711\n",
      "loss: 4.87972\n",
      "loss: 4.92877\n",
      "loss: 4.88587\n",
      "loss: 4.99655\n",
      "loss: 4.85987\n",
      "loss: 4.85246\n",
      "loss: 4.82947\n",
      "loss: 4.92136\n",
      "loss: 4.89664\n",
      "loss: 4.86340\n",
      "loss: 4.87915\n",
      "loss: 4.81151\n",
      "loss: 4.88286\n",
      "loss: 4.93707\n",
      "loss: 4.90237\n",
      "loss: 4.87193\n",
      "loss: 4.86521\n",
      "loss: 4.90740\n",
      "loss: 4.97913\n",
      "loss: 4.87823\n",
      "loss: 4.90199\n",
      "loss: 4.94855\n",
      "loss: 4.90803\n",
      "loss: 4.88414\n",
      "loss: 4.97651\n",
      "loss: 4.88179\n",
      "loss: 4.98386\n",
      "loss: 4.90820\n",
      "loss: 4.91185\n",
      "loss: 4.58335\n",
      "loss: 4.99336\n",
      "loss: 4.90960\n",
      "loss: 4.86131\n",
      "loss: 4.89030\n",
      "loss: 4.85747\n",
      "loss: 4.83024\n",
      "loss: 4.85121\n",
      "loss: 4.84006\n",
      "loss: 4.81791\n",
      "loss: 4.80357\n",
      "loss: 4.84262\n",
      "loss: 4.86461\n",
      "loss: 4.78838\n",
      "loss: 4.89126\n",
      "loss: 4.80353\n",
      "loss: 4.88136\n",
      "loss: 4.86599\n",
      "loss: 4.87348\n",
      "loss: 4.95642\n",
      "loss: 4.85621\n",
      "loss: 4.87437\n",
      "loss: 4.80182\n",
      "loss: 4.93787\n",
      "loss: 4.88400\n",
      "loss: 4.83583\n",
      "loss: 4.74971\n",
      "loss: 4.84704\n",
      "loss: 4.88048\n",
      "loss: 4.86544\n",
      "loss: 4.97093\n",
      "loss: 4.85398\n",
      "loss: 4.93021\n",
      "loss: 4.79596\n",
      "loss: 5.06302\n",
      "loss: 4.86125\n",
      "loss: 4.83197\n",
      "loss: 4.77702\n",
      "loss: 4.80786\n",
      "loss: 4.82156\n",
      "loss: 5.02357\n",
      "loss: 4.74335\n",
      "loss: 4.93583\n",
      "loss: 4.85458\n",
      "loss: 4.88806\n",
      "loss: 4.70432\n",
      "loss: 4.76827\n",
      "loss: 4.85803\n",
      "loss: 4.86580\n",
      "loss: 4.76706\n",
      "loss: 4.89616\n",
      "loss: 4.82675\n",
      "loss: 4.94422\n",
      "loss: 5.03205\n",
      "loss: 4.82225\n",
      "loss: 4.86383\n",
      "loss: 4.74294\n",
      "loss: 4.81846\n",
      "loss: 4.83916\n",
      "loss: 4.84658\n",
      "loss: 4.93238\n",
      "loss: 4.92329\n",
      "loss: 4.85199\n",
      "loss: 4.80585\n",
      "loss: 4.79038\n",
      "loss: 4.81898\n",
      "loss: 4.82818\n",
      "loss: 4.86359\n",
      "loss: 4.81102\n",
      "loss: 4.87736\n",
      "loss: 4.92571\n",
      "loss: 4.95210\n",
      "loss: 5.01187\n",
      "loss: 4.75122\n",
      "loss: 4.87716\n",
      "loss: 4.82431\n",
      "loss: 4.77522\n",
      "loss: 4.83639\n",
      "loss: 4.83824\n",
      "loss: 4.78433\n",
      "loss: 4.86033\n",
      "loss: 4.76651\n",
      "loss: 4.93427\n",
      "loss: 4.94883\n",
      "loss: 4.91170\n",
      "loss: 4.90983\n",
      "loss: 4.83622\n",
      "loss: 4.85440\n",
      "loss: 4.92721\n",
      "loss: 4.71128\n",
      "loss: 4.90657\n",
      "loss: 4.75347\n",
      "loss: 4.96752\n",
      "loss: 4.81963\n",
      "loss: 4.82677\n",
      "loss: 4.69634\n",
      "loss: 4.90868\n",
      "loss: 4.85156\n",
      "loss: 4.81645\n",
      "loss: 4.94466\n",
      "loss: 4.81019\n",
      "loss: 4.86696\n",
      "loss: 4.80806\n",
      "loss: 4.85293\n",
      "loss: 4.84591\n",
      "loss: 4.80605\n",
      "loss: 4.91935\n",
      "loss: 4.82544\n",
      "loss: 4.79989\n",
      "loss: 4.84162\n",
      "loss: 4.82391\n",
      "loss: 4.89606\n",
      "loss: 4.91722\n",
      "loss: 4.83840\n",
      "loss: 4.87536\n",
      "loss: 4.93883\n",
      "loss: 4.82459\n",
      "loss: 4.83507\n",
      "loss: 4.88137\n",
      "loss: 4.95310\n",
      "loss: 4.89475\n",
      "loss: 4.76686\n",
      "loss: 4.70908\n",
      "loss: 4.82150\n",
      "loss: 4.82521\n",
      "loss: 4.87152\n",
      "loss: 4.77375\n",
      "loss: 4.81482\n",
      "loss: 4.94136\n",
      "loss: 4.89616\n",
      "loss: 4.92963\n",
      "loss: 4.90166\n",
      "loss: 4.74731\n",
      "loss: 4.69298\n",
      "loss: 4.93330\n",
      "loss: 4.83686\n",
      "loss: 4.80991\n",
      "loss: 4.81557\n",
      "loss: 4.76971\n",
      "loss: 4.83546\n",
      "loss: 4.87950\n",
      "loss: 4.79294\n",
      "loss: 4.85572\n",
      "loss: 4.91724\n",
      "loss: 4.87465\n",
      "loss: 4.79244\n",
      "loss: 4.91531\n",
      "loss: 4.90229\n",
      "loss: 4.88929\n",
      "loss: 4.81138\n",
      "loss: 4.79733\n",
      "loss: 4.75208\n",
      "loss: 4.85146\n",
      "loss: 4.74986\n",
      "loss: 4.83982\n",
      "loss: 4.68565\n",
      "loss: 4.82906\n",
      "loss: 4.74671\n",
      "loss: 4.84676\n",
      "loss: 4.82289\n",
      "loss: 4.84691\n",
      "loss: 4.85199\n",
      "loss: 4.84976\n",
      "loss: 4.85110\n",
      "loss: 4.86980\n",
      "loss: 4.88011\n",
      "loss: 4.80158\n",
      "loss: 4.78598\n",
      "loss: 4.91030\n",
      "loss: 4.80328\n",
      "loss: 4.82709\n",
      "loss: 4.93197\n",
      "loss: 4.74311\n",
      "loss: 4.95411\n",
      "loss: 4.87688\n",
      "loss: 4.80650\n",
      "loss: 4.84866\n",
      "loss: 4.91734\n",
      "loss: 4.75112\n",
      "loss: 4.84381\n",
      "loss: 4.81597\n",
      "loss: 4.79139\n",
      "loss: 4.80939\n",
      "loss: 4.77617\n",
      "loss: 4.69338\n",
      "loss: 4.83815\n",
      "loss: 4.72902\n",
      "loss: 4.71335\n",
      "loss: 4.82396\n",
      "loss: 4.81365\n",
      "loss: 4.45798\n",
      "loss: 4.89536\n",
      "loss: 4.93887\n",
      "loss: 4.80347\n",
      "loss: 4.83224\n",
      "loss: 4.78185\n",
      "loss: 4.87284\n",
      "loss: 4.72404\n",
      "loss: 4.90268\n",
      "loss: 4.86547\n",
      "loss: 4.72818\n",
      "loss: 4.80827\n",
      "loss: 4.80553\n",
      "loss: 4.88439\n",
      "loss: 4.80923\n",
      "loss: 4.87332\n",
      "loss: 4.69207\n",
      "loss: 4.80472\n",
      "loss: 4.78533\n",
      "loss: 4.75361\n",
      "loss: 4.94686\n",
      "loss: 4.85062\n",
      "loss: 4.82423\n",
      "loss: 4.97603\n",
      "loss: 4.74453\n",
      "loss: 4.83844\n"
     ]
    }
   ],
   "source": [
    "# Create a PyTorch module for the SimCLR model.\n",
    "class SimCLR(torch.nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projection_head = heads.SimCLRProjectionHead(\n",
    "            input_dim=1280,  # Resnet18 features have 512 dimensions.\n",
    "            hidden_dim=1280,\n",
    "            output_dim=128,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(features)\n",
    "        return z\n",
    "\n",
    "\n",
    "# Use a resnet backbone.\n",
    "backbone = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "# Ignore the classification head as we only want the features.\n",
    "backbone._fc = torch.nn.Identity()\n",
    "\n",
    "# Build the SimCLR model.\n",
    "model = SimCLR(backbone)\n",
    "\n",
    "\n",
    "# Prepare transform that creates multiple random views for every image.\n",
    "transform = transforms.SimCLRTransform(input_size=32, cj_prob=0.5)\n",
    "\n",
    "\n",
    "# Create a dataset from your image folder.\n",
    "dataset = LightlyDataset(input_dir=\"../data/02_data_split/train_data/\", transform=transform)\n",
    "\n",
    "# Build a PyTorch dataloader.\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,  # Pass the dataset to the dataloader.\n",
    "    batch_size=128,  # A large batch size helps with the learning.\n",
    "    shuffle=True,  # Shuffling is important!\n",
    ")\n",
    "\n",
    "# Lightly exposes building blocks such as loss functions.\n",
    "criterion = loss.NTXentLoss(temperature=0.5)\n",
    "\n",
    "# Get a PyTorch optimizer.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-6)\n",
    "\n",
    "# Train the model.\n",
    "for epoch in range(10):\n",
    "    for (view0, view1), targets, filenames in dataloader:\n",
    "        z0 = model(view0)\n",
    "        z1 = model(view1)\n",
    "        loss = criterion(z0, z1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(f\"loss: {loss.item():.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "renal-pathology-retrieval-P_udDvkW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
