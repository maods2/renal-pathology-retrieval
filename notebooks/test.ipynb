{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Callable, Optional, Tuple, Any\n",
    "\n",
    "\n",
    "class ImageDataLoader:\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        self.dataset = CustomImageFolder(\n",
    "            self.data_dir, transform=self.transform, target_transform=self._get_class_name)\n",
    "\n",
    "        self.dataloader = DataLoader(self.dataset, shuffle=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.dataloader)\n",
    "\n",
    "    def _get_class_name(self, index):\n",
    "        return index\n",
    "\n",
    "\n",
    "class CustomImageFolder(ImageFolder):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            root=root,\n",
    "            transform=transform,\n",
    "            target_transform=target_transform,\n",
    "        )\n",
    "        # self.paths = [s[0] for s in self.samples]\n",
    "        # self.labels = [self.classes[s[1]]\n",
    "        #                for s in self.samples]\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (sample, target) where target is class_index of the target class.\n",
    "        \"\"\"\n",
    "        path, target = self.samples[index]\n",
    "\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return sample, target, path, self.classes[target]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': 'a', 'value2': 4}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestClass:\n",
    "    def __init__(self) -> None:\n",
    "        self.value='a'\n",
    "        self.value2=4\n",
    "\n",
    "a = TestClass()\n",
    "a.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch \n",
    "\n",
    "kl_loss = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "# input should be a distribution in the log space\n",
    "input = F.log_softmax(torch.randn(3, 5, requires_grad=True), dim=1)\n",
    "# Sample a batch of distributions. Usually this would come from the dataset\n",
    "target = F.softmax(torch.rand(3, 5), dim=1)\n",
    "output = kl_loss(input, target)\n",
    "\n",
    "kl_loss = torch.nn.KLDivLoss(reduction=\"batchmean\", log_target=True)\n",
    "log_target = F.log_softmax(torch.rand(3, 5), dim=1)\n",
    "output = kl_loss(input, log_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2092, 0.2701, 0.1380, 0.2157, 0.1670],\n",
       "        [0.2003, 0.1553, 0.1906, 0.2305, 0.2233],\n",
       "        [0.2406, 0.2203, 0.2519, 0.1886, 0.0987]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"C:/Users/Maods/Documents/Development/Mestrado/terumo/apps/renal-pathology-retrieval/data/02_data_split/train_data\"\n",
    "data_loader = ImageDataLoader(data_dir)\n",
    "dataloader = DataLoader(data_loader.dataset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0196, -0.0431, -0.0824,  ..., -0.1529, -0.1059, -0.1373],\n",
       "          [ 0.0510, -0.0510, -0.1451,  ..., -0.1294, -0.1451, -0.1451],\n",
       "          [ 0.0510, -0.0824, -0.1608,  ..., -0.0980, -0.1451, -0.0588],\n",
       "          ...,\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9059,  0.7882,  0.7804],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.8510,  0.7804,  0.8275],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.8431,  0.6941,  0.5608]],\n",
       " \n",
       "         [[-0.4902, -0.5843, -0.4196,  ..., -0.6549, -0.6549, -0.6549],\n",
       "          [-0.4980, -0.4902, -0.4667,  ..., -0.7098, -0.6863, -0.6235],\n",
       "          [-0.5137, -0.4353, -0.4745,  ..., -0.6863, -0.6000, -0.4196],\n",
       "          ...,\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.5451,  0.4118,  0.4588],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.5686,  0.4510,  0.4902],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.6157,  0.4039,  0.2078]],\n",
       " \n",
       "         [[-0.3725, -0.4039, -0.2157,  ..., -0.4431, -0.4039, -0.3804],\n",
       "          [-0.3569, -0.2941, -0.2078,  ..., -0.4667, -0.3725, -0.2706],\n",
       "          [-0.3490, -0.2157, -0.1843,  ..., -0.4196, -0.2627, -0.0745],\n",
       "          ...,\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.6078,  0.5137,  0.4980],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.5608,  0.4902,  0.4902],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.5608,  0.3961,  0.1843]]]),\n",
       " 0,\n",
       " 'C:/Users/Maods/Documents/Development/Mestrado/terumo/apps/renal-pathology-retrieval/data/02_data_split/train_data\\\\Podocitopatia\\\\With\\\\AZAN\\\\FIOCRUZ20190122 (1302).jpg',\n",
       " 'Podocitopatia')"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = data_loader.dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target = []\n",
    "paths = []\n",
    "labels = []\n",
    "for i, (x, y, path, label) in enumerate(dataloader):\n",
    "\n",
    "\n",
    "    target.extend(list(y.cpu().detach().numpy()))\n",
    "    paths.extend([i.split('/')[11].replace('\\\\','/') for i in path])\n",
    "    labels.extend(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12131"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path, label in zip(paths, labels):\n",
    "    # print(path.split('/')[1], label)\n",
    "    assert path.split('/')[1] == label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for path, y in zip(paths, target):\n",
    "    # print(path.split('/')[1], label)\n",
    "    assert path.split('/')[1] == mask[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, y in zip(labels, target):\n",
    "    # print(path.split('/')[1], label)\n",
    "    assert label == mask[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def load_embeddings(pickle_file_path):\n",
    "    with open(pickle_file_path, 'rb') as pickle_file:\n",
    "        loaded_data_dict = pickle.load(pickle_file)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    data = loaded_data_dict[\"embedding\"]\n",
    "    labels = np.array(loaded_data_dict[\"target\"])\n",
    "    return data, labels, loaded_data_dict\n",
    "\n",
    "path = '../data_output/embeddings/vgg16_4096_pretrained.pickle'\n",
    "\n",
    "data, labels, result = load_embeddings(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'embedding', 'target', 'paths', 'classes'])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path, label in zip(result['paths'], result['classes']):\n",
    "    # print(path.split('/')[0])\n",
    "    assert path.split('/')[0] == label, print(path.split('/')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y, label in zip(result['target'], result['classes']):\n",
    "    # print(path.split('/')[0])\n",
    "    assert mask[y] == label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y, path in zip(result['target'], result['paths']):\n",
    "    # print(path.split('/')[0])\n",
    "    assert mask[y] == path.split('/')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "base_path = \"C:/Users/Maods/Documents/Development/Mestrado/terumo/apps/renal-pathology-retrieval/data/01_raw/\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import numpy as np\n",
    "import requests\n",
    "from renumics import spotlight\n",
    "import json\n",
    "\n",
    "df = {\n",
    "    \"label\": result['classes'],\n",
    "    \"image\":[base_path+i for i in result['paths']],\n",
    "    \"path\":result['paths']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "reducer = umap.UMAP(n_components=3)\n",
    "reduced_embedding = reducer.fit_transform(result['embedding'])\n",
    "\n",
    "df[\"embedding_reduced\"] = np.array(reduced_embedding).tolist()\n",
    "df[\"embx\"] = [emb[0] for emb in df[\"embedding_reduced\"]]\n",
    "df[\"emby\"] = [emb[1] for emb in df[\"embedding_reduced\"]]\n",
    "\n",
    "layout_url = \"https://raw.githubusercontent.com/Renumics/spotlight/main/playbook/rookie/embedding_layout.json\"\n",
    "response = requests.get(layout_url)\n",
    "layout = spotlight.layout.nodes.Layout(**json.loads(response.text))\n",
    "spotlight.show(\n",
    "    df,\n",
    "    dtype={\"image\": spotlight.Image, \"embedding_reduced\": spotlight.Embedding},\n",
    "    layout=layout,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPModel, CLIPProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f2a8779ce14a0c960718ae60106b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maods\\.virtualenvs\\renal-pathology-retrieval-P_udDvkW\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Maods\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd4d95e7fab47e4932c758a48f13577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "model = CLIPModel.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.rand((1,3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = model.get_image_features(inputs)\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features.view(image_features.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "class TripletData(Dataset):\n",
    "    def __init__(self, path, transforms, split=\"train\"):\n",
    "        self.path = path\n",
    "        self.split = split    # train or valid\n",
    "        self.cats = 6       # number of categories\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # our positive class for the triplet\n",
    "        idx = str(idx%self.cats + 1)\n",
    "        \n",
    "        # choosing our pair of positive images (im1, im2)\n",
    "        positives = os.listdir(os.path.join(self.path, idx))\n",
    "        im1, im2 = random.sample(positives, 2)\n",
    "        \n",
    "        # choosing a negative class and negative image (im3)\n",
    "        negative_cats = [str(x+1) for x in range(self.cats)]\n",
    "        negative_cats.remove(idx)\n",
    "        negative_cat = str(random.choice(negative_cats))\n",
    "        negatives = os.listdir(os.path.join(self.path, negative_cat))\n",
    "        im3 = random.choice(negatives)\n",
    "        \n",
    "        im1,im2,im3 = os.path.join(self.path, idx, im1), os.path.join(self.path, idx, im2), os.path.join(self.path, negative_cat, im3)\n",
    "        \n",
    "        im1 = self.transforms(Image.open(im1))\n",
    "        im2 = self.transforms(Image.open(im2))\n",
    "        im3 = self.transforms(Image.open(im3))\n",
    "        \n",
    "        return [im1, im2, im3]\n",
    "        \n",
    "    # we'll put some value that we want since there can be far too many triplets possible\n",
    "    # multiples of the number of images/ number of categories is a good choice\n",
    "    def __len__(self):\n",
    "        return self.cats*8\n",
    "    \n",
    "\n",
    "# Transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "\n",
    "# Datasets and Dataloaders\n",
    "train_data = TripletData('../data/02_data_split/train_data', train_transforms)\n",
    "val_data = TripletData('../data/02_data_split/test_data', val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "path = '../data/02_data_split/train_data'\n",
    "split = 'train'    # train or valid\n",
    "cats = 6       # number of categories\n",
    "transforms = train_transforms\n",
    "\n",
    "idx=0\n",
    "# our positive class for the triplet\n",
    "idx = str(idx%cats + 1)\n",
    "# idx\n",
    "# choosing our pair of positive images (im1, im2)\n",
    "# positives = os.listdir(os.path.join(path, idx))\n",
    "# im1, im2 = random.sample(positives, 2)\n",
    "\n",
    "# # choosing a negative class and negative image (im3)\n",
    "negative_cats = [str(x+1) for x in range(cats)]\n",
    "negative_cats.remove(idx)\n",
    "negative_cat = str(random.choice(negative_cats))\n",
    "# negatives = os.listdir(os.path.join(path, negative_cat))\n",
    "# im3 = random.choice(negatives)\n",
    "\n",
    "# im1,im2,im3 = os.path.join(path, idx, im1), os.path.join(path, idx, im2), os.path.join(path, negative_cat, im3)\n",
    "\n",
    "# im1 = transforms(Image.open(im1))\n",
    "# im2 = transforms(Image.open(im2))\n",
    "# im3 = transforms(Image.open(im3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_cat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "renal-pathology-retrieval-P_udDvkW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
