{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FolderDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creates a PyTorch dataset from folder, returning two tensor images.\n",
    "    Args: \n",
    "    main_dir : directory where images are stored.\n",
    "    transform (optional) : torchvision transforms to be applied while making dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, main_dir, transform=None):\n",
    "        self.main_dir = main_dir\n",
    "        self.transform = transform\n",
    "        self.all_imgs = self._get_images_path(main_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_loc = os.path.join(self.main_dir, self.all_imgs[idx])\n",
    "        image = Image.open(img_loc).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            tensor_image = self.transform(image)\n",
    "\n",
    "        return tensor_image, tensor_image\n",
    "    \n",
    "    def _get_images_path(self, main_dir):\n",
    "        image_extensions = [\"jpg\", \"jpeg\", \"png\", \"gif\", \"bmp\", \"tiff\"]\n",
    "        data_dir = Path(main_dir)\n",
    "        images_path = [file for file in data_dir.glob(\n",
    "            '**/*') if file.suffix.lower()[1:] in image_extensions]\n",
    "        return images_path\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Convolutional Encoder Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, (3, 3), padding=(1, 1))\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.maxpool1 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, (3, 3), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.maxpool2 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, (3, 3), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.maxpool3 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "        self.conv4 = nn.Conv2d(64, 128, (3, 3), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "        self.maxpool4 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "        self.conv5 = nn.Conv2d(128, 256, (3, 3), padding=(1, 1))\n",
    "        self.relu5 = nn.ReLU(inplace=True)\n",
    "        self.maxpool5 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Downscale the image with conv maxpool etc.\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.maxpool3(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.maxpool4(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.maxpool5(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Convolutional Decoder Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.deconv1 = nn.ConvTranspose2d(256, 128, (2, 2), stride=(2, 2))\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, (2, 2), stride=(2, 2))\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, 32, (2, 2), stride=(2, 2))\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.deconv4 = nn.ConvTranspose2d(32, 16, (2, 2), stride=(2, 2))\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.deconv5 = nn.ConvTranspose2d(16, 3, (2, 2), stride=(2, 2))\n",
    "        self.relu5 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "         # Upscale the image with convtranspose etc.\n",
    "        x = self.deconv1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.deconv2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.deconv3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.deconv4(x)\n",
    "        x = self.relu4(x)\n",
    "\n",
    "        x = self.deconv5(x)\n",
    "        x = self.relu5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 7, 7])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = ConvEncoder()\n",
    "model(torch.rand((1,3,224,224))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "# model._avg_pooling = Identity()\n",
    "# model._dropout = Identity()\n",
    "model._fc = Identity()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nn.Sequential(*list(model.children())[:-2])\n",
    "model._avg_pooling = nn.AdaptiveAvgPool2d(1)\n",
    "model._fc = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(1280, 256 * 7 * 7)  # Adjust based on the specific EfficientNet variant\n",
    ")\n",
    "model._fc=Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([150, 256, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "class CustomEncoder(nn.Module):\n",
    "    def __init__(self, model,embedding_size):\n",
    "        super(CustomEncoder, self).__init__()\n",
    "        self.encoder = model\n",
    "        self.encoder._avg_pooling = nn.AdaptiveAvgPool2d(1)  # Modify average pooling\n",
    "        self.encoder._fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(embedding_size, 256 * 7 * 7)  # Adjust based on the specific EfficientNet variant\n",
    "        )  # Modify the output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x).view(x.shape[0], 256, 7, 7)\n",
    "\n",
    "# Create an instance of the custom encoder\n",
    "custom_encoder = CustomEncoder(model=model, embedding_size=1280)\n",
    "\n",
    "# Test the encoder with a random input of size (batch_size, channels, height, width)\n",
    "input_tensor = torch.randn(150, 3, 224, 224)  # Adjust the input size as needed\n",
    "output = custom_encoder(input_tensor)\n",
    "print(output.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(encoder, decoder, train_loader, loss_fn, optimizer, device):\n",
    "    \"\"\"\n",
    "    Performs a single training step\n",
    "    Args:\n",
    "    encoder: A convolutional Encoder. E.g. torch_model ConvEncoder\n",
    "    decoder: A convolutional Decoder. E.g. torch_model ConvDecoder\n",
    "    train_loader: PyTorch dataloader, containing (images, images).\n",
    "    loss_fn: PyTorch loss_fn, computes loss between 2 images.\n",
    "    optimizer: PyTorch optimizer.\n",
    "    device: \"cuda\" or \"cpu\"\n",
    "    Returns: Train Loss\n",
    "    \"\"\"\n",
    "    #  Set networks to train mode.\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    for batch_idx, (train_img, target_img) in enumerate(train_loader):\n",
    "        # Move images to device\n",
    "        train_img = train_img.to(device)\n",
    "        target_img = target_img.to(device)\n",
    "        \n",
    "        # Zero grad the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # Feed the train images to encoder\n",
    "        enc_output = encoder(train_img)\n",
    "        # The output of encoder is input to decoder !\n",
    "        dec_output = decoder(enc_output)\n",
    "        \n",
    "        # Decoder output is reconstructed image\n",
    "        # Compute loss with it and orginal image which is target image.\n",
    "        loss = loss_fn(dec_output, target_img)\n",
    "        # Backpropogate\n",
    "        loss.backward()\n",
    "        # Apply the optimizer to network by calling step.\n",
    "        optimizer.step()\n",
    "    # Return the loss\n",
    "    return loss.item()\n",
    "\n",
    "def val_step(encoder, decoder, val_loader, loss_fn, device):\n",
    "    \"\"\"\n",
    "    Performs a single training step\n",
    "    Args:\n",
    "    encoder: A convolutional Encoder. E.g. torch_model ConvEncoder\n",
    "    decoder: A convolutional Decoder. E.g. torch_model ConvDecoder\n",
    "    val_loader: PyTorch dataloader, containing (images, images).\n",
    "    loss_fn: PyTorch loss_fn, computes loss between 2 images.\n",
    "    device: \"cuda\" or \"cpu\"\n",
    "    Returns: Validation Loss\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set to eval mode.\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    # We don't need to compute gradients while validating.\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (train_img, target_img) in enumerate(val_loader):\n",
    "            # Move to device\n",
    "            train_img = train_img.to(device)\n",
    "            target_img = target_img.to(device)\n",
    "\n",
    "            # Again as train. Feed encoder the train image.\n",
    "            enc_output = encoder(train_img)\n",
    "            # Decoder takes encoder output and reconstructs the image.\n",
    "            dec_output = decoder(enc_output)\n",
    "\n",
    "            # Validation loss for encoder and decoder.\n",
    "            loss = loss_fn(dec_output, target_img)\n",
    "    # Return the loss\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs = 0, Training Loss : 0.526820182800293\n",
      "Epochs = 0, Validation Loss : 0.4606035351753235\n",
      "Validation Loss decreased, saving new best model\n",
      "Epochs = 1, Training Loss : 0.3397727310657501\n",
      "Epochs = 1, Validation Loss : 0.4182150959968567\n",
      "Validation Loss decreased, saving new best model\n",
      "Epochs = 2, Training Loss : 0.2586904764175415\n",
      "Epochs = 2, Validation Loss : 0.389679491519928\n",
      "Validation Loss decreased, saving new best model\n",
      "Epochs = 3, Training Loss : 0.21819019317626953\n",
      "Epochs = 3, Validation Loss : 0.3790033459663391\n",
      "Validation Loss decreased, saving new best model\n",
      "Epochs = 4, Training Loss : 0.2112281173467636\n",
      "Epochs = 4, Validation Loss : 0.3613903522491455\n",
      "Validation Loss decreased, saving new best model\n",
      "Epochs = 5, Training Loss : 0.3766658306121826\n",
      "Epochs = 5, Validation Loss : 0.3549455404281616\n",
      "Validation Loss decreased, saving new best model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Maods\\Documents\\Development\\Mestrado\\terumo\\apps\\renal-pathology-retrieval\\notebooks\\autoencoder_training.ipynb Cell 11\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Maods/Documents/Development/Mestrado/terumo/apps/renal-pathology-retrieval/notebooks/autoencoder_training.ipynb#X13sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# Usual Training Loop\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Maods/Documents/Development/Mestrado/terumo/apps/renal-pathology-retrieval/notebooks/autoencoder_training.ipynb#X13sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Maods/Documents/Development/Mestrado/terumo/apps/renal-pathology-retrieval/notebooks/autoencoder_training.ipynb#X13sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m         train_loss \u001b[39m=\u001b[39m train_step(encoder, decoder, train_loader, loss_fn, optimizer, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Maods/Documents/Development/Mestrado/terumo/apps/renal-pathology-retrieval/notebooks/autoencoder_training.ipynb#X13sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpochs = \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, Training Loss : \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Maods/Documents/Development/Mestrado/terumo/apps/renal-pathology-retrieval/notebooks/autoencoder_training.ipynb#X13sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m         val_loss \u001b[39m=\u001b[39m val_step(encoder, decoder, val_loader, loss_fn, device\u001b[39m=\u001b[39mdevice)\n",
      "\u001b[1;32mc:\\Users\\Maods\\Documents\\Development\\Mestrado\\terumo\\apps\\renal-pathology-retrieval\\notebooks\\autoencoder_training.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Maods/Documents/Development/Mestrado/terumo/apps/renal-pathology-retrieval/notebooks/autoencoder_training.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m encoder\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Maods/Documents/Development/Mestrado/terumo/apps/renal-pathology-retrieval/notebooks/autoencoder_training.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m decoder\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Maods/Documents/Development/Mestrado/terumo/apps/renal-pathology-retrieval/notebooks/autoencoder_training.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (train_img, target_img) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Maods/Documents/Development/Mestrado/terumo/apps/renal-pathology-retrieval/notebooks/autoencoder_training.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m# Move images to device\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Maods/Documents/Development/Mestrado/terumo/apps/renal-pathology-retrieval/notebooks/autoencoder_training.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     train_img \u001b[39m=\u001b[39m train_img\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Maods/Documents/Development/Mestrado/terumo/apps/renal-pathology-retrieval/notebooks/autoencoder_training.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     target_img \u001b[39m=\u001b[39m target_img\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\Maods\\.virtualenvs\\renal-pathology-retrieval-P_udDvkW\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Maods\\.virtualenvs\\renal-pathology-retrieval-P_udDvkW\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Maods\\.virtualenvs\\renal-pathology-retrieval-P_udDvkW\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Maods\\.virtualenvs\\renal-pathology-retrieval-P_udDvkW\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Maods\\.virtualenvs\\renal-pathology-retrieval-P_udDvkW\\lib\\site-packages\\torch\\utils\\data\\dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "\u001b[1;32mc:\\Users\\Maods\\Documents\\Development\\Mestrado\\terumo\\apps\\renal-pathology-retrieval\\notebooks\\autoencoder_training.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Maods/Documents/Development/Mestrado/terumo/apps/renal-pathology-retrieval/notebooks/autoencoder_training.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Maods/Documents/Development/Mestrado/terumo/apps/renal-pathology-retrieval/notebooks/autoencoder_training.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     img_loc \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmain_dir, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_imgs[idx])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Maods/Documents/Development/Mestrado/terumo/apps/renal-pathology-retrieval/notebooks/autoencoder_training.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(img_loc)\u001b[39m.\u001b[39;49mconvert(\u001b[39m\"\u001b[39;49m\u001b[39mRGB\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Maods/Documents/Development/Mestrado/terumo/apps/renal-pathology-retrieval/notebooks/autoencoder_training.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Maods/Documents/Development/Mestrado/terumo/apps/renal-pathology-retrieval/notebooks/autoencoder_training.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         tensor_image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(image)\n",
      "File \u001b[1;32mc:\\Users\\Maods\\.virtualenvs\\renal-pathology-retrieval-P_udDvkW\\lib\\site-packages\\PIL\\Image.py:911\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert\u001b[39m(\n\u001b[0;32m    864\u001b[0m     \u001b[39mself\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, matrix\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dither\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, palette\u001b[39m=\u001b[39mPalette\u001b[39m.\u001b[39mWEB, colors\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m\n\u001b[0;32m    865\u001b[0m ):\n\u001b[0;32m    866\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    867\u001b[0m \u001b[39m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    868\u001b[0m \u001b[39m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[39m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    909\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 911\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m    913\u001b[0m     has_transparency \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtransparency\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    914\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mode \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mP\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    915\u001b[0m         \u001b[39m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Maods\\.virtualenvs\\renal-pathology-retrieval-P_udDvkW\\lib\\site-packages\\PIL\\ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(msg)\n\u001b[0;32m    268\u001b[0m b \u001b[39m=\u001b[39m b \u001b[39m+\u001b[39m s\n\u001b[1;32m--> 269\u001b[0m n, err_code \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(b)\n\u001b[0;32m    270\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    271\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "\n",
    "transforms = T.Compose([T.ToTensor()]) # Normalize the pixels and convert to tensor.\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                            (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "path= \"C:/Users/Maods/Documents/Development/Mestrado/terumo/apps/renal-pathology-retrieval/data/02_data_split/train_data/\"\n",
    "full_dataset = FolderDataset(path, transform) # Create folder dataset.\n",
    "\n",
    "train_size = 0.75\n",
    "val_size = 1 - train_size\n",
    "\n",
    "# Split data to train and test\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size]) \n",
    "\n",
    "# Create the train dataloader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    " \n",
    "# Create the validation dataloader\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "\n",
    "\n",
    "loss_fn = nn.MSELoss() # We use Mean squared loss which computes difference between two images.\n",
    "\n",
    "encoder = ConvEncoder() # Our encoder model\n",
    "decoder = ConvDecoder() # Our decoder model\n",
    "\n",
    "device = \"cuda\"  # GPU device\n",
    "\n",
    "# Shift models to GPU\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Both the enocder and decoder parameters\n",
    "autoencoder_params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "optimizer = optim.Adam(autoencoder_params, lr=1e-3) # Adam Optimizer\n",
    "\n",
    "max_loss = sys.maxsize\n",
    "\n",
    "# Time to Train !!!\n",
    "EPOCHS = 10\n",
    "# Usual Training Loop\n",
    "for epoch in range(EPOCHS):\n",
    "        train_loss = train_step(encoder, decoder, train_loader, loss_fn, optimizer, device=device)\n",
    "        \n",
    "        print(f\"Epochs = {epoch}, Training Loss : {train_loss}\")\n",
    "        \n",
    "        val_loss = val_step(encoder, decoder, val_loader, loss_fn, device=device)\n",
    "        \n",
    "        print(f\"Epochs = {epoch}, Validation Loss : {val_loss}\")\n",
    "\n",
    "        # Simple Best Model saving\n",
    "        if val_loss < max_loss:\n",
    "            print(\"Validation Loss decreased, saving new best model\")\n",
    "            torch.save(encoder.state_dict(), \"encoder_model.pt\")\n",
    "            torch.save(decoder.state_dict(), \"decoder_model.pt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the full dataloader\n",
    "# full_loader = torch.utils.data.DataLoader(full_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Epochs = 0, Training Loss : 0.713015079498291\n",
      "Epochs = 0, Validation Loss : 0.7330640554428101\n",
      "Validation Loss decreased, saving new best model\n",
      "Epochs = 1, Training Loss : 0.34706607460975647\n",
      "Epochs = 1, Validation Loss : 0.7095859050750732\n",
      "Validation Loss decreased, saving new best model\n",
      "Epochs = 2, Training Loss : 0.5655218362808228\n",
      "Epochs = 2, Validation Loss : 0.6460226774215698\n",
      "Validation Loss decreased, saving new best model\n",
      "Epochs = 3, Training Loss : 0.48382991552352905\n",
      "Epochs = 3, Validation Loss : 0.6189921498298645\n",
      "Validation Loss decreased, saving new best model\n",
      "Epochs = 4, Training Loss : 0.40977224707603455\n",
      "Epochs = 4, Validation Loss : 0.6004781126976013\n",
      "Validation Loss decreased, saving new best model\n"
     ]
    }
   ],
   "source": [
    "full_dataset = FolderDataset(path, transform) # Create folder dataset.\n",
    "\n",
    "train_size = 0.75\n",
    "val_size = 1 - train_size\n",
    "\n",
    "# Split data to train and test\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size]) \n",
    "\n",
    "# Create the train dataloader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    " \n",
    "# Create the validation dataloader\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "\n",
    "\n",
    "loss_fn = nn.MSELoss() # We use Mean squared loss which computes difference between two images.\n",
    "\n",
    "encoder = CustomEncoder(model=EfficientNet.from_pretrained('efficientnet-b0'), embedding_size=1280) # Our encoder model\n",
    "decoder = ConvDecoder() # Our decoder model\n",
    "\n",
    "device = \"cuda\"  # GPU device\n",
    "\n",
    "# Shift models to GPU\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Both the enocder and decoder parameters\n",
    "autoencoder_params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "optimizer = optim.Adam(autoencoder_params, lr=1e-3) # Adam Optimizer\n",
    "\n",
    "max_loss = sys.maxsize\n",
    "\n",
    "# Time to Train !!!\n",
    "EPOCHS = 5\n",
    "# Usual Training Loop\n",
    "for epoch in range(EPOCHS):\n",
    "        train_loss = train_step(encoder, decoder, train_loader, loss_fn, optimizer, device=device)\n",
    "        \n",
    "        print(f\"Epochs = {epoch}, Training Loss : {train_loss}\")\n",
    "        \n",
    "        val_loss = val_step(encoder, decoder, val_loader, loss_fn, device=device)\n",
    "        \n",
    "        print(f\"Epochs = {epoch}, Validation Loss : {val_loss}\")\n",
    "\n",
    "        # Simple Best Model saving\n",
    "        if val_loss < max_loss:\n",
    "            print(\"Validation Loss decreased, saving new best model\")\n",
    "            torch.save(encoder.state_dict(), \"encoder_model.pt\")\n",
    "            torch.save(decoder.state_dict(), \"decoder_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoder = ConvEncoder()\n",
    "encoder(torch.rand((1,3,224,224)))\n",
    "\n",
    "\n",
    "encoder = CustomEncoder(model=EfficientNet.from_pretrained('efficientnet-b0'), embedding_size=1280)\n",
    "output =encoder(torch.rand((40,3,224,224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 12544])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.reshape((output.shape[0], -1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "encoder = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "encoder._fc = Identity()\n",
    "output = encoder(torch.rand((40,3,224,224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 1280])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.reshape((output.shape[0], -1)).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "renal-pathology-retrieval-P_udDvkW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
