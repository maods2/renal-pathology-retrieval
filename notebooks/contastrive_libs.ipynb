{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from lightly import loss\n",
    "from lightly import transforms\n",
    "from lightly.data import LightlyDataset\n",
    "from lightly.models.modules import heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.46061\n",
      "loss: 5.48240\n",
      "loss: 5.48412\n",
      "loss: 5.45836\n",
      "loss: 5.42060\n",
      "loss: 5.48608\n",
      "loss: 5.38136\n",
      "loss: 5.36370\n",
      "loss: 5.38447\n",
      "loss: 5.51065\n",
      "loss: 5.39140\n",
      "loss: 5.35425\n",
      "loss: 5.43354\n",
      "loss: 5.31290\n",
      "loss: 5.33749\n",
      "loss: 5.38600\n",
      "loss: 5.45125\n",
      "loss: 5.42239\n",
      "loss: 5.39240\n",
      "loss: 5.41819\n",
      "loss: 5.35401\n",
      "loss: 5.41428\n",
      "loss: 5.31243\n",
      "loss: 5.32486\n",
      "loss: 5.19039\n",
      "loss: 5.45044\n",
      "loss: 5.40671\n",
      "loss: 5.36622\n",
      "loss: 5.39603\n",
      "loss: 5.25069\n",
      "loss: 5.21410\n",
      "loss: 5.29823\n",
      "loss: 5.25373\n",
      "loss: 5.23811\n",
      "loss: 5.21855\n",
      "loss: 5.23667\n",
      "loss: 5.18649\n",
      "loss: 5.33392\n",
      "loss: 5.15611\n",
      "loss: 5.34880\n",
      "loss: 5.18890\n",
      "loss: 5.20524\n",
      "loss: 5.12260\n",
      "loss: 5.31541\n",
      "loss: 5.32747\n",
      "loss: 5.21321\n",
      "loss: 5.13281\n",
      "loss: 5.21041\n",
      "loss: 5.34033\n",
      "loss: 5.26621\n",
      "loss: 5.22065\n",
      "loss: 5.42814\n",
      "loss: 5.16786\n",
      "loss: 5.19425\n",
      "loss: 5.35930\n",
      "loss: 5.17207\n",
      "loss: 5.22336\n",
      "loss: 5.29948\n",
      "loss: 5.28620\n",
      "loss: 5.28562\n",
      "loss: 5.34113\n",
      "loss: 5.31198\n",
      "loss: 5.19699\n",
      "loss: 5.26075\n",
      "loss: 5.24062\n",
      "loss: 5.27193\n",
      "loss: 5.27743\n",
      "loss: 5.22130\n",
      "loss: 5.18495\n",
      "loss: 5.28786\n",
      "loss: 5.17443\n",
      "loss: 5.29876\n",
      "loss: 5.24311\n",
      "loss: 5.24284\n",
      "loss: 5.17196\n",
      "loss: 5.21128\n",
      "loss: 5.27616\n",
      "loss: 5.12278\n",
      "loss: 5.24532\n",
      "loss: 5.29330\n",
      "loss: 5.25504\n",
      "loss: 5.17945\n",
      "loss: 5.18056\n",
      "loss: 5.17594\n",
      "loss: 5.28362\n",
      "loss: 5.13320\n",
      "loss: 5.09152\n",
      "loss: 5.13655\n",
      "loss: 5.25466\n",
      "loss: 5.28368\n",
      "loss: 5.14735\n",
      "loss: 5.18650\n",
      "loss: 5.15917\n",
      "loss: 5.21185\n",
      "loss: 5.01568\n",
      "loss: 5.11324\n",
      "loss: 5.26927\n",
      "loss: 5.24473\n",
      "loss: 5.16605\n",
      "loss: 5.19743\n",
      "loss: 5.13708\n",
      "loss: 5.26103\n",
      "loss: 5.41906\n",
      "loss: 5.26408\n",
      "loss: 5.09756\n",
      "loss: 5.24537\n",
      "loss: 5.20299\n",
      "loss: 5.29761\n",
      "loss: 5.22056\n",
      "loss: 5.15070\n",
      "loss: 5.14728\n",
      "loss: 5.27043\n",
      "loss: 5.28386\n",
      "loss: 5.27463\n",
      "loss: 5.13224\n",
      "loss: 5.15254\n",
      "loss: 5.24887\n",
      "loss: 5.23464\n",
      "loss: 5.25860\n",
      "loss: 5.29132\n",
      "loss: 5.11039\n",
      "loss: 5.08440\n",
      "loss: 5.17821\n",
      "loss: 5.27687\n",
      "loss: 5.04987\n",
      "loss: 5.12811\n",
      "loss: 5.32987\n",
      "loss: 5.34915\n",
      "loss: 5.15696\n",
      "loss: 5.07815\n",
      "loss: 5.18012\n",
      "loss: 5.17235\n",
      "loss: 5.24449\n",
      "loss: 5.30919\n",
      "loss: 5.06922\n",
      "loss: 5.26775\n",
      "loss: 5.21904\n",
      "loss: 5.14148\n",
      "loss: 5.07563\n",
      "loss: 5.09962\n",
      "loss: 5.07530\n",
      "loss: 5.18358\n",
      "loss: 5.14852\n",
      "loss: 5.22404\n",
      "loss: 5.15860\n",
      "loss: 5.22579\n",
      "loss: 5.18850\n",
      "loss: 5.29307\n",
      "loss: 5.22649\n",
      "loss: 5.10267\n",
      "loss: 5.19639\n",
      "loss: 5.25613\n",
      "loss: 5.15998\n",
      "loss: 5.09351\n",
      "loss: 5.13600\n",
      "loss: 5.17912\n",
      "loss: 5.08761\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 55\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Maods\\.virtualenvs\\renal-pathology-retrieval-P_udDvkW\\lib\\site-packages\\torch\\optim\\optimizer.py:456\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m foreach:\n\u001b[0;32m    455\u001b[0m     per_device_and_dtype_grads \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28;01mlambda\u001b[39;00m: defaultdict(\u001b[38;5;28mlist\u001b[39m))\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_zero_grad_profile_name):\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\Maods\\.virtualenvs\\renal-pathology-retrieval-P_udDvkW\\lib\\site-packages\\torch\\autograd\\profiler.py:507\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[1;32m--> 507\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_exit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RecordFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    509\u001b[0m     torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_exit(record)\n",
      "File \u001b[1;32mc:\\Users\\Maods\\.virtualenvs\\renal-pathology-retrieval-P_udDvkW\\lib\\site-packages\\torch\\_ops.py:287\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs \u001b[38;5;129;01mor\u001b[39;00m {})\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a PyTorch module for the SimCLR model.\n",
    "class SimCLR(torch.nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projection_head = heads.SimCLRProjectionHead(\n",
    "            input_dim=512,  # Resnet18 features have 512 dimensions.\n",
    "            hidden_dim=512,\n",
    "            output_dim=128,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(features)\n",
    "        return z\n",
    "\n",
    "\n",
    "# Use a resnet backbone.\n",
    "backbone = torchvision.models.resnet18()\n",
    "# Ignore the classification head as we only want the features.\n",
    "backbone.fc = torch.nn.Identity()\n",
    "\n",
    "# Build the SimCLR model.\n",
    "model = SimCLR(backbone)\n",
    "\n",
    "\n",
    "# Prepare transform that creates multiple random views for every image.\n",
    "transform = transforms.SimCLRTransform(input_size=32, cj_prob=0.5)\n",
    "\n",
    "\n",
    "# Create a dataset from your image folder.\n",
    "dataset = LightlyDataset(input_dir=\"../data/02_data_split/train_data/\", transform=transform)\n",
    "\n",
    "# Build a PyTorch dataloader.\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,  # Pass the dataset to the dataloader.\n",
    "    batch_size=128,  # A large batch size helps with the learning.\n",
    "    shuffle=True,  # Shuffling is important!\n",
    ")\n",
    "\n",
    "# Lightly exposes building blocks such as loss functions.\n",
    "criterion = loss.NTXentLoss(temperature=0.5)\n",
    "\n",
    "# Get a PyTorch optimizer.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-6)\n",
    "\n",
    "# Train the model.\n",
    "for epoch in range(10):\n",
    "    for (view0, view1), targets, filenames in dataloader:\n",
    "        z0 = model(view0)\n",
    "        z1 = model(view1)\n",
    "        loss = criterion(z0, z1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(f\"loss: {loss.item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "# class Identity(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Identity, self).__init__()\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         return x\n",
    "\n",
    "# class CustomEncoder(nn.Module):\n",
    "#     def __init__(self, model,embedding_size=1280):\n",
    "#         super(CustomEncoder, self).__init__()\n",
    "#         self.encoder = model\n",
    "#         self.encoder._avg_pooling = nn.AdaptiveAvgPool2d(1)  # Modify average pooling\n",
    "#         self.encoder._fc = nn.Sequential(\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(embedding_size, 256 * 7 * 7)  # Adjust based on the specific EfficientNet variant\n",
    "#         )  # Modify the output layer\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.encoder(x).view(x.shape[0], 256, 7, 7)\n",
    "    \n",
    "# def get_efficient_netb0():\n",
    "#     model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "#     model._fc = Identity()\n",
    "\n",
    "\n",
    "#     return model\n",
    "\n",
    "# def get_efficient_netb0_encoder():\n",
    "#     model = CustomEncoder(\n",
    "#         model=EfficientNet.from_pretrained('efficientnet-b0'), \n",
    "#         embedding_size=1280\n",
    "#         )\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "# Create a PyTorch module for the SimCLR model.\n",
    "class SimCLR(torch.nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projection_head = heads.SimCLRProjectionHead(\n",
    "            input_dim=512,  # Resnet18 features have 512 dimensions.\n",
    "            hidden_dim=512,\n",
    "            output_dim=128,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(features)\n",
    "        return z\n",
    "\n",
    "\n",
    "# Use a resnet backbone.\n",
    "backbone = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "# Ignore the classification head as we only want the features.\n",
    "backbone._fc = torch.nn.Identity()\n",
    "\n",
    "# Build the SimCLR model.\n",
    "model = SimCLR(backbone)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "loss: 5.50475\n",
      "loss: 5.51909\n",
      "loss: 5.52904\n",
      "loss: 5.51790\n",
      "loss: 5.53173\n",
      "loss: 5.49575\n",
      "loss: 5.51858\n",
      "loss: 5.53091\n",
      "loss: 5.48558\n",
      "loss: 5.46340\n",
      "loss: 5.45240\n",
      "loss: 5.47274\n",
      "loss: 5.50354\n",
      "loss: 5.50478\n",
      "loss: 5.49191\n",
      "loss: 5.47412\n",
      "loss: 5.44785\n",
      "loss: 5.42565\n",
      "loss: 5.51401\n",
      "loss: 5.45893\n",
      "loss: 5.44463\n",
      "loss: 5.43077\n",
      "loss: 5.41673\n",
      "loss: 5.47465\n",
      "loss: 5.43301\n",
      "loss: 5.41797\n",
      "loss: 5.43376\n",
      "loss: 5.45916\n",
      "loss: 5.38749\n",
      "loss: 5.41056\n",
      "loss: 5.35334\n",
      "loss: 5.35617\n",
      "loss: 5.42437\n",
      "loss: 5.40366\n",
      "loss: 5.38953\n",
      "loss: 5.35424\n",
      "loss: 5.41400\n",
      "loss: 5.35691\n",
      "loss: 5.41279\n",
      "loss: 5.42426\n",
      "loss: 5.45534\n",
      "loss: 5.37404\n",
      "loss: 5.36493\n",
      "loss: 5.34767\n",
      "loss: 5.40132\n",
      "loss: 5.31602\n",
      "loss: 5.37155\n",
      "loss: 5.38717\n",
      "loss: 5.34566\n",
      "loss: 5.36895\n",
      "loss: 5.35160\n",
      "loss: 5.42077\n",
      "loss: 5.33988\n",
      "loss: 5.32502\n",
      "loss: 5.31049\n",
      "loss: 5.33960\n",
      "loss: 5.36135\n",
      "loss: 5.29119\n",
      "loss: 5.35251\n",
      "loss: 5.29370\n",
      "loss: 5.27821\n",
      "loss: 5.31126\n",
      "loss: 5.33855\n",
      "loss: 5.30880\n",
      "loss: 5.26585\n",
      "loss: 5.25315\n",
      "loss: 5.31130\n",
      "loss: 5.26712\n",
      "loss: 5.32240\n",
      "loss: 5.29432\n",
      "loss: 5.21763\n",
      "loss: 5.23851\n",
      "loss: 5.34727\n",
      "loss: 5.34732\n",
      "loss: 5.25144\n",
      "loss: 5.23767\n",
      "loss: 5.29244\n",
      "loss: 5.36769\n",
      "loss: 5.27898\n",
      "loss: 5.25829\n",
      "loss: 5.25916\n",
      "loss: 5.26426\n",
      "loss: 5.26545\n",
      "loss: 5.31731\n",
      "loss: 5.26851\n",
      "loss: 5.23805\n",
      "loss: 5.33766\n",
      "loss: 5.26882\n",
      "loss: 5.30104\n",
      "loss: 5.25088\n",
      "loss: 5.27816\n",
      "loss: 5.36790\n",
      "loss: 5.16281\n",
      "loss: 5.28304\n",
      "loss: 4.96840\n",
      "loss: 5.23495\n",
      "loss: 5.23375\n",
      "loss: 5.19603\n",
      "loss: 5.27680\n",
      "loss: 5.22345\n",
      "loss: 5.19739\n",
      "loss: 5.21859\n",
      "loss: 5.21448\n",
      "loss: 5.33250\n",
      "loss: 5.24118\n",
      "loss: 5.23815\n",
      "loss: 5.24163\n",
      "loss: 5.34775\n",
      "loss: 5.24289\n",
      "loss: 5.10970\n",
      "loss: 5.24979\n",
      "loss: 5.25785\n",
      "loss: 5.20410\n",
      "loss: 5.23827\n",
      "loss: 5.18586\n",
      "loss: 5.22474\n",
      "loss: 5.19527\n",
      "loss: 5.24346\n",
      "loss: 5.19151\n",
      "loss: 5.25406\n",
      "loss: 5.27880\n",
      "loss: 5.21557\n",
      "loss: 5.20987\n",
      "loss: 5.24981\n",
      "loss: 5.12666\n",
      "loss: 5.29347\n",
      "loss: 5.25444\n",
      "loss: 5.17906\n",
      "loss: 5.15016\n",
      "loss: 5.21453\n",
      "loss: 5.26425\n",
      "loss: 5.13081\n",
      "loss: 5.20321\n",
      "loss: 5.10347\n",
      "loss: 5.27999\n",
      "loss: 5.11571\n",
      "loss: 5.16805\n",
      "loss: 5.14394\n",
      "loss: 5.24983\n",
      "loss: 5.12932\n",
      "loss: 5.14470\n",
      "loss: 5.22811\n",
      "loss: 5.17189\n",
      "loss: 5.16156\n",
      "loss: 5.07418\n",
      "loss: 5.15989\n",
      "loss: 5.10964\n",
      "loss: 5.16882\n",
      "loss: 5.11898\n",
      "loss: 5.14390\n",
      "loss: 5.28333\n",
      "loss: 5.24870\n",
      "loss: 5.29389\n",
      "loss: 5.13892\n",
      "loss: 5.21731\n",
      "loss: 5.13492\n",
      "loss: 5.17589\n",
      "loss: 5.22327\n",
      "loss: 5.17909\n",
      "loss: 5.14335\n",
      "loss: 5.13465\n",
      "loss: 5.17674\n",
      "loss: 5.13974\n",
      "loss: 5.06595\n",
      "loss: 5.10100\n",
      "loss: 5.07383\n",
      "loss: 5.16168\n",
      "loss: 5.15439\n",
      "loss: 5.21358\n",
      "loss: 5.06204\n",
      "loss: 5.11681\n",
      "loss: 5.12439\n",
      "loss: 5.12796\n",
      "loss: 5.14495\n",
      "loss: 5.17582\n",
      "loss: 5.10242\n",
      "loss: 5.26125\n",
      "loss: 5.10602\n",
      "loss: 5.15298\n",
      "loss: 5.24217\n",
      "loss: 5.14628\n",
      "loss: 5.14695\n",
      "loss: 5.16913\n",
      "loss: 5.16762\n",
      "loss: 5.12792\n",
      "loss: 5.18101\n",
      "loss: 5.08265\n",
      "loss: 5.19598\n",
      "loss: 5.17135\n",
      "loss: 4.83310\n",
      "loss: 5.10971\n",
      "loss: 5.03502\n",
      "loss: 5.21438\n",
      "loss: 5.24844\n",
      "loss: 5.14442\n",
      "loss: 5.17030\n",
      "loss: 5.08980\n",
      "loss: 5.04122\n",
      "loss: 5.08592\n",
      "loss: 5.09316\n",
      "loss: 5.13400\n",
      "loss: 5.05797\n",
      "loss: 5.03685\n",
      "loss: 5.04014\n",
      "loss: 5.16148\n",
      "loss: 5.19618\n",
      "loss: 5.12134\n",
      "loss: 5.06220\n",
      "loss: 5.03197\n",
      "loss: 5.19392\n",
      "loss: 5.06072\n",
      "loss: 5.14831\n",
      "loss: 5.02791\n",
      "loss: 5.01339\n",
      "loss: 5.03206\n",
      "loss: 5.17566\n",
      "loss: 5.14780\n",
      "loss: 5.13394\n",
      "loss: 5.06963\n",
      "loss: 5.08400\n",
      "loss: 5.17003\n",
      "loss: 5.12685\n",
      "loss: 5.00350\n",
      "loss: 5.09927\n",
      "loss: 5.06999\n",
      "loss: 5.08304\n",
      "loss: 5.01288\n",
      "loss: 5.06628\n",
      "loss: 4.97768\n",
      "loss: 4.96063\n",
      "loss: 5.15714\n",
      "loss: 5.04582\n",
      "loss: 5.12169\n",
      "loss: 5.11022\n",
      "loss: 5.03539\n",
      "loss: 5.09079\n",
      "loss: 5.10705\n",
      "loss: 5.08486\n",
      "loss: 5.16416\n",
      "loss: 5.22446\n",
      "loss: 5.09624\n",
      "loss: 4.97817\n",
      "loss: 5.03234\n",
      "loss: 5.04695\n",
      "loss: 5.04241\n",
      "loss: 5.08484\n",
      "loss: 4.94865\n",
      "loss: 4.98726\n",
      "loss: 5.04014\n",
      "loss: 4.98956\n",
      "loss: 4.98183\n",
      "loss: 4.96491\n",
      "loss: 5.09516\n",
      "loss: 5.01324\n",
      "loss: 5.00578\n",
      "loss: 4.96213\n",
      "loss: 5.13053\n",
      "loss: 5.10208\n",
      "loss: 4.99560\n",
      "loss: 4.97163\n",
      "loss: 5.03118\n",
      "loss: 4.96109\n",
      "loss: 4.98113\n",
      "loss: 5.19919\n",
      "loss: 5.04561\n",
      "loss: 5.12345\n",
      "loss: 5.02705\n",
      "loss: 4.99046\n",
      "loss: 4.96573\n",
      "loss: 4.96487\n",
      "loss: 5.07607\n",
      "loss: 5.02365\n",
      "loss: 5.06355\n",
      "loss: 5.07012\n",
      "loss: 4.99777\n",
      "loss: 5.02578\n",
      "loss: 4.98071\n",
      "loss: 4.95491\n",
      "loss: 5.03798\n",
      "loss: 4.95212\n",
      "loss: 4.90078\n",
      "loss: 4.99162\n",
      "loss: 4.96810\n",
      "loss: 5.03965\n",
      "loss: 4.81200\n",
      "loss: 4.96027\n",
      "loss: 5.01895\n",
      "loss: 5.08426\n",
      "loss: 5.04482\n",
      "loss: 4.99471\n",
      "loss: 5.14309\n",
      "loss: 4.94260\n",
      "loss: 4.96117\n",
      "loss: 4.97786\n",
      "loss: 5.03771\n",
      "loss: 5.00207\n",
      "loss: 4.93534\n",
      "loss: 4.95269\n",
      "loss: 4.97276\n",
      "loss: 5.04707\n",
      "loss: 4.94894\n",
      "loss: 5.06124\n",
      "loss: 4.96404\n",
      "loss: 4.97379\n",
      "loss: 5.03096\n",
      "loss: 4.97558\n",
      "loss: 5.05808\n",
      "loss: 4.95282\n",
      "loss: 4.98456\n",
      "loss: 5.06032\n",
      "loss: 5.05690\n",
      "loss: 4.97901\n",
      "loss: 4.96513\n",
      "loss: 5.13352\n",
      "loss: 5.01180\n",
      "loss: 5.06327\n",
      "loss: 5.04388\n",
      "loss: 5.02709\n",
      "loss: 5.01733\n",
      "loss: 4.97671\n",
      "loss: 5.01009\n",
      "loss: 5.02370\n",
      "loss: 5.00706\n",
      "loss: 5.01539\n",
      "loss: 4.92529\n",
      "loss: 5.02799\n",
      "loss: 4.86484\n",
      "loss: 4.94006\n",
      "loss: 4.90147\n",
      "loss: 5.06353\n",
      "loss: 4.95351\n",
      "loss: 5.00948\n",
      "loss: 4.93084\n",
      "loss: 4.92124\n",
      "loss: 4.96263\n",
      "loss: 4.94364\n",
      "loss: 5.01356\n",
      "loss: 5.00428\n",
      "loss: 5.13046\n",
      "loss: 4.96380\n",
      "loss: 5.08814\n",
      "loss: 4.93371\n",
      "loss: 4.92380\n",
      "loss: 5.01051\n",
      "loss: 4.92390\n",
      "loss: 4.90644\n",
      "loss: 4.97434\n",
      "loss: 4.90848\n",
      "loss: 4.99039\n",
      "loss: 5.04546\n",
      "loss: 5.00504\n",
      "loss: 5.00052\n",
      "loss: 4.92300\n",
      "loss: 4.99870\n",
      "loss: 4.93117\n",
      "loss: 4.96215\n",
      "loss: 4.96242\n",
      "loss: 5.02202\n",
      "loss: 4.88916\n",
      "loss: 4.96407\n",
      "loss: 4.87256\n",
      "loss: 4.94917\n",
      "loss: 4.90012\n",
      "loss: 4.90401\n",
      "loss: 4.88087\n",
      "loss: 4.96594\n",
      "loss: 4.95936\n",
      "loss: 5.02597\n",
      "loss: 4.96759\n",
      "loss: 4.98626\n",
      "loss: 4.95577\n",
      "loss: 4.87848\n",
      "loss: 5.03481\n",
      "loss: 4.87706\n",
      "loss: 5.06924\n",
      "loss: 4.98723\n",
      "loss: 4.90171\n",
      "loss: 4.96741\n",
      "loss: 4.81600\n",
      "loss: 4.68567\n",
      "loss: 4.94042\n",
      "loss: 5.05268\n",
      "loss: 5.06205\n",
      "loss: 4.87801\n",
      "loss: 4.94815\n",
      "loss: 4.92989\n",
      "loss: 4.93663\n",
      "loss: 4.91907\n",
      "loss: 4.99931\n",
      "loss: 4.87814\n",
      "loss: 4.97530\n",
      "loss: 5.02453\n",
      "loss: 4.84724\n",
      "loss: 5.05004\n",
      "loss: 4.89889\n",
      "loss: 4.88360\n",
      "loss: 4.82376\n",
      "loss: 4.93732\n",
      "loss: 5.02023\n",
      "loss: 5.02548\n",
      "loss: 4.90383\n",
      "loss: 4.94252\n",
      "loss: 4.92204\n",
      "loss: 4.88648\n",
      "loss: 4.89192\n",
      "loss: 4.92876\n",
      "loss: 4.87786\n",
      "loss: 4.95573\n",
      "loss: 4.86046\n",
      "loss: 4.91077\n",
      "loss: 4.89840\n",
      "loss: 4.94123\n",
      "loss: 4.93150\n",
      "loss: 4.98169\n",
      "loss: 4.88539\n",
      "loss: 4.94753\n",
      "loss: 4.96854\n",
      "loss: 4.92188\n",
      "loss: 4.95599\n",
      "loss: 4.97869\n",
      "loss: 4.84495\n",
      "loss: 4.86723\n",
      "loss: 4.86528\n",
      "loss: 4.93045\n",
      "loss: 4.95840\n",
      "loss: 4.86095\n",
      "loss: 4.95334\n",
      "loss: 4.82483\n",
      "loss: 4.92875\n",
      "loss: 4.94509\n",
      "loss: 4.93120\n",
      "loss: 4.90524\n",
      "loss: 4.96771\n",
      "loss: 4.96217\n",
      "loss: 4.91711\n",
      "loss: 4.89088\n",
      "loss: 4.88785\n",
      "loss: 4.92089\n",
      "loss: 5.03769\n",
      "loss: 4.94036\n",
      "loss: 4.89980\n",
      "loss: 5.03200\n",
      "loss: 4.89572\n",
      "loss: 4.80702\n",
      "loss: 4.96711\n",
      "loss: 4.87972\n",
      "loss: 4.92877\n",
      "loss: 4.88587\n",
      "loss: 4.99655\n",
      "loss: 4.85987\n",
      "loss: 4.85246\n",
      "loss: 4.82947\n",
      "loss: 4.92136\n",
      "loss: 4.89664\n",
      "loss: 4.86340\n",
      "loss: 4.87915\n",
      "loss: 4.81151\n",
      "loss: 4.88286\n",
      "loss: 4.93707\n",
      "loss: 4.90237\n",
      "loss: 4.87193\n",
      "loss: 4.86521\n",
      "loss: 4.90740\n",
      "loss: 4.97913\n",
      "loss: 4.87823\n",
      "loss: 4.90199\n",
      "loss: 4.94855\n",
      "loss: 4.90803\n",
      "loss: 4.88414\n",
      "loss: 4.97651\n",
      "loss: 4.88179\n",
      "loss: 4.98386\n",
      "loss: 4.90820\n",
      "loss: 4.91185\n",
      "loss: 4.58335\n",
      "loss: 4.99336\n",
      "loss: 4.90960\n",
      "loss: 4.86131\n",
      "loss: 4.89030\n",
      "loss: 4.85747\n",
      "loss: 4.83024\n",
      "loss: 4.85121\n",
      "loss: 4.84006\n",
      "loss: 4.81791\n",
      "loss: 4.80357\n",
      "loss: 4.84262\n",
      "loss: 4.86461\n",
      "loss: 4.78838\n",
      "loss: 4.89126\n",
      "loss: 4.80353\n",
      "loss: 4.88136\n",
      "loss: 4.86599\n",
      "loss: 4.87348\n",
      "loss: 4.95642\n",
      "loss: 4.85621\n",
      "loss: 4.87437\n",
      "loss: 4.80182\n",
      "loss: 4.93787\n",
      "loss: 4.88400\n",
      "loss: 4.83583\n",
      "loss: 4.74971\n",
      "loss: 4.84704\n",
      "loss: 4.88048\n",
      "loss: 4.86544\n",
      "loss: 4.97093\n",
      "loss: 4.85398\n",
      "loss: 4.93021\n",
      "loss: 4.79596\n",
      "loss: 5.06302\n",
      "loss: 4.86125\n",
      "loss: 4.83197\n",
      "loss: 4.77702\n",
      "loss: 4.80786\n",
      "loss: 4.82156\n",
      "loss: 5.02357\n",
      "loss: 4.74335\n",
      "loss: 4.93583\n",
      "loss: 4.85458\n",
      "loss: 4.88806\n",
      "loss: 4.70432\n",
      "loss: 4.76827\n",
      "loss: 4.85803\n",
      "loss: 4.86580\n",
      "loss: 4.76706\n",
      "loss: 4.89616\n",
      "loss: 4.82675\n",
      "loss: 4.94422\n",
      "loss: 5.03205\n",
      "loss: 4.82225\n",
      "loss: 4.86383\n",
      "loss: 4.74294\n",
      "loss: 4.81846\n",
      "loss: 4.83916\n",
      "loss: 4.84658\n",
      "loss: 4.93238\n",
      "loss: 4.92329\n",
      "loss: 4.85199\n",
      "loss: 4.80585\n",
      "loss: 4.79038\n",
      "loss: 4.81898\n",
      "loss: 4.82818\n",
      "loss: 4.86359\n",
      "loss: 4.81102\n",
      "loss: 4.87736\n",
      "loss: 4.92571\n",
      "loss: 4.95210\n",
      "loss: 5.01187\n",
      "loss: 4.75122\n",
      "loss: 4.87716\n",
      "loss: 4.82431\n",
      "loss: 4.77522\n",
      "loss: 4.83639\n",
      "loss: 4.83824\n",
      "loss: 4.78433\n",
      "loss: 4.86033\n",
      "loss: 4.76651\n",
      "loss: 4.93427\n",
      "loss: 4.94883\n",
      "loss: 4.91170\n",
      "loss: 4.90983\n",
      "loss: 4.83622\n",
      "loss: 4.85440\n",
      "loss: 4.92721\n",
      "loss: 4.71128\n",
      "loss: 4.90657\n",
      "loss: 4.75347\n",
      "loss: 4.96752\n",
      "loss: 4.81963\n",
      "loss: 4.82677\n",
      "loss: 4.69634\n",
      "loss: 4.90868\n",
      "loss: 4.85156\n",
      "loss: 4.81645\n",
      "loss: 4.94466\n",
      "loss: 4.81019\n",
      "loss: 4.86696\n",
      "loss: 4.80806\n",
      "loss: 4.85293\n",
      "loss: 4.84591\n",
      "loss: 4.80605\n",
      "loss: 4.91935\n",
      "loss: 4.82544\n",
      "loss: 4.79989\n",
      "loss: 4.84162\n",
      "loss: 4.82391\n",
      "loss: 4.89606\n",
      "loss: 4.91722\n",
      "loss: 4.83840\n",
      "loss: 4.87536\n",
      "loss: 4.93883\n",
      "loss: 4.82459\n",
      "loss: 4.83507\n",
      "loss: 4.88137\n",
      "loss: 4.95310\n",
      "loss: 4.89475\n",
      "loss: 4.76686\n",
      "loss: 4.70908\n",
      "loss: 4.82150\n",
      "loss: 4.82521\n",
      "loss: 4.87152\n",
      "loss: 4.77375\n",
      "loss: 4.81482\n",
      "loss: 4.94136\n",
      "loss: 4.89616\n",
      "loss: 4.92963\n",
      "loss: 4.90166\n",
      "loss: 4.74731\n",
      "loss: 4.69298\n",
      "loss: 4.93330\n",
      "loss: 4.83686\n",
      "loss: 4.80991\n",
      "loss: 4.81557\n",
      "loss: 4.76971\n",
      "loss: 4.83546\n",
      "loss: 4.87950\n",
      "loss: 4.79294\n",
      "loss: 4.85572\n",
      "loss: 4.91724\n",
      "loss: 4.87465\n",
      "loss: 4.79244\n",
      "loss: 4.91531\n",
      "loss: 4.90229\n",
      "loss: 4.88929\n",
      "loss: 4.81138\n",
      "loss: 4.79733\n",
      "loss: 4.75208\n",
      "loss: 4.85146\n",
      "loss: 4.74986\n",
      "loss: 4.83982\n",
      "loss: 4.68565\n",
      "loss: 4.82906\n",
      "loss: 4.74671\n",
      "loss: 4.84676\n",
      "loss: 4.82289\n",
      "loss: 4.84691\n",
      "loss: 4.85199\n",
      "loss: 4.84976\n",
      "loss: 4.85110\n",
      "loss: 4.86980\n",
      "loss: 4.88011\n",
      "loss: 4.80158\n",
      "loss: 4.78598\n",
      "loss: 4.91030\n",
      "loss: 4.80328\n",
      "loss: 4.82709\n",
      "loss: 4.93197\n",
      "loss: 4.74311\n",
      "loss: 4.95411\n",
      "loss: 4.87688\n",
      "loss: 4.80650\n",
      "loss: 4.84866\n",
      "loss: 4.91734\n",
      "loss: 4.75112\n",
      "loss: 4.84381\n",
      "loss: 4.81597\n",
      "loss: 4.79139\n",
      "loss: 4.80939\n",
      "loss: 4.77617\n",
      "loss: 4.69338\n",
      "loss: 4.83815\n",
      "loss: 4.72902\n",
      "loss: 4.71335\n",
      "loss: 4.82396\n",
      "loss: 4.81365\n",
      "loss: 4.45798\n",
      "loss: 4.89536\n",
      "loss: 4.93887\n",
      "loss: 4.80347\n",
      "loss: 4.83224\n",
      "loss: 4.78185\n",
      "loss: 4.87284\n",
      "loss: 4.72404\n",
      "loss: 4.90268\n",
      "loss: 4.86547\n",
      "loss: 4.72818\n",
      "loss: 4.80827\n",
      "loss: 4.80553\n",
      "loss: 4.88439\n",
      "loss: 4.80923\n",
      "loss: 4.87332\n",
      "loss: 4.69207\n",
      "loss: 4.80472\n",
      "loss: 4.78533\n",
      "loss: 4.75361\n",
      "loss: 4.94686\n",
      "loss: 4.85062\n",
      "loss: 4.82423\n",
      "loss: 4.97603\n",
      "loss: 4.74453\n",
      "loss: 4.83844\n",
      "loss: 4.81705\n",
      "loss: 4.81362\n",
      "loss: 4.90183\n",
      "loss: 4.84522\n",
      "loss: 4.90524\n",
      "loss: 4.98327\n",
      "loss: 4.71222\n",
      "loss: 4.83890\n",
      "loss: 4.92731\n",
      "loss: 4.87935\n",
      "loss: 4.85492\n",
      "loss: 4.89716\n",
      "loss: 4.70313\n",
      "loss: 4.63587\n",
      "loss: 4.77472\n",
      "loss: 4.75745\n",
      "loss: 4.75854\n",
      "loss: 4.82546\n",
      "loss: 4.78492\n",
      "loss: 4.79936\n",
      "loss: 4.77931\n",
      "loss: 4.77249\n",
      "loss: 4.71084\n",
      "loss: 4.72418\n",
      "loss: 4.91164\n",
      "loss: 4.80249\n",
      "loss: 4.77825\n",
      "loss: 4.89526\n",
      "loss: 4.92804\n",
      "loss: 4.91556\n",
      "loss: 4.76172\n",
      "loss: 4.75860\n",
      "loss: 4.86685\n",
      "loss: 4.92003\n",
      "loss: 4.84714\n",
      "loss: 4.75628\n",
      "loss: 4.76878\n",
      "loss: 4.64276\n",
      "loss: 4.89416\n",
      "loss: 4.84180\n",
      "loss: 4.81356\n",
      "loss: 4.68419\n",
      "loss: 4.82244\n",
      "loss: 4.81253\n",
      "loss: 4.68817\n",
      "loss: 4.80280\n",
      "loss: 4.81574\n",
      "loss: 4.84720\n",
      "loss: 4.92916\n",
      "loss: 4.85452\n",
      "loss: 4.73588\n",
      "loss: 4.80980\n",
      "loss: 4.83777\n",
      "loss: 4.85810\n",
      "loss: 4.83986\n",
      "loss: 4.96207\n",
      "loss: 4.81414\n",
      "loss: 4.77140\n",
      "loss: 4.69310\n",
      "loss: 4.89137\n",
      "loss: 4.86266\n",
      "loss: 4.90684\n",
      "loss: 4.84260\n",
      "loss: 4.79079\n",
      "loss: 4.81091\n",
      "loss: 4.81919\n",
      "loss: 4.78444\n",
      "loss: 4.72268\n",
      "loss: 4.81406\n",
      "loss: 4.58077\n",
      "loss: 4.83543\n",
      "loss: 4.88173\n",
      "loss: 4.82827\n",
      "loss: 4.86986\n",
      "loss: 4.77278\n",
      "loss: 4.80562\n",
      "loss: 4.72367\n",
      "loss: 4.74481\n",
      "loss: 4.78879\n",
      "loss: 4.82850\n",
      "loss: 4.75548\n",
      "loss: 4.89358\n",
      "loss: 4.85430\n",
      "loss: 4.79220\n",
      "loss: 4.69144\n",
      "loss: 4.83353\n",
      "loss: 4.76039\n",
      "loss: 4.84559\n",
      "loss: 4.86115\n",
      "loss: 4.73797\n",
      "loss: 4.80957\n",
      "loss: 4.71952\n",
      "loss: 4.71510\n",
      "loss: 4.81158\n",
      "loss: 4.74752\n",
      "loss: 4.74630\n",
      "loss: 4.67873\n",
      "loss: 4.79865\n",
      "loss: 4.78165\n",
      "loss: 4.76806\n",
      "loss: 4.73070\n",
      "loss: 4.70950\n",
      "loss: 4.85253\n",
      "loss: 4.79690\n",
      "loss: 4.88345\n",
      "loss: 4.94752\n",
      "loss: 4.91823\n",
      "loss: 4.75338\n",
      "loss: 4.76463\n",
      "loss: 4.81352\n",
      "loss: 4.68755\n",
      "loss: 4.75139\n",
      "loss: 4.79553\n",
      "loss: 4.70772\n",
      "loss: 4.92685\n",
      "loss: 4.77517\n",
      "loss: 4.84081\n",
      "loss: 4.79315\n",
      "loss: 4.87729\n",
      "loss: 4.91909\n",
      "loss: 4.73271\n",
      "loss: 4.81958\n",
      "loss: 4.82378\n",
      "loss: 4.74972\n",
      "loss: 4.65258\n",
      "loss: 4.79672\n",
      "loss: 4.79174\n",
      "loss: 4.74642\n",
      "loss: 4.77562\n",
      "loss: 4.87803\n",
      "loss: 4.92339\n",
      "loss: 4.84520\n",
      "loss: 4.74449\n",
      "loss: 4.80933\n",
      "loss: 4.84881\n",
      "loss: 4.85087\n",
      "loss: 4.76130\n",
      "loss: 4.84027\n",
      "loss: 4.71598\n",
      "loss: 4.77745\n",
      "loss: 4.69284\n",
      "loss: 4.77015\n",
      "loss: 4.81980\n",
      "loss: 4.89033\n",
      "loss: 4.81267\n",
      "loss: 4.71031\n",
      "loss: 4.76565\n",
      "loss: 4.84193\n",
      "loss: 4.83798\n",
      "loss: 4.72814\n",
      "loss: 4.77479\n",
      "loss: 4.85162\n",
      "loss: 4.80914\n",
      "loss: 4.76456\n",
      "loss: 4.71100\n",
      "loss: 4.71134\n",
      "loss: 4.74868\n",
      "loss: 4.72867\n",
      "loss: 4.69081\n",
      "loss: 4.69825\n",
      "loss: 4.88340\n",
      "loss: 4.88989\n",
      "loss: 4.77034\n",
      "loss: 4.82915\n",
      "loss: 4.46971\n",
      "loss: 4.71946\n",
      "loss: 4.84099\n",
      "loss: 4.71469\n",
      "loss: 4.73164\n",
      "loss: 4.70589\n",
      "loss: 4.77618\n",
      "loss: 4.80921\n",
      "loss: 4.86517\n",
      "loss: 4.80087\n",
      "loss: 4.71553\n",
      "loss: 4.79312\n",
      "loss: 4.83163\n",
      "loss: 4.91752\n",
      "loss: 4.84655\n",
      "loss: 4.69009\n",
      "loss: 4.89090\n",
      "loss: 4.82406\n",
      "loss: 4.73671\n",
      "loss: 4.84210\n",
      "loss: 4.83955\n",
      "loss: 4.75710\n",
      "loss: 4.87869\n",
      "loss: 4.70938\n",
      "loss: 4.69307\n",
      "loss: 4.80240\n",
      "loss: 4.79435\n",
      "loss: 4.74244\n",
      "loss: 4.74569\n",
      "loss: 4.71934\n",
      "loss: 4.77469\n",
      "loss: 4.68095\n",
      "loss: 4.75745\n",
      "loss: 4.80588\n",
      "loss: 4.63580\n",
      "loss: 4.84677\n",
      "loss: 4.72855\n",
      "loss: 4.67089\n",
      "loss: 4.75806\n",
      "loss: 4.78044\n",
      "loss: 4.76972\n",
      "loss: 4.77358\n",
      "loss: 4.74293\n",
      "loss: 4.76714\n",
      "loss: 4.67872\n",
      "loss: 4.72092\n",
      "loss: 4.69920\n",
      "loss: 4.87653\n",
      "loss: 4.70050\n",
      "loss: 4.89747\n",
      "loss: 4.77573\n",
      "loss: 4.73311\n",
      "loss: 4.79666\n",
      "loss: 4.75560\n",
      "loss: 4.82921\n",
      "loss: 4.79660\n",
      "loss: 4.71855\n",
      "loss: 4.78664\n",
      "loss: 4.77405\n",
      "loss: 4.85944\n",
      "loss: 4.75705\n",
      "loss: 4.75817\n",
      "loss: 4.79526\n",
      "loss: 4.66633\n",
      "loss: 4.70772\n",
      "loss: 4.74605\n",
      "loss: 4.75818\n",
      "loss: 4.68193\n",
      "loss: 4.81734\n",
      "loss: 4.69199\n",
      "loss: 4.79092\n",
      "loss: 4.76440\n",
      "loss: 4.70002\n",
      "loss: 4.71852\n",
      "loss: 4.78571\n",
      "loss: 4.70315\n",
      "loss: 4.77957\n",
      "loss: 4.76299\n",
      "loss: 4.71935\n",
      "loss: 5.01787\n",
      "loss: 4.78199\n",
      "loss: 4.72565\n",
      "loss: 4.81228\n",
      "loss: 4.79670\n",
      "loss: 4.78728\n",
      "loss: 4.74017\n",
      "loss: 4.80551\n",
      "loss: 4.67589\n",
      "loss: 4.80347\n",
      "loss: 4.76813\n",
      "loss: 4.73460\n",
      "loss: 4.74632\n",
      "loss: 4.65698\n",
      "loss: 4.65960\n",
      "loss: 4.70740\n",
      "loss: 4.41880\n"
     ]
    }
   ],
   "source": [
    "# Create a PyTorch module for the SimCLR model.\n",
    "class SimCLR(torch.nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projection_head = heads.SimCLRProjectionHead(\n",
    "            input_dim=1280,  # Resnet18 features have 512 dimensions.\n",
    "            hidden_dim=1280,\n",
    "            output_dim=128,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(features)\n",
    "        return z\n",
    "\n",
    "\n",
    "# Use a resnet backbone.\n",
    "backbone = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "# Ignore the classification head as we only want the features.\n",
    "backbone._fc = torch.nn.Identity()\n",
    "\n",
    "# Build the SimCLR model.\n",
    "model = SimCLR(backbone)\n",
    "\n",
    "\n",
    "# Prepare transform that creates multiple random views for every image.\n",
    "transform = transforms.SimCLRTransform(input_size=32, cj_prob=0.5)\n",
    "\n",
    "\n",
    "# Create a dataset from your image folder.\n",
    "dataset = LightlyDataset(input_dir=\"../data/02_data_split/train_data/\", transform=transform)\n",
    "\n",
    "# Build a PyTorch dataloader.\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,  # Pass the dataset to the dataloader.\n",
    "    batch_size=128,  # A large batch size helps with the learning.\n",
    "    shuffle=True,  # Shuffling is important!\n",
    ")\n",
    "\n",
    "# Lightly exposes building blocks such as loss functions.\n",
    "criterion = loss.NTXentLoss(temperature=0.5)\n",
    "\n",
    "# Get a PyTorch optimizer.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-6)\n",
    "\n",
    "# Train the model.\n",
    "for epoch in range(10):\n",
    "    for (view0, view1), targets, filenames in dataloader:\n",
    "        z0 = model(view0)\n",
    "        z1 = model(view1)\n",
    "        loss = criterion(z0, z1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(f\"loss: {loss.item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6400])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x).flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../embeddings/')\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from dataset import ImageDataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def slice_image_paths(paths):\n",
    "    return [i.split('/')[-1].replace('\\\\','/') for i in paths]\n",
    "\n",
    "def create_timestamp_folder():\n",
    "    \"\"\"\n",
    "    Create a folder name based on the current timestamp.\n",
    "    Returns:\n",
    "        folder_name (str): The name of the folder, in the format 'YYYY-MM-DD-HH-MM-SS'.\n",
    "    \"\"\"\n",
    "    current_time = time.localtime()\n",
    "    folder_name = time.strftime('%Y-%m-%d-%H-%M-%S', current_time)\n",
    "    return f'efficient_SimCLR_{folder_name}'    \n",
    "\n",
    "def save_checkpoint(model, optimizer):\n",
    "    timestamp_folder = create_timestamp_folder()\n",
    "    state = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict() if optimizer is not None else None}\n",
    "    Path(f'./data_output/checkpoints/{timestamp_folder}').mkdir(exist_ok=True)\n",
    "    filename=f\"./data_output/checkpoints/{timestamp_folder}/checkpoint.pth.tar\"\n",
    "    # draw_loss_curve(\n",
    "    #     history=loss, \n",
    "    #     results_path=f\"./data_output/checkpoints/{timestamp_folder}\"\n",
    "    #     )\n",
    "    torch.save(state, filename)\n",
    "\n",
    "    # with open(f\"./data_output/checkpoints/{timestamp_folder}/config.yaml\", \"w\") as yaml_file:\n",
    "    #     yaml.dump(config.__dict__, yaml_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 124)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data = ImageDataLoader(\"../data/02_data_split/train_data/\")\n",
    "dataloader = DataLoader(data.dataset, batch_size=50, shuffle=False)\n",
    "\n",
    "# select embedding model training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# compute embeddings and save\n",
    "target = []\n",
    "paths = []\n",
    "labels = []\n",
    "feature_embeddings = np.empty((0, 128))\n",
    "\n",
    "for i, (x, y, path, label) in enumerate(dataloader):\n",
    "    x = x.to(device=device)\n",
    "    with torch.no_grad():\n",
    "        batch_features = model(x)\n",
    "\n",
    "    batch_features_np = batch_features.view(batch_features.size(0), -1).cpu().numpy()\n",
    "    feature_embeddings = np.vstack((feature_embeddings, batch_features_np))\n",
    "    target.extend(list(y.cpu().detach().numpy()))\n",
    "    paths.extend(slice_image_paths(path))\n",
    "    labels.extend(label)\n",
    "\n",
    "\n",
    "data_dict = {\n",
    "    \"model\": 'efficient_SimCLR',\n",
    "    \"embedding\":feature_embeddings,\n",
    "    \"target\":target,\n",
    "    \"paths\": paths,\n",
    "    \"classes\":labels\n",
    "}\n",
    "\n",
    "with open('./efficient_SimCLR.pickle', 'wb') as pickle_file:\n",
    "    pickle.dump(data_dict, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho_do_arquivo = 'efficient_SimCLR_model.pth'\n",
    "torch.save(model, caminho_do_arquivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightly.loss import SwaVLoss\n",
    "from lightly.models.modules import SwaVProjectionHead, SwaVPrototypes\n",
    "from lightly.transforms.swav_transform import SwaVTransform\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Starting Training\n",
      "epoch: 00, loss: 6.59559\n",
      "epoch: 01, loss: 6.35090\n",
      "epoch: 02, loss: 6.25399\n",
      "epoch: 03, loss: 6.14697\n",
      "epoch: 04, loss: 6.09689\n",
      "epoch: 05, loss: 6.06442\n",
      "epoch: 06, loss: 6.01974\n",
      "epoch: 07, loss: 5.98131\n",
      "epoch: 08, loss: 5.97840\n",
      "epoch: 09, loss: 5.94289\n",
      "epoch: 10, loss: 5.91916\n",
      "epoch: 11, loss: 5.90350\n",
      "epoch: 12, loss: 5.88035\n",
      "epoch: 13, loss: 5.87450\n",
      "epoch: 14, loss: 5.85381\n",
      "epoch: 15, loss: 5.84562\n",
      "epoch: 16, loss: 5.82016\n",
      "epoch: 17, loss: 5.82021\n",
      "epoch: 18, loss: 5.80116\n",
      "epoch: 19, loss: 5.81117\n"
     ]
    }
   ],
   "source": [
    "class SwaV(torch.nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projection_head = SwaVProjectionHead(1280, 1280, 128)\n",
    "        self.prototypes = SwaVPrototypes(128, n_prototypes=1280)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        x = self.projection_head(x)\n",
    "        x = torch.nn.functional.normalize(x, dim=1, p=2)\n",
    "        p = self.prototypes(x)\n",
    "        return p\n",
    "\n",
    "\n",
    "# Use a resnet backbone.\n",
    "backbone = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "# Ignore the classification head as we only want the features.\n",
    "backbone._fc = torch.nn.Identity()\n",
    "\n",
    "\n",
    "model = SwaV(backbone)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "transform = SwaVTransform()\n",
    "# we ignore object detection annotations by setting target_transform to return 0\n",
    "# dataset = torchvision.datasets.VOCDetection(\n",
    "#     \"../data/02_data_split/train_data/\",\n",
    "#     download=True,\n",
    "#     transform=transform,\n",
    "#     target_transform=lambda t: 0,\n",
    "# )\n",
    "dataset = LightlyDataset(input_dir=\"../data/02_data_split/train_data/\", transform=transform)\n",
    "# or create a dataset from a folder containing images or videos:\n",
    "# dataset = LightlyDataset(\"path/to/folder\")\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=20,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    # num_workers=8,\n",
    ")\n",
    "\n",
    "criterion = SwaVLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "loss_curve = np.array([])\n",
    "num_epochs = 20\n",
    "print(\"Starting Training\")\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        views = batch[0]\n",
    "        model.prototypes.normalize()\n",
    "        multi_crop_features = [model(view.to(device)) for view in views]\n",
    "        high_resolution = multi_crop_features[:2]\n",
    "        low_resolution = multi_crop_features[2:]\n",
    "        loss = criterion(high_resolution, low_resolution)\n",
    "        total_loss += loss.detach()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    loss_curve = np.append(loss_curve, avg_loss.cpu().numpy())\n",
    "    print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABf60lEQVR4nO3dd1gUV9sG8HuXsvTelSaioCKiWBCNMfYWW9TYTbESjSa+Ub8UW155TaIxFaOJJRpNYqLG2BC7okaxxYKAgoBUUTqylJ3vD+ImGzoCs7vcv+vaK+zsmdlnGNa9c+bMHIkgCAKIiIiItIRU7AKIiIiI6hPDDREREWkVhhsiIiLSKgw3REREpFUYboiIiEirMNwQERGRVmG4ISIiIq3CcENERERaheGGiIiItArDDVETNW3aNLi5udVp3WXLlkEikdRvQURE9YThhkjNSCSSGj1OnjwpdqmimDZtGkxMTMQuo8b27NmDQYMGwcbGBvr6+nBycsLYsWNx/PhxsUsj0loSzi1FpF62b9+u8vz7779HWFgYtm3bprK8X79+sLe3r/P7FBcXQ6FQQCaT1XrdkpISlJSUwMDAoM7vX1fTpk3DL7/8gry8vEZ/79oQBAGvvvoqtmzZAj8/P7z00ktwcHBASkoK9uzZg8uXLyM8PBzdu3cXu1QiraMrdgFEpGrSpEkqzy9cuICwsLByy/+toKAARkZGNX4fPT29OtUHALq6utDV5T8fVVmzZg22bNmC+fPnY+3atSqn8d59911s27atXn6HgiCgsLAQhoaGz7wtIm3B01JEGuj5559Hu3btcPnyZTz33HMwMjLC//3f/wEAfvvtNwwZMgROTk6QyWTw8PDAypUrUVpaqrKNf4+5uX//PiQSCT755BNs2LABHh4ekMlk6Ny5My5duqSybkVjbiQSCd544w3s3bsX7dq1g0wmQ9u2bXH48OFy9Z88eRL+/v4wMDCAh4cHvvnmm3ofx7Nr1y506tQJhoaGsLGxwaRJk5CUlKTSJjU1Fa+88gqaN28OmUwGR0dHDB8+HPfv31e2iYiIwIABA2BjYwNDQ0O4u7vj1VdfrfK9nzx5guDgYHh5eeGTTz6pcL8mT56MLl26AKh8DNOWLVsgkUhU6nFzc8PQoUMRGhoKf39/GBoa4ptvvkG7du3Qu3fvcttQKBRo1qwZXnrpJZVl69atQ9u2bWFgYAB7e3vMnDkTmZmZVe4Xkabg/3oRaahHjx5h0KBBePnllzFp0iTlKaotW7bAxMQEb731FkxMTHD8+HF88MEHyMnJwccff1ztdnfs2IHc3FzMnDkTEokEH330EUaNGoXY2Nhqe3vOnj2L3bt3Y86cOTA1NcXnn3+O0aNHIyEhAdbW1gCAq1evYuDAgXB0dMTy5ctRWlqKFStWwNbW9tl/KX/ZsmULXnnlFXTu3BnBwcFIS0vDZ599hvDwcFy9ehUWFhYAgNGjR+PWrVuYO3cu3NzckJ6ejrCwMCQkJCif9+/fH7a2tli8eDEsLCxw//597N69u9rfw+PHjzF//nzo6OjU2349FRUVhfHjx2PmzJmYPn06WrdujXHjxmHZsmVITU2Fg4ODSi3Jycl4+eWXlctmzpyp/B3NmzcPcXFx+PLLL3H16lWEh4c/U68ekVoQiEitBQUFCf/+qPbq1UsAIKxfv75c+4KCgnLLZs6cKRgZGQmFhYXKZVOnThVcXV2Vz+Pi4gQAgrW1tfD48WPl8t9++00AIPz+++/KZUuXLi1XEwBBX19fuHv3rnLZ9evXBQDCF198oVw2bNgwwcjISEhKSlIui4mJEXR1dcttsyJTp04VjI2NK329qKhIsLOzE9q1ayc8efJEuXz//v0CAOGDDz4QBEEQMjMzBQDCxx9/XOm29uzZIwAQLl26VG1d//TZZ58JAIQ9e/bUqH1Fv09BEITNmzcLAIS4uDjlMldXVwGAcPjwYZW2UVFR5X7XgiAIc+bMEUxMTJR/F2fOnBEACD/88INKu8OHD1e4nEgT8bQUkYaSyWR45ZVXyi3/59iL3NxcZGRkoGfPnigoKMCdO3eq3e64ceNgaWmpfN6zZ08AQGxsbLXr9u3bFx4eHsrn7du3h5mZmXLd0tJSHD16FCNGjICTk5OyXcuWLTFo0KBqt18TERERSE9Px5w5c1QGPA8ZMgReXl44cOAAgLLfk76+Pk6ePFnp6ZinPTz79+9HcXFxjWvIyckBAJiamtZxL6rm7u6OAQMGqCxr1aoVOnTogJ9++km5rLS0FL/88guGDRum/LvYtWsXzM3N0a9fP2RkZCgfnTp1gomJCU6cONEgNRM1JoYbIg3VrFkz6Ovrl1t+69YtjBw5Eubm5jAzM4Otra1yMHJ2dna123VxcVF5/jTo1GQ8xr/Xfbr+03XT09Px5MkTtGzZsly7ipbVRXx8PACgdevW5V7z8vJSvi6TybB69WocOnQI9vb2eO655/DRRx8hNTVV2b5Xr14YPXo0li9fDhsbGwwfPhybN2+GXC6vsgYzMzMAZeGyIbi7u1e4fNy4cQgPD1eOLTp58iTS09Mxbtw4ZZuYmBhkZ2fDzs4Otra2Ko+8vDykp6c3SM1EjYnhhkhDVXR1TFZWFnr16oXr169jxYoV+P333xEWFobVq1cDKBtIWp3KxogINbhrxLOsK4b58+cjOjoawcHBMDAwwPvvvw9vb29cvXoVQNkg6V9++QXnz5/HG2+8gaSkJLz66qvo1KlTlZeie3l5AQBu3LhRozoqG0j970HgT1V2ZdS4ceMgCAJ27doFAPj5559hbm6OgQMHKtsoFArY2dkhLCyswseKFStqVDOROmO4IdIiJ0+exKNHj7Blyxa8+eabGDp0KPr27atymklMdnZ2MDAwwN27d8u9VtGyunB1dQVQNuj236KiopSvP+Xh4YG3334bR44cwc2bN1FUVIQ1a9aotOnWrRv++9//IiIiAj/88ANu3bqFH3/8sdIaevToAUtLS+zcubPSgPJPT49PVlaWyvKnvUw15e7uji5duuCnn35CSUkJdu/ejREjRqjcy8jDwwOPHj1CYGAg+vbtW+7h6+tbq/ckUkcMN0Ra5GnPyT97SoqKivD111+LVZIKHR0d9O3bF3v37kVycrJy+d27d3Ho0KF6eQ9/f3/Y2dlh/fr1KqePDh06hMjISAwZMgRA2X2BCgsLVdb18PCAqampcr3MzMxyvU4dOnQAgCpPTRkZGWHRokWIjIzEokWLKuy52r59Oy5evKh8XwA4ffq08vX8/Hxs3bq1prutNG7cOFy4cAGbNm1CRkaGyikpABg7dixKS0uxcuXKcuuWlJSUC1hEmoiXghNpke7du8PS0hJTp07FvHnzIJFIsG3bNrU6LbRs2TIcOXIEgYGBmD17NkpLS/Hll1+iXbt2uHbtWo22UVxcjA8//LDccisrK8yZMwerV6/GK6+8gl69emH8+PHKS8Hd3NywYMECAEB0dDT69OmDsWPHok2bNtDV1cWePXuQlpamvGx669at+PrrrzFy5Eh4eHggNzcXGzduhJmZGQYPHlxljf/5z39w69YtrFmzBidOnFDeoTg1NRV79+7FxYsXce7cOQBA//794eLigtdeew3/+c9/oKOjg02bNsHW1hYJCQm1+O2WhZeFCxdi4cKFsLKyQt++fVVe79WrF2bOnIng4GBcu3YN/fv3h56eHmJiYrBr1y589tlnKvfEIdJIIl6pRUQ1UNml4G3btq2wfXh4uNCtWzfB0NBQcHJyEt555x0hNDRUACCcOHFC2a6yS8ErujQagLB06VLl88ouBQ8KCiq3rqurqzB16lSVZceOHRP8/PwEfX19wcPDQ/j222+Ft99+WzAwMKjkt/C3qVOnCgAqfHh4eCjb/fTTT4Kfn58gk8kEKysrYeLEicKDBw+Ur2dkZAhBQUGCl5eXYGxsLJibmwtdu3YVfv75Z2WbK1euCOPHjxdcXFwEmUwm2NnZCUOHDhUiIiKqrfOpX375Rejfv79gZWUl6OrqCo6OjsK4ceOEkydPqrS7fPmy0LVrV0FfX19wcXER1q5dW+ml4EOGDKnyPQMDAwUAwuuvv15pmw0bNgidOnUSDA0NBVNTU8HHx0d45513hOTk5BrvG5G64txSRKQWRowYgVu3biEmJkbsUohIw3HMDRE1uidPnqg8j4mJwcGDB/H888+LUxARaRX23BBRo3N0dMS0adPQokULxMfHIyQkBHK5HFevXoWnp6fY5RGRhuOAYiJqdAMHDsTOnTuRmpoKmUyGgIAArFq1isGGiOoFe26IiIhIq3DMDREREWkVhhsiIiLSKk1uzI1CoUBycjJMTU0rnc+FiIiI1IsgCMjNzYWTkxOk0qr7ZppcuElOToazs7PYZRAREVEdJCYmonnz5lW2aXLhxtTUFEDZL8fMzEzkaoiIiKgmcnJy4OzsrPwer0qTCzdPT0WZmZkx3BAREWmYmgwp4YBiIiIi0ioMN0RERKRVGG6IiIhIqzS5MTdERCSe0tJSFBcXi10GqSl9ff1qL/OuCYYbIiJqcIIgIDU1FVlZWWKXQmpMKpXC3d0d+vr6z7QdhhsiImpwT4ONnZ0djIyMeBNVKufpTXZTUlLg4uLyTH8jDDdERNSgSktLlcHG2tpa7HJIjdna2iI5ORklJSXQ09Or83Y4oJiIiBrU0zE2RkZGIldC6u7p6ajS0tJn2o7o4SYpKQmTJk2CtbU1DA0N4ePjg4iIiCrXkcvlePfdd+Hq6gqZTAY3Nzds2rSpkSomIqK64Kkoqk59/Y2IeloqMzMTgYGB6N27Nw4dOgRbW1vExMTA0tKyyvXGjh2LtLQ0fPfdd2jZsiVSUlKgUCgaqWoiIiJSZ6KGm9WrV8PZ2RmbN29WLnN3d69yncOHD+PUqVOIjY2FlZUVAMDNza0hyyQiIqoXbm5umD9/PubPn1+j9idPnkTv3r2RmZkJCwuLBq1Nm4h6Wmrfvn3w9/fHmDFjYGdnBz8/P2zcuLFG63z00Udo1qwZWrVqhYULF+LJkycVtpfL5cjJyVF5EBERVUUikVT5WLZsWZ22e+nSJcyYMaPG7bt3746UlBSYm5vX6f1q6uTJk5BIJFpzqb6oPTexsbEICQnBW2+9hf/7v//DpUuXMG/ePOjr62Pq1KmVrnP27FkYGBhgz549yMjIwJw5c/Do0SOVHqCngoODsXz58obeFQBATmExEh4VoF2zhv0jJCKihpWSkqL8+aeffsIHH3yAqKgo5TITExPlz4IgoLS0FLq61X+l2tra1qoOfX19ODg41GodErnnRqFQoGPHjli1ahX8/PwwY8YMTJ8+HevXr69yHYlEgh9++AFdunTB4MGDsXbtWmzdurXC3pslS5YgOztb+UhMTGyQfbmVnA3f5UcwZdNFCILQIO9BRESNw8HBQfkwNzeHRCJRPr9z5w5MTU1x6NAhdOrUCTKZDGfPnsW9e/cwfPhw2Nvbw8TEBJ07d8bRo0dVtuvm5oZ169Ypn0skEnz77bcYOXIkjIyM4OnpiX379ilf/3ePypYtW2BhYYHQ0FB4e3vDxMQEAwcOVAljJSUlmDdvHiwsLGBtbY1FixZh6tSpGDFiRJ1/H5mZmZgyZQosLS1hZGSEQYMGISYmRvl6fHw8hg0bBktLSxgbG6Nt27Y4ePCgct2JEyfC1tYWhoaG8PT0rLAzoj6JGm4cHR3Rpk0blWXe3t5ISEiocp1mzZqpdNF5e3tDEAQ8ePCgXHuZTAYzMzOVR0NoaWcCPR0pHucXIS4jv0Heg4hIGwiCgIKiElEe9fk/n4sXL8b//vc/REZGon379sjLy8PgwYNx7NgxXL16FQMHDsSwYcOq/E4DgOXLl2Ps2LH4888/MXjwYEycOBGPHz+utH1BQQE++eQTbNu2DadPn0ZCQgIWLlyofH316tX44YcfsHnzZoSHhyMnJwd79+59pn2dNm0aIiIisG/fPpw/fx6CIGDw4MHKy/yDgoIgl8tx+vRp3LhxA6tXr1b2br3//vu4ffs2Dh06hMjISISEhMDGxuaZ6qmOqKelAgMDVbr5ACA6Ohqurq5VrrNr1y7k5eUpf3HR0dGQSqVo3rx5g9ZbFZmuDnybm+PS/UxExGeiha1J9SsRETVBT4pL0eaDUFHe+/aKATDSr5+vvhUrVqBfv37K51ZWVvD19VU+X7lyJfbs2YN9+/bhjTfeqHQ706ZNw/jx4wEAq1atwueff46LFy9i4MCBFbYvLi7G+vXr4eHhAQB44403sGLFCuXrX3zxBZYsWYKRI0cCAL788ktlL0pdxMTEYN++fQgPD0f37t0BAD/88AOcnZ2xd+9ejBkzBgkJCRg9ejR8fHwAAC1atFCun5CQAD8/P/j7+wNonIuARO25WbBgAS5cuIBVq1bh7t272LFjBzZs2ICgoCBlmyVLlmDKlCnK5xMmTIC1tTVeeeUV3L59G6dPn8Z//vMfvPrqqzA0NBRjN5Q6upZdwn4lPlPUOoiIqOE9/bJ+Ki8vDwsXLoS3tzcsLCxgYmKCyMjIantu2rdvr/zZ2NgYZmZmSE9Pr7S9kZGRMtgAZWc0nrbPzs5GWloaunTponxdR0cHnTp1qtW+/VNkZCR0dXXRtWtX5TJra2u0bt0akZGRAIB58+bhww8/RGBgIJYuXYo///xT2Xb27Nn48ccf0aFDB7zzzjs4d+5cnWupKVF7bjp37ow9e/ZgyZIlWLFiBdzd3bFu3TpMnDhR2SYlJUXlD8PExARhYWGYO3cu/P39YW1tjbFjx+LDDz8UYxdU+Lta4RvEIoLhhoioUoZ6Ori9YoBo711fjI2NVZ4vXLgQYWFh+OSTT9CyZUsYGhripZdeQlFRUZXb+fc0AxKJpMp7t1XUXuyxnq+//joGDBiAAwcO4MiRIwgODsaaNWswd+5cDBo0CPHx8Th48CDCwsLQp08fBAUF4ZNPPmmwekSfW2ro0KEYOnRopa9v2bKl3DIvLy+EhYU1YFV10+mvnpu76XnIKiiChdGzzWpKRKSNJBJJvZ0aUifh4eGYNm2a8nRQXl4e7t+/36g1mJubw97eHpcuXcJzzz0HoGwqgytXrqBDhw512qa3tzdKSkrwxx9/KE9LPXr0CFFRUSrjZp2dnTFr1izMmjULS5YswcaNGzF37lwAZVeJTZ06FVOnTkXPnj3xn//8R7vDjTaxMtZHCxtjxGbk40pCJl7wshe7JCIiaiSenp7YvXs3hg0bBolEgvfff1+Uu+fPnTsXwcHBaNmyJby8vPDFF18gMzOzRlMb3LhxA6ampsrnEokEvr6+GD58OKZPn45vvvkGpqamWLx4MZo1a4bhw4cDAObPn49BgwahVatWyMzMxIkTJ+Dt7Q0A+OCDD9CpUye0bdsWcrkc+/fvV77WUBhu6lknV0vEZuQj4j7DDRFRU7J27Vq8+uqr6N69O2xsbLBo0SJRbhy7aNEipKamYsqUKdDR0cGMGTMwYMAA6OhUf0ruaW/PUzo6OigpKcHmzZvx5ptvYujQoSgqKsJzzz2HgwcPKk+RlZaWIigoCA8ePICZmRkGDhyITz/9FEDZvXqWLFmC+/fvw9DQED179sSPP/5Y/zv+DxJB7BN1jSwnJwfm5ubIzs5ukMvCf7qUgEW/3kAXdyv8PDOg3rdPRKRpCgsLERcXB3d3dxgYGIhdTpOjUCjg7e2NsWPHYuXKlWKXU6Wq/lZq8/3Nnpt61sm1bL6r64lZKCpRQF9X9InXiYioCYmPj8eRI0fQq1cvyOVyfPnll4iLi8OECRPELq3R8Ju3nrWwMYaFkR7kJQrcTuE8VkRE1LikUim2bNmCzp07IzAwEDdu3MDRo0cbfJyLOmHPTT2TSiXo5GKJY3fSEXH/MTo4W4hdEhERNSHOzs4IDw8XuwxRseemAXRyK7sk/DLvd0NERNToGG4aQCeXsnATEZ8p+o2ViIjUBf89pOrU198Iw00D8HW2gJ6OBA9z5XiQWX6mciKipuTp5cIFBQUiV0Lq7undnGty2XpVOOamARjo6aCtkzmuJWYhIv4xnK2MxC6JiEg0Ojo6sLCwUM5/ZGRkVKMbylHTolAo8PDhQxgZGUFX99niCcNNA/F3tSwLN/czMdJPvNnKiYjUgYODAwBUOSEkkVQqhYuLyzOHX4abBtLJ1RLfno3joGIiIpTdxt/R0RF2dnYoLi4WuxxSU/r6+pBKn33EDMNNA3l6xVRUWi5yCothZqBXzRpERNpPR0fnmcdTEFWHA4obiJ2pAVysjCAIwNWELLHLISIiajIYbhpQJ1fe74aIiKixMdw0oL/DzWORKyEiImo6GG4akP9f426uJmShpFQhcjVERERNA8NNA/K0M4WpTBcFRaW4k5ordjlERERNAsNNA9KRSuDHcTdERESNiuGmgfm7/j3PFBERETU8hpsG9jTcXL7PQcVERESNgeGmgfk6W0BHKkFydiGSsziJJhERUUNjuGlgxjJdeDuaAuC4GyIiosbAcNMI/F2tADDcEBERNQaGm0bQkVdMERERNRqGm0bwdFDx7ZQc5MtLRK6GiIhIuzHcNAInC0M4mRugVCHgemKW2OUQERFpNYabRtLJrWzcDe93Q0RE1LAYbhpJJxcLABx3Q0RE1NAYbhqJ/189N1cSMqFQCCJXQ0REpL0YbhqJl4MpjPR1kFtYguh0TqJJRETUUEQPN0lJSZg0aRKsra1haGgIHx8fREREVNr+5MmTkEgk5R6pqamNWHXt6epI0cHZAgBPTRERETUkXTHfPDMzE4GBgejduzcOHToEW1tbxMTEwNLSstp1o6KiYGZmpnxuZ2fXkKXWC39XS5y79wiX72diYldXscshIiLSSqKGm9WrV8PZ2RmbN29WLnN3d6/RunZ2drCwsGigyhoGr5giIiJqeKKeltq3bx/8/f0xZswY2NnZwc/PDxs3bqzRuh06dICjoyP69euH8PDwStvJ5XLk5OSoPMTi52IBiQRIeFyA9NxC0eogIiLSZqKGm9jYWISEhMDT0xOhoaGYPXs25s2bh61bt1a6jqOjI9avX49ff/0Vv/76K5ydnfH888/jypUrFbYPDg6Gubm58uHs7NxQu1MtMwM9tLYvm0TzCntviIiIGoREEATRrkvW19eHv78/zp07p1w2b948XLp0CefPn6/xdnr16gUXFxds27at3GtyuRxyuVz5PCcnB87OzsjOzlYZs9NY3t1zAz/8kYDXe7jjvaFtGv39iYiINFFOTg7Mzc1r9P0tas+No6Mj2rRR/YL39vZGQkJCrbbTpUsX3L17t8LXZDIZzMzMVB5i8ncrGyzNcTdEREQNQ9RwExgYiKioKJVl0dHRcHWt3ZVE165dg6OjY32W1mA6uZQNKr6VnI3C4lKRqyEiItI+ol4ttWDBAnTv3h2rVq3C2LFjcfHiRWzYsAEbNmxQtlmyZAmSkpLw/fffAwDWrVsHd3d3tG3bFoWFhfj2229x/PhxHDlyRKzdqBVnK0PYmsrwMFeOPx9ko4u7ldglERERaRVRe246d+6MPXv2YOfOnWjXrh1WrlyJdevWYeLEico2KSkpKqepioqK8Pbbb8PHxwe9evXC9evXcfToUfTp00eMXag1iUQCf9enp6Yei1wNERGR9hF1QLEYajMgqaF8eyYWHx6IRF9vO3w7tbMoNRAREWkSjRlQ3FR1+qvn5nJ8JppYtiQiImpwDDciaOtkDpmuFJkFxbj3MF/scoiIiLQKw40I9HWl8FVOoslxN0RERPWJ4UYk/zw1RURERPWH4UYkf18xxXBDRERUnxhuRNLRpSzcxD7Mx+P8IpGrISIi0h4MNyKxNNaHh60xAE6iSUREVJ8YbkTk71p2d2KemiIiIqo/DDci6uT2dFAxr5giIiKqLww3Inp6xdT1B9koKlGIXA0REZF2YLgRUQsbY1gZ66OoRIGbydlil0NERKQVGG5EJJFIlFdNXb7PcTdERET1geFGZP5unCGciIioPjHciOzvOxVncRJNIiKiesBwIzKfZubQ15EiI0+OhMcFYpdDRESk8RhuRGagp4N2zcwAABEcd0NERPTMGG7UgPLUVALDDRER0bNiuFEDnf66UzGvmCIiInp2DDdq4GnPTXR6LrKfFItcDRERkWZjuFEDtqYyuFkbQRCAKzw1RURE9EwYbtREx796bzhDOBER0bNhuFETyhnCOe6GiIjomTDcqImndyq+lpiF4lJOoklERFRXDDdqoqWtCcwMdPGkuBR3UnLFLoeIiEhjMdyoCalUohx3w3mmiIiI6o7hRo34K8MNx90QERHVFcONGuEVU0RERM+O4UaNdHC2gI5UgpTsQiRlPRG7HCIiIo3EcKNGjPR10dbp6SSaHHdDRERUFww3akY5iSZPTREREdUJw42aYbghIiJ6NqKHm6SkJEyaNAnW1tYwNDSEj48PIiIiarRueHg4dHV10aFDh4YtshE9vVNxZEoO8uQlIldDRESkeUQNN5mZmQgMDISenh4OHTqE27dvY82aNbC0tKx23aysLEyZMgV9+vRphEobj4O5AZpZGEIhANcSssQuh4iISOPoivnmq1evhrOzMzZv3qxc5u7uXqN1Z82ahQkTJkBHRwd79+5toArF0cnVEklZT3A5PhM9PG3ELoeIiEijiNpzs2/fPvj7+2PMmDGws7ODn58fNm7cWO16mzdvRmxsLJYuXdoIVTa+p/NM8U7FREREtSdquImNjUVISAg8PT0RGhqK2bNnY968edi6dWul68TExGDx4sXYvn07dHWr73iSy+XIyclReai7p4OKryZkoVQhiFwNERGRZhE13CgUCnTs2BGrVq2Cn58fZsyYgenTp2P9+vUVti8tLcWECROwfPlytGrVqkbvERwcDHNzc+XD2dm5PnehQXg5mMFYXwd58hJEp3ESTSIiotoQNdw4OjqiTZs2Ksu8vb2RkJBQYfvc3FxERETgjTfegK6uLnR1dbFixQpcv34durq6OH78eLl1lixZguzsbOUjMTGxQfalPulIJfBz4TxTREREdSHqgOLAwEBERUWpLIuOjoarq2uF7c3MzHDjxg2VZV9//TWOHz+OX375pcLByDKZDDKZrP6KbiSdXC1x9m4GLt9/jMndKv59EBERUXmihpsFCxage/fuWLVqFcaOHYuLFy9iw4YN2LBhg7LNkiVLkJSUhO+//x5SqRTt2rVT2YadnR0MDAzKLdd0fw8qZs8NERFRbYh6Wqpz587Ys2cPdu7ciXbt2mHlypVYt24dJk6cqGyTkpJS6WkqbdbB2QJSCfAg8wnScgrFLoeIiEhjSARBaFKX4+Tk5MDc3BzZ2dkwMzMTu5wqDfrsDCJTcvD1xI4Y7OModjlERESiqc33t+jTL1Dl/P+6JDziPk9NERER1RTDjRpTTqKZwHBDRERUUww3auxpuLmVlI0nRaUiV0NERKQZGG7UWHNLQ9ibyVCiEHD9QZbY5RAREWkEhhs1JpFI4O9qBQC4zEvCiYiIaoThRs11fDruhuGGiIioRhhu1Jz/P8KNgpNoEhERVYvhRs21cTKDgZ4U2U+Kce9hntjlEBERqT2GGzWnpyOFb3MLADw1RUREVBMMNxqA80wRERHVHMONBuAVU0RERDXHcKMBOrqU9dzEZeTjUZ5c5GqIiIjUG8ONBjA30oOnnQkA9t4QERFVh+FGQzwdd8NwQ0REVDWGGw3Rxb1s3M3ea0nIl5eIXA0REZH6YrjREIPaOcLZyhBpOXKsP3VP7HKIiIjUFsONhjDQ08G7g9sAAL45HYvExwUiV0RERKSeGG40yIC29ujuYY2iEgWCD0WKXQ4REZFaYrjRIBKJBB8MawOpBDh4IxXn7mWIXRIREZHaYbjRMF4OZpjY1RUAsOL32ygpVYhcERERkXphuNFAb/VrBXNDPdxJzcWPlxLFLoeIiEitMNxoIEtjfbzVrxUAYM2RKGQXFItcERERkfpguNFQE7u6oJW9CTILivHp0WixyyEiIlIbDDcaSldHig+GtgUAbLsQj5i0XJErIiIiUg8MNxqsh6cN+rWxR6lCwIr9tyEIgtglERERiY7hRsO9N8Qb+jpSnInJwLHIdLHLISIiEh3DjYZztTbGaz3dAQArD9yGvKRU5IqIiIjExXCjBYJ6t4StqQzxjwqwOfy+2OUQERGJiuFGC5jIdLFooBcA4ItjMUjPLRS5IiIiIvEw3GiJUX7N4OtsgfyiUnx8OErscoiIiETDcKMlpFIJlg4rmzV81+UHuJ6YJW5BREREImG40SIdXSwxyq8ZAGD577d4aTgRETVJooebpKQkTJo0CdbW1jA0NISPjw8iIiIqbX/27FkEBgYq23t5eeHTTz9txIrV26JBXjDS18GVhCz8di1Z7HKIiIgana6Yb56ZmYnAwED07t0bhw4dgq2tLWJiYmBpaVnpOsbGxnjjjTfQvn17GBsb4+zZs5g5cyaMjY0xY8aMRqxePdmbGSCod0t8HBqF4EOR6NfGHsYyUQ8zERFRo5IIIp67WLx4McLDw3HmzJln2s6oUaNgbGyMbdu2Vds2JycH5ubmyM7OhpmZ2TO9r7oqLC5Fv09PIfHxE8x9oSXe7t9a7JKIiIieSW2+v0U9LbVv3z74+/tjzJgxsLOzg5+fHzZu3FirbVy9ehXnzp1Dr169KnxdLpcjJydH5aHtDPR08O7gssHF35yOReLjApErIiIiajyihpvY2FiEhITA09MToaGhmD17NubNm4etW7dWu27z5s0hk8ng7++PoKAgvP766xW2Cw4Ohrm5ufLh7Oxc37uhlga0tUd3D2sUlSiw6mCk2OUQERE1GlFPS+nr68Pf3x/nzp1TLps3bx4uXbqE8+fPV7luXFwc8vLycOHCBSxevBhffvklxo8fX66dXC6HXC5XPs/JyYGzs7NWn5Z6Kio1F4M+Ow2FAOyY3hXdPWzELomIiKhONOa0lKOjI9q0aaOyzNvbGwkJCdWu6+7uDh8fH0yfPh0LFizAsmXLKmwnk8lgZmam8mgqWjuYYlI3VwDAit9vo6RUIXJFREREDU/UcBMYGIioKNW76UZHR8PV1bVW21EoFCq9M/S3BX1bwdxQD3dSc7HzUqLY5RARETU4UcPNggULcOHCBaxatQp3797Fjh07sGHDBgQFBSnbLFmyBFOmTFE+/+qrr/D7778jJiYGMTEx+O677/DJJ59g0qRJYuyC2rM01sdb/VoBANYeiUJWQZHIFRERETUsUW+A0rlzZ+zZswdLlizBihUr4O7ujnXr1mHixInKNikpKSqnqRQKBZYsWYK4uDjo6urCw8MDq1evxsyZM8XYBY0wsasLfvgjHtFpeVh3NAbLXmwrdklEREQNRtQBxWJoCve5qUj43QxM/PYP6EglOPxmT3jam4pdEhERUY1pzIBiajyBLW3Qv409ShUCVuy/zXmniIhIazHcNCHvDvGGvo4UZ2IycDQyXexyiIiIGgTDTRPiam2M13q6AwA+PHAb8pJSkSsiIiKqfww3TUxQ75awM5Uh/lEBNoffF7scIiKiesdw08SYyHSxaKAXAOCLYzFIzy0UuSIiIqL6xXDTBI30awZfZwvkF5Xio8NR1a9ARESkQRhumiCpVIJlw8qmvfjl8gNcT8wStyAiIqJ6xHDTRPm5WGJUx2YAgGW/3+Kl4UREpDUYbpqwRQO9YKSvg6sJWfjtWrLY5RAREdULhpsmzN7MAEG9WwIAgg9FIl9eInJFREREz47hpol7rYc7XKyMkJYjR8jJe2KXQ0RE9MwYbpo4Az0dvDvEGwCw4UwsEh8XiFwRERHRs2G4IfRvY4/AltYoKlFgzRFeGk5ERJqN4YYgkUiwZFBZ781v15NxJzVH5IqIiIjqjuGGAADtmpljiI8jBAFYcyRa7HKIiIjqjOGGlBb0awWpBAi7nYYrCZlil0NERFQnDDek1NLOBKM7NgcAfBLKsTdERKSZGG5IxZt9PaGvI8W5e48QfjdD7HKIiIhqjeGGVDS3NMKEri4AgI9CozgtAxERaRyGGyonqHdLGOrp4HpiFsJup4ldDhERUa0w3FA5tqYyvNrDDQDwyZEolCrYe0NERJqD4YYqNKOnB8wMdBGdlod915PELoeIiKjGGG6oQuZGepj1vAcA4NOwGBSVKESuiIiIqGYYbqhS07q7wcZEhoTHBfg5IlHscoiIiGqkTuEmMTERDx48UD6/ePEi5s+fjw0bNtRbYSQ+I31dzH2hJQDg82MxeFJUKnJFRERE1atTuJkwYQJOnDgBAEhNTUW/fv1w8eJFvPvuu1ixYkW9FkjiermLM5pZGCI9V47vz98XuxwiIqJq1Snc3Lx5E126dAEA/Pzzz2jXrh3OnTuHH374AVu2bKnP+khkMl0dLOjXCgAQcuoecgqLRa6IiIioanUKN8XFxZDJZACAo0eP4sUXXwQAeHl5ISUlpf6qI7Uw0q8ZWtqZIKugGN+eiRO7HCIioirVKdy0bdsW69evx5kzZxAWFoaBAwcCAJKTk2FtbV2vBZL4dKQSvP1X7813Z2LxKE8uckVERESVq1O4Wb16Nb755hs8//zzGD9+PHx9fQEA+/btU56uIu0ysJ0DfJqZI7+oFF+fvCd2OURERJWSCHWcPKi0tBQ5OTmwtLRULrt//z6MjIxgZ2dXbwXWt5ycHJibmyM7OxtmZmZil6NRTkc/xJRNF6GvK8XJhc/DycJQ7JKIiKiJqM33d516bp48eQK5XK4MNvHx8Vi3bh2ioqJqHWySkpIwadIkWFtbw9DQED4+PoiIiKi0/e7du9GvXz/Y2trCzMwMAQEBCA0NrctuUC319LRBV3crFJUo8MXxGLHLISIiqlCdws3w4cPx/fffAwCysrLQtWtXrFmzBiNGjEBISEiNt5OZmYnAwEDo6enh0KFDuH37NtasWaPSG/Rvp0+fRr9+/XDw4EFcvnwZvXv3xrBhw3D16tW67ArVgkQiwX8GtAYA/BzxAHEZ+SJXREREVF6dTkvZ2Njg1KlTaNu2Lb799lt88cUXuHr1Kn799Vd88MEHiIyMrNF2Fi9ejPDwcJw5c6bWhf9T27ZtMW7cOHzwwQfVtuVpqWf36pZLOH4nHcN8nfDFeD+xyyEioiagwU9LFRQUwNTUFABw5MgRjBo1ClKpFN26dUN8fHyNt7Nv3z74+/tjzJgxsLOzg5+fHzZu3FirWhQKBXJzc2FlZVXh63K5HDk5OSoPejYL+5f13vx+PRm3k/n7JCIi9VKncNOyZUvs3bsXiYmJCA0NRf/+/QEA6enpteoNiY2NRUhICDw9PREaGorZs2dj3rx52Lp1a4238cknnyAvLw9jx46t8PXg4GCYm5srH87OzjXeNlWsjZMZhvk6AQDWHIkSuRoiIiJVdTot9csvv2DChAkoLS3FCy+8gLCwMABlQeL06dM4dOhQjbajr68Pf39/nDt3Trls3rx5uHTpEs6fP1/t+jt27MD06dPx22+/oW/fvhW2kcvlkMv/vi9LTk4OnJ2deVrqGcU+zEO/T0+jVCHg19kB6ORacc8ZERFRfWjw01IvvfQSEhISEBERoXKlUp8+ffDpp5/WeDuOjo5o06aNyjJvb28kJCRUu+6PP/6I119/HT///HOlwQYAZDIZzMzMVB707FrYmmBMp+YAgI8OR6GOdxQgIiKqd3UKNwDg4OAAPz8/JCcnK2cI79KlC7y8vGq8jcDAQERFqZ7WiI6Ohqura5Xr7dy5E6+88gp27tyJIUOG1L54qhfz+nhCX1eKP+Ie40xMhtjlEBERAahjuFEoFFixYgXMzc3h6uoKV1dXWFhYYOXKlVAoFDXezoIFC3DhwgWsWrUKd+/exY4dO7BhwwYEBQUp2yxZsgRTpkxRPt+xYwemTJmCNWvWoGvXrkhNTUVqaiqys7Prsiv0DJwsDDG5W1kQ/TiUvTdERKQe6hRu3n33XXz55Zf43//+h6tXr+Lq1atYtWoVvvjiC7z//vs13k7nzp2xZ88e7Ny5E+3atcPKlSuxbt06TJw4UdkmJSVF5TTVhg0bUFJSgqCgIDg6Oiofb775Zl12hZ7RnOc9YKyvgxtJ2Qi9lSp2OURERHUbUOzk5IT169crZwN/6rfffsOcOXOQlJRUbwXWN97npv6tPRKFz4/fRUs7E4TOfw46UonYJRERkZZp8AHFjx8/rnBsjZeXFx4/flyXTZIGe/25FrAw0sPd9Dzsuaq+wZaIiJqGOoUbX19ffPnll+WWf/nll2jfvv0zF0WaxcxAD7N7eQAAPg2LhrykVOSKiIioKdOty0offfQRhgwZgqNHjyIgIAAAcP78eSQmJuLgwYP1WiBphikBbvjubBySsp7gp0uJmBLgJnZJRETURNWp56ZXr16Ijo7GyJEjkZWVhaysLIwaNQq3bt3Ctm3b6rtG0gCG+jqY28cTAPD5sbsoKCoRuSIiImqq6jSguDLXr19Hx44dUVqqvqclOKC44RSVKNBn7UkkPn6Cdwa2xpznW4pdEhERaYkGH1BMVBF9XSne6tcKALD+5D1kPykWuSIiImqKGG6oXr3o2wyt7E2QU1iCjadjxS6HiIiaIIYbqlc6Ugne7t8aALApPA4Pc+XVrEFERFS/anW11KhRo6p8PSsr61lqIS3Rv409fJub4/qDbHx14i6WvdhW7JKIiKgJqVXPjbm5eZUPV1dXlXmgqGmSSCT4z4Cymzzu+CMBDzILRK6IiIiaklr13GzevLmh6iAt08PTBt09rHHu3iN8fiwGH73kK3ZJRETURHDMDTWYhQPKxt78cvkB7j3ME7kaIiJqKhhuqMF0dLFEX297KARgbVi02OUQEVETwXBDDWrhgFaQSIADf6bgZlK22OUQEVETwHBDDcrLwQzDfZ0AAJ8ciRK5GiIiagoYbqjBze/bCrpSCU5GPcTqw3dQjzN+EBERlcNwQw3OzcYY7wwsG1wccvIe3t51HcWlCpGrIiIibcVwQ41ixnMe+Gh0e+hIJdh9JQmvbrmEPDlnDiciovrHcEONZmxnZ3w7xR+Gejo4E5OBlzec5/QMRERU7xhuqFH19rLDzhndYGWsj5tJORgVEo64jHyxyyIiIi3CcEONroOzBX6d3R0uVkZIfPwEo0PO4VpilthlERGRlmC4IVG42xjj19nd4dPMHI/zizB+wwUcv5MmdllERKQFGG5INLamMvw4oxuea2WLJ8WlmP79Zfx4MUHssoiISMMx3JCojGW6+G6qP0Z3bI5ShYDFu2/gs6MxvBcOERHVGcMNiU5PR4pPxrRHUG8PAMCnR6Pxf3tuoIT3wiEiojpguCG1IJFI8J8BXlg5vC0kEmDnxUTM2n4ZT4pKxS6NiIg0DMMNqZXJAW4ImdgJMl0pjkamY8K3F/A4v0jssoiISIMw3JDaGdjOAT+83hXmhnq4mpCFl0LOIfFxgdhlERGRhmC4IbXk72aFX2YFwMncALEZ+RgVcg43k7LFLouIiDQAww2pLU97U+yeEwgvB1M8zJVj3DfncSbmodhlERGRmmO4IbXmYG6An2cFIKCFNfKLSvHK5kvYc/WB2GUREZEaEz3cJCUlYdKkSbC2toahoSF8fHwQERFRafuUlBRMmDABrVq1glQqxfz58xuvWBKFmYEetrzaGcN8nVCiELDgp+tYf+oe74VDREQVEjXcZGZmIjAwEHp6ejh06BBu376NNWvWwNLSstJ15HI5bG1t8d5778HX17cRqyUxyXR18Nm4Dni9hzsA4H+H7mD577dRqmDAISIiVbpivvnq1avh7OyMzZs3K5e5u7tXuY6bmxs+++wzAMCmTZsatD5SL1KpBO8NbQMHcwN8eCASW87dR3puIdaO7QADPR2xyyMiIjUhas/Nvn374O/vjzFjxsDOzg5+fn7YuHGjmCWRBni9Zwt8Pt4P+jpSHLyRiimbLiK7oFjssoiISE2IGm5iY2MREhICT09PhIaGYvbs2Zg3bx62bt1ab+8hl8uRk5Oj8iDN96KvE7a82hmmMl1cjHuMMd+cQ3LWE7HLIiIiNSBquFEoFOjYsSNWrVoFPz8/zJgxA9OnT8f69evr7T2Cg4Nhbm6ufDg7O9fbtklc3T1s8NPMANiZyhCdlodBn53B1nP3OScVEVETJ2q4cXR0RJs2bVSWeXt7IyEhod7eY8mSJcjOzlY+EhMT623bJL42TmbYPac72jiaIftJMZbuu4XBn5/B2ZgMsUsjIiKRiBpuAgMDERUVpbIsOjoarq6u9fYeMpkMZmZmKg/SLs0tjbDvjUB8OKIdLI30EJ2Wh0nf/YHp30cg/lG+2OUREVEjEzXcLFiwABcuXMCqVatw9+5d7NixAxs2bEBQUJCyzZIlSzBlyhSV9a5du4Zr164hLy8PDx8+xLVr13D79u3GLp/UiK6OFJO6ueLEwucxrbsbdKQShN1OQ7+1p7H68B3kyUvELpGIiBqJRBD5Tmj79+/HkiVLEBMTA3d3d7z11luYPn268vVp06bh/v37OHnypHKZRCIptx1XV1fcv3+/2vfLycmBubk5srOz2YujxWLScrFi/22c+ev0lK2pDIsGemGUXzNIpeX/foiISL3V5vtb9HDT2Bhumg5BEHAsMh0rD9xG/KOyWcV9nS2wdFgbdHSp/EaRRESkfhhuqsBw0/TIS0qxJfw+vjh+V3l6apRfM7wz0AsO5gYiV0dERDXBcFMFhpumKz23EB8fjsKuy2UTbxrp6yCod0u81sOddzgmIlJzDDdVYLih64lZWP77LVxJyAIANLc0xHtDvDGgrUOF47mIiEh8DDdVYLghoGw8zr7ryQg+eAepOYUAgO4e1vhgWBt4OfDvgohI3TDcVIHhhv6poKgEISfv4ZvTsSgqUUAqASZ2dcVb/VrB0lhf7PKIiOgvDDdVYLihiiQ+LkDwoUgcvJEKADA31MOCvp6Y2M0Vejqi3g6KiIjAcFMlhhuqyvl7j7D891u4k5oLAPC0M8EHw9qgp6etyJURETVtDDdVYLih6pQqBPx4KQGfhEYhs6AYANDX2x7LXmyD5pZGIldHRNQ01eb7m/3tRP+iI5VgYldXnFzYG68Elk3lcDQyDcO+OMsJOYmINADDDVElzI30sHRYWxx+syd8mpkjs6AYUzb9gQ2n76GJdXgSEWkUhhuianjam2LXrAC81Kk5FAKw6uAdzN15FQVFnIyTiEgdMdwQ1YCBng4+fqk9Vg5vC12pBPv/TMGor88h/lG+2KUREdG/MNwQ1ZBEIsHkADfsmN4NNiYy3EnNxbAvzuJkVLrYpRER0T8w3BDVUhd3K+yf2wMdnC2QU1iCV7Zcwlcn7nIcDhGRmmC4IaoDB3MD/DSzG8Z3cYYgAB+HRmHOD1eUs44TEZF4GG6I6kimq4PgUe2xaqQP9HQkOHQzFSO/CkdcBsfhEBGJieGG6BlN6OqCH2cEwM5Uhpj0PLz45Vkcv5MmdllERE0Www1RPejkaon9c3vA39USuYUleG1rBD47GgOFguNwiIgaG8MNUT2xMzPAjundMLmbKwQB+PRoNGZuv4zcwmKxSyMialIYbojqkb6uFCtHtMNHL7WHvq4UYbfTMPyrcNxNzxO7NCKiJoPhhqgBjPV3xq6ZAXA0N0Dsw3yM+CocobdSxS6LiKhJYLghaiC+zhb4fW4PdHW3Qp68BDO3XcbaI1Ech0NE1MAYbogakI2JDNtf74pXAt0AAJ8fv4vXtl5C9hOOwyEiaigMN0QNTE9HiqXD2uLTcb6Q6UpxIuohhn95FtFpuWKXRkSklRhuiBrJSL/m+HV2dzSzMMT9RwUY8VU4Dt5IEbssIiKtw3BD1IjaNTPH73N7ILClNQqKSjHnhytYffgOSjkOh4io3jDcEDUyK2N9bH2lC2Y81wIAEHLyHqZtvoiMPLnIlRERaQeGGyIR6OpI8X+DvfH5eD8Y6ElxJiYDAz49zcvFiYjqAcMNkYhe9HXC3qBAeDmY4lF+EWZuu4y3f76OHN7VmIiozhhuiETm5WCG394IxKxeHpBKgF+vPMDAT08j/G6G2KUREWkkhhsiNSDT1cHiQV74eWYAXK2NkJxdiInf/oFl+27hSVGp2OUREWkUhhsiNeLvZoWD83piUjcXAMCWc/cx5PMzuJqQKXJlRESaQ/Rwk5SUhEmTJsHa2hqGhobw8fFBREREleucPHkSHTt2hEwmQ8uWLbFly5bGKZaoERjLdPHhCB98/2oXOJgZIDYjH6NDzuGT0CgUlSjELo+ISO2JGm4yMzMRGBgIPT09HDp0CLdv38aaNWtgaWlZ6TpxcXEYMmQIevfujWvXrmH+/Pl4/fXXERoa2oiVEzW851rZInT+cxjRwQkKAfjyxF2M+Cocd1JzxC6NiEitSQRBEO3uYYsXL0Z4eDjOnDlT43UWLVqEAwcO4ObNm8plL7/8MrKysnD48OFq18/JyYG5uTmys7NhZmZWp7qJGtvBGyl4d88NZBYUQ19Hirf6t8L0ni2gI5WIXRoRUaOozfe3qD03+/btg7+/P8aMGQM7Ozv4+flh48aNVa5z/vx59O3bV2XZgAEDcP78+Qrby+Vy5OTkqDyINM1gH0eELngOfb3tUFSqwP8O3cG4b84j/lG+2KUREakdUcNNbGwsQkJC4OnpidDQUMyePRvz5s3D1q1bK10nNTUV9vb2Ksvs7e2Rk5ODJ0+elGsfHBwMc3Nz5cPZ2bne94OoMdiZGmDjFH989FJ7mMh0ERGfiUGfncH2C/EQsQOWiEjtiBpuFAoFOnbsiFWrVsHPzw8zZszA9OnTsX79+np7jyVLliA7O1v5SExMrLdtEzU2iUSCsf7OODy/J7q1sEJBUSne23sTUzdfQmp2odjlERGpBVHDjaOjI9q0aaOyzNvbGwkJCZWu4+DggLS0NJVlaWlpMDMzg6GhYbn2MpkMZmZmKg8iTdfc0gg7Xu+GD4a2gUxXitPRD9H/01P47VoSe3GIqMkTNdwEBgYiKipKZVl0dDRcXV0rXScgIADHjh1TWRYWFoaAgIAGqZFIXUmlErzawx0H5vWEb3Nz5BSW4M0fryFoxxU8zi8SuzwiItGIGm4WLFiACxcuYNWqVbh79y527NiBDRs2ICgoSNlmyZIlmDJlivL5rFmzEBsbi3feeQd37tzB119/jZ9//hkLFiwQYxeIRNfSzgS/zu6Ot/q1gq5UgoM3UtH/09M4FplW/cpERFpI1HDTuXNn7NmzBzt37kS7du2wcuVKrFu3DhMnTlS2SUlJUTlN5e7ujgMHDiAsLAy+vr5Ys2YNvv32WwwYMECMXSBSC7o6Uszr44m9QYHwtDNBRp4cr22NwDu/XEcuJ+EkoiZG1PvciIH3uSFtV1hcirVh0dh4JhaCADSzMMQ7A1vD3swAJjJdmMh0YSzThamBLmS6UkgkvFcOEam/2nx/M9wQaamLcY/x9q5rSHxc/hYJT+lIJTDW1ykLPQZlocdEpgtj/bLnZUFIByYyPZjIdGD8NBj99d+nIcnOVMaQREQNiuGmCgw31JTkyUvwaVg0LsY9Rr68BHnyEuTLS5BfzzONezmYIniUD/xcKp86hYjoWTDcVIHhhghQKATkF5UgX16qDDx5/ww/8hLkKn8ua5NXWIL8or/b5BX+vY5CACQSYGqAGxYOaA0Tma7Yu0hEWqY239/8F4ioCZJKJTA10IOpgd4zb+tRnhz/PRCJ3VeTsOXcfYTeSsWK4e3Qr4199SsTETUAUa+WIiLNZ20iw9pxHbDttS5wsTJCSnYhpn8fgdnbLyM9h3dNJqLGx3BDRPWip6ctQuc/h1m9PKAjleDQzVT0WXsK2y/EQ6FoUme/iUhkDDdEVG8M9XWweJAXfn+jB3ybmyO3sATv7b2Jsd+cR0xartjlEVETwXBDRPWujZMZds8JxNJhbWCkr4OI+EwM/vwM1oZFo7C4fq/UIiL6N4YbImoQOlIJXgl0R9hbvdDHyw7FpQI+PxaDwZ+fwYXYR2KXR0RajOGGiBpUMwtDfDvVH19P7AhbUxliH+bj5Q0XsPjXP5FdwKkhiKj+MdwQUYOTSCQY7OOIo2/1woSuLgCAHy8los/aU/j9ejKa2O22iKiBMdwQUaMxN9TDqpE+2DUrAC3/muBz7s6reHXLJTzILBC7PCLSEgw3RNToOrtZ4cC8HljQtxX0daQ4EfUQ/daexrdnYlFSqhC7PCLScAw3RCQKma4O3uzriYNv9kQXNys8KS7FhwciMfLrc7iZlC12eUSkwRhuiEhULe1M8OOMbvjfKB+YGejiRlI2hn8VjlUHI1FQVCJ2eUSkgRhuiEh0UqkEL3dxwdG3e2Foe0eUKgRsOB2LAetO41T0Q7HLIyINw1nBiUjtHL+Thvf33kJS1hMAQLcWVpjW3Q19ve2hq8P/JyNqimrz/c1wQ0RqKV9egjVHorH1/H2U/jU3laO5ASZ1c8W4zs6wMZGJXCERNSaGmyow3BBpluSsJ9jxRwJ2XkzAo/wiAIC+jhRDfR0xNcANvs4W4hZIRI2C4aYKDDdEmkleUoqDN1Kw5Vw8ridmKZf7OltgaoArhrR3hExXR7wCiahBMdxUgeGGSPNdS8zC9+fvY//1FBT9dV8ca2N9jO/iggldXeBkYShyhURU3xhuqsBwQ6Q9MvLk+OlSIrZfiEdKdiGAsgk7+7exx5QAN3RrYQWJRCJylURUHxhuqsBwQ6R9SkoVOBqZhq3n4nH+HzOOt7Y3xZTurhjRoRmMZboiVkhEz4rhpgoMN0TaLSo1F9+fv4/dV5LwpLgUAGBqoIsxnZwxOcAV7jbGIldIRHXBcFMFhhuipiH7STF+vfwA35+/j/uP/p6Us1crW0zr7oZerWwhlfKUFZGmYLipAsMNUdOiUAg4HfMQ35+Px4modDz9F8/V2giTu7liTCdnmBvpiVskEVWL4aYKDDdETdf9jHxsvxCPnyMSkVNYNm+Vvq4UXd2t8JynLXq1toWnnQkHIROpIYabKjDcEFFBUQl+u5aMrefu405qrsprDmYGeK6VDXq1skOPljbs1SFSEww3VWC4IaKnBEHAvYd5OBn1EKdjMvBH7CPISxTK16USoIOzBZ5rZYterWzRvrkFdDhOh0gUDDdVYLghosoUFpfij7jHOBX1EKdjHuJuep7K6xZGeujR0kYZduzNDESqlKjpYbipAsMNEdVUUtYTnI5+iNPRD3H2bgZy/xqn85SXg6ky6Pi7WXL6B6IGpDHhZtmyZVi+fLnKstatW+POnTsVti8uLkZwcDC2bt2KpKQktG7dGqtXr8bAgQNr/J4MN0RUFyWlClxLzMKpv8LOn0nZ+Oe/noZ6OgjwsMZznjbo1doObtZGHJhMVI9q8/0t+i0727Zti6NHjyqf6+pWXtJ7772H7du3Y+PGjfDy8kJoaChGjhyJc+fOwc/PrzHKJaImSldHCn83K/i7WeHt/q3xOL8IZ2Ie4nR0Bk7HPMTDXDmO30nH8TvpwO+34WxliOc8bdG/rQN6trThPXWIGpHoPTd79+7FtWvXatTeyckJ7777LoKCgpTLRo8eDUNDQ2zfvr1G22DPDRHVN0EQEJmSq+zViYh/jOLSv/9pdbEywsSuLhjj7wwrY30RKyXSXBrVcxMTEwMnJycYGBggICAAwcHBcHFxqbCtXC6HgYHqAD5DQ0OcPXu20u3L5XLI5XLl85ycnPopnIjoLxKJBG2czNDGyQyzn/dAvrwE5+89womodOy7noyExwUIPnQHa8KiMdTHERO7uaKjiwVPWxE1EFF7bg4dOoS8vDy0bt0aKSkpWL58OZKSknDz5k2YmpqWaz9hwgRcv34de/fuhYeHB44dO4bhw4ejtLRUJcD8U0XjegCw54aIGsWTolL8fj0Z2y7E40ZStnJ5G0czTA5wxfAOTjDSF/3/M4nUnsYMKP63rKwsuLq6Yu3atXjttdfKvf7w4UNMnz4dv//+OyQSCTw8PNC3b19s2rQJT548qXCbFfXcODs7M9wQUaO7npiFbRfi8fv1ZOX9dExluhjdqTkmdXNBS7vy/1NHRGU0NtwAQOfOndG3b18EBwdX2qawsBCPHj2Ck5MTFi9ejP379+PWrVs12j7H3BCR2LIKivDL5QfYfiFeZVLPbi2sMLmbG/q3tYeejlTEConUj0aNufmnvLw83Lt3D5MnT66ynYGBAZo1a4bi4mL8+uuvGDt2bCNVSET07CyM9PF6zxZ4NdAdZ+9mYPuFeByNTMOF2Me4EPsYtqYyjO/sjPFdXeBobih2uUQaR9Sem4ULF2LYsGFwdXVFcnIyli5dimvXruH27duwtbXFlClT0KxZM2Uvzh9//IGkpCR06NABSUlJWLZsGeLi4nDlyhVYWFjU6D3Zc0NE6ig56wl2XkzAzouJyMgrO5UulQB9ve0xOcAVgR68nJyaNo3puXnw4AHGjx+PR48ewdbWFj169MCFCxdga2sLAEhISIBU+nfXbGFhId577z3ExsbCxMQEgwcPxrZt22ocbIiI1JWThSHe7t8ac1/wxJHbqdh+IR4XYh/jyO00HLmdBjdrI0zq5oqXOjWHhREvJyeqitqNuWlo7LkhIk0Rk5aL7RfisftKEnLlZVM/yHSlGObrhMndXOHrbCFugUSNSKMHFDc0hhsi0jT58hL8dq3scvLIlL/v1eXTzBzdPaxhZ2YABzMDOJjLYGdqAHszA+jrckAyaReGmyow3BCRphIEAVcSsrD9QjwO/JmColJFpW2tjfX/Cj0yOJgbwM7UAA7mZSHIzkwGBzMDWBnr80aCpDEYbqrAcENE2uBRnhwHbqQg/lEBUnMKkZZdiLTcQqRly6sMPf+kryOFnZkM9n/1/NibGcD+rzBkb2aAZhaGaG5pyABEakFjBhQTEVHdWJvIMCXArdxyQRCQWVCMtJzCv0NPjrzs55xCpGYXIj23EBl5RSgqVeBB5hM8yKz4JqgA0MreBKM7NscIv2awNzOotB2ROmHPDRFRE1RUokB6blnweRp6ynp+ykJReo4ciZkFyglApRKgh6ctRndshv5tHGCoryPyHlBTw9NSVWC4ISKqmewnxTh4IwW/Xn6AiPhM5XITmS4G+zhgVMfm6OJmxfvvUKNguKkCww0RUe3FP8rH7itJ2H31ARIf/30aq7mlIUb5NcPIjs3hbmMsYoWk7RhuqsBwQ0RUdwqFgIj4TPx6+QEO3EhB3l/33wGATq6WGNWxGYb6OMHcSE/EKkkbMdxUgeGGiKh+PCkqRVhkGn69/ABnYh5C8de3ib6uFP287TGqYzM818qWk4BSvWC4qQLDDRFR/UvPKcTea0n49XISotJylcttTPTxom8zjOrYDG2dzHhZOdUZw00VGG6IiBqOIAi4nZKDXy8n4bdrSXiUX6R8zcvBFKM6NsOIDs1gx8vKqZYYbqrAcENE1DiKSxU4Hf0Qu68kIex2mvLmglIJ0NPTFkPaO6KVvSncrY05RoeqxXBTBYYbIqLGl11QjP03krH7ShIu/+Oy8qesjPXhZm0EdxsTuNsYwc3GGO42xnCzNoaxjPebJYabKjHcEBGJKy4jH3uuPMCFuMe4n5GP9Fx5le3tzWRwsy4LO+42xnCzMUYLG2M4WxnBQI83E2wqGG6qwHBDRKRe8uQluJ+Rj/uP8hH3MB9xj/JxPyMfcRn5yCwornQ9iQRwMjdEC9uyHp6nocfNxhjNLQ15lZaWYbipAsMNEZHmyC4oRtyjfMRl5CEuo0AZeu5n5CP3H/fY+TddqQQdXSzxWk939PW2hw7voqzxGG6qwHBDRKT5BEHAo/wixP0Vdp4Gnri/eoAKi/+eGd3dxhiv9XDHS52a8zSWBmO4qQLDDRGRdlMoBDzIfIKfIhKw7Xw8cgrLenisjPUxJcAVk7u5wtpEJnKVVFsMN1VguCEiajry5SX4OSIR352Nw4PMsjmxZLpSvNSpOV7r4Y4WtiYiV0g1xXBTBYYbIqKmp6RUgcO3UrHhdCz+fJANoGxAcj9ve8x4rgX83axErpCqw3BTBYYbIqKmSxAEXIx7jI1nYnE0Ml253M/FAjN6tkD/tg4cfKymGG6qwHBDREQAcDc9F9+djcOvV5JQVFI2ANnV2giv93DHS52cYajPwcfqhOGmCgw3RET0Tw9z5fj+/H1suxCPrL/uq2NppIfJ3VwxOcANtqYcfKwOGG6qwHBDREQVKSgqwS+XH+DbM3FIeFwAANDXlWJ0x2Z4rUcLtLTj4GMxMdxUgeGGiIiqUqoQcORWKr45HYtriVnK5X297TDjOQ90drOERMJxOY2N4aYKDDdERFQTgiAgIj4TG07H4mhkGp5+W/o6W2B6T3d4OZj+q30F2yi3zYralF9ooKsDV2sjhqh/YLipAsMNERHV1r2HeWWDjy8/gLxEUf0K9cDV2ghDfBwxtL0TvB1Nm3zQYbipAsMNERHVVUaeHNvOx+OXyw9QUFR+bquKAsi/l1ScUVQX5hQWK6/gAoAWtsYY2t4Jw9o7wtPe9N8rNwkMN1VguCEiInVXUFSCY5Hp2P9nMk5EPVQJOq3tTTG0vSOG+jrB3cZYxCobF8NNFRhuiIhIk+QWFuNoZBr2X0/B6ZiHKC79+2u7rZMZhrZ3wtD2jnC2MhKxyr/n9IpMzUFhcSmGd2hWr9tnuKkCww0REWmq7IJihN5Oxf4/UxB+NwOlir+/wn2bm2NoeycMae8IJwvDBq/jTmoOotJyEZmSizupOYhOzUV+USkAoJmFIcIXv1Cv76kx4WbZsmVYvny5yrLWrVvjzp07la6zbt06hISEICEhATY2NnjppZcQHBwMAwODGr0nww0REWmDx/lFOHwzFfv/TMaF2Ef4R85BJ1dLDG3viCE+jrAzq9n3Y0WKSxWIy8hHZEoO7qTmIio1F3dScpCcXVhhe30dKTztTeDlYIbVo32gqyOt83v/W22+v3Xr7V3rqG3btjh69Kjyua5u5SXt2LEDixcvxqZNm9C9e3dER0dj2rRpkEgkWLt2bWOUS0REpBasjPUxoasLJnR1wcNcOQ7dTMH+P1Nw6f5jXI7PxOX4TKzYfxtd3Kww1NcJg9o5wMak4rstC4KAh7ly3Ekt64W5k5KLyNRc3EvPQ1FpxVeHNbMwhJeDKbwcTeHlYAYvB1O42xjXa6CpK9HDja6uLhwcHGrU9ty5cwgMDMSECRMAAG5ubhg/fjz++OOPhiyRiIhIrdmayjAlwA1TAtyQml2IgzdSsP/PZFxJyMIfcY/xR9xjLP3tJrp72GBoe0e0djBFTFoeIlNzynpjUnPxOL+owm2byHTR2sG0LMg4mMLL0Qyt7E1hbqjXyHtZc6KHm5iYGDg5OcHAwAABAQEIDg6Gi4tLhW27d++O7du34+LFi+jSpQtiY2Nx8OBBTJ48udLty+VyyOVy5fOcnJx63wciIiJ14WBugFd7uOPVHu54kFnwV9BJwZ8PsnH2bgbO3s2ocD2pBHC3MVb2wng5lv23mYUhpBo2U7qoY24OHTqEvLw8tG7dGikpKVi+fDmSkpJw8+ZNmJpWfB3/559/joULF0IQBJSUlGDWrFkICQmp9D0qGtcDgGNuiIioSYl/lI/9f6bgwJ8pSM+Vo7VD2diY1g6m8HYwg6e9CQz01HcmdI0ZUPxvWVlZcHV1xdq1a/Haa6+Ve/3kyZN4+eWX8eGHH6Jr1664e/cu3nzzTUyfPh3vv/9+hdusqOfG2dmZ4YaIiEiDaNSA4n+ysLBAq1atcPfu3Qpff//99zF58mS8/vrrAAAfHx/k5+djxowZePfddyGVlh/EJJPJIJNxunoiIqKmQvwhzf+Ql5eHe/fuwdHRscLXCwoKygUYHZ2yLjQ16oAiIiIiEYkabhYuXIhTp07h/v37OHfuHEaOHAkdHR2MHz8eADBlyhQsWbJE2X7YsGEICQnBjz/+iLi4OISFheH999/HsGHDlCGHiIiImjZRT0s9ePAA48ePx6NHj2Bra4sePXrgwoULsLW1BQAkJCSo9NS89957kEgkeO+995CUlARbW1sMGzYM//3vf8XaBSIiIlIzajWguDHwDsVERESapzbf32o15oaIiIjoWTHcEBERkVZhuCEiIiKtwnBDREREWoXhhoiIiLQKww0RERFpFYYbIiIi0ioMN0RERKRVGG6IiIhIq6jVrOCN4ekNmXNyckSuhIiIiGrq6fd2TSZWaHLhJjc3FwDg7OwsciVERERUW7m5uTA3N6+yTZObW0qhUCA5ORmmpqaQSCRil9OgcnJy4OzsjMTERK2fR4v7qr2a0v5yX7VXU9rfhtpXQRCQm5sLJycnlUm1K9Lkem6kUimaN28udhmNyszMTOs/TE9xX7VXU9pf7qv2akr72xD7Wl2PzVMcUExERERaheGGiIiItArDjRaTyWRYunQpZDKZ2KU0OO6r9mpK+8t91V5NaX/VYV+b3IBiIiIi0m7suSEiIiKtwnBDREREWoXhhoiIiLQKww0RERFpFYYbDRUcHIzOnTvD1NQUdnZ2GDFiBKKioqpcZ8uWLZBIJCoPAwODRqq47pYtW1aubi8vryrX2bVrF7y8vGBgYAAfHx8cPHiwkap9Nm5ubuX2VSKRICgoqML2mnZMT58+jWHDhsHJyQkSiQR79+5VeV0QBHzwwQdwdHSEoaEh+vbti5iYmGq3+9VXX8HNzQ0GBgbo2rUrLl682EB7UHNV7WtxcTEWLVoEHx8fGBsbw8nJCVOmTEFycnKV26zLZ6ExVHdcp02bVq7ugQMHVrtddTyuQPX7W9FnWCKR4OOPP650m+p6bGvyXVNYWIigoCBYW1vDxMQEo0ePRlpaWpXbretnvaYYbjTUqVOnEBQUhAsXLiAsLAzFxcXo378/8vPzq1zPzMwMKSkpykd8fHwjVfxs2rZtq1L32bNnK2177tw5jB8/Hq+99hquXr2KESNGYMSIEbh582YjVlw3ly5dUtnPsLAwAMCYMWMqXUeTjml+fj58fX3x1VdfVfj6Rx99hM8//xzr16/HH3/8AWNjYwwYMACFhYWVbvOnn37CW2+9haVLl+LKlSvw9fXFgAEDkJ6e3lC7USNV7WtBQQGuXLmC999/H1euXMHu3bsRFRWFF198sdrt1uaz0FiqO64AMHDgQJW6d+7cWeU21fW4AtXv7z/3MyUlBZs2bYJEIsHo0aOr3K46HtuafNcsWLAAv//+O3bt2oVTp04hOTkZo0aNqnK7dfms14pAWiE9PV0AIJw6darSNps3bxbMzc0br6h6snTpUsHX17fG7ceOHSsMGTJEZVnXrl2FmTNn1nNlDe/NN98UPDw8BIVCUeHrmnpMBUEQAAh79uxRPlcoFIKDg4Pw8ccfK5dlZWUJMplM2LlzZ6Xb6dKlixAUFKR8XlpaKjg5OQnBwcENUndd/HtfK3Lx4kUBgBAfH19pm9p+FsRQ0b5OnTpVGD58eK22ownHVRBqdmyHDx8uvPDCC1W20YRjKwjlv2uysrIEPT09YdeuXco2kZGRAgDh/PnzFW6jrp/12mDPjZbIzs4GAFhZWVXZLi8vD66urnB2dsbw4cNx69atxijvmcXExMDJyQktWrTAxIkTkZCQUGnb8+fPo2/fvirLBgwYgPPnzzd0mfWqqKgI27dvx6uvvlrlJK+aekz/LS4uDqmpqSrHztzcHF27dq302BUVFeHy5csq60ilUvTt21fjjnd2djYkEgksLCyqbFebz4I6OXnyJOzs7NC6dWvMnj0bjx49qrStNh3XtLQ0HDhwAK+99lq1bTXh2P77u+by5csoLi5WOVZeXl5wcXGp9FjV5bNeWww3WkChUGD+/PkIDAxEu3btKm3XunVrbNq0Cb/99hu2b98OhUKB7t2748GDB41Ybe117doVW7ZsweHDhxESEoK4uDj07NkTubm5FbZPTU2Fvb29yjJ7e3ukpqY2Rrn1Zu/evcjKysK0adMqbaOpx7QiT49PbY5dRkYGSktLNf54FxYWYtGiRRg/fnyVEw3W9rOgLgYOHIjvv/8ex44dw+rVq3Hq1CkMGjQIpaWlFbbXluMKAFu3boWpqWm1p2k04dhW9F2TmpoKfX39cqG8qmNVl896bTW5WcG1UVBQEG7evFnt+dmAgAAEBAQon3fv3h3e3t745ptvsHLlyoYus84GDRqk/Ll9+/bo2rUrXF1d8fPPP9fo/4Y01XfffYdBgwbBycmp0jaaekzpb8XFxRg7diwEQUBISEiVbTX1s/Dyyy8rf/bx8UH79u3h4eGBkydPok+fPiJW1vA2bdqEiRMnVjvQXxOObU2/a9QBe2403BtvvIH9+/fjxIkTaN68ea3W1dPTg5+fH+7evdtA1TUMCwsLtGrVqtK6HRwcyo3UT0tLg4ODQ2OUVy/i4+Nx9OhRvP7667VaT1OPKQDl8anNsbOxsYGOjo7GHu+nwSY+Ph5hYWFV9tpUpLrPgrpq0aIFbGxsKq1b04/rU2fOnEFUVFStP8eA+h3byr5rHBwcUFRUhKysLJX2VR2runzWa4vhRkMJgoA33ngDe/bswfHjx+Hu7l7rbZSWluLGjRtwdHRsgAobTl5eHu7du1dp3QEBATh27JjKsrCwMJUeDnW3efNm2NnZYciQIbVaT1OPKQC4u7vDwcFB5djl5OTgjz/+qPTY6evro1OnTirrKBQKHDt2TO2P99NgExMTg6NHj8La2rrW26jus6CuHjx4gEePHlVatyYf13/67rvv0KlTJ/j6+tZ6XXU5ttV913Tq1Al6enoqxyoqKgoJCQmVHqu6fNbrUjhpoNmzZwvm5ubCyZMnhZSUFOWjoKBA2Wby5MnC4sWLlc+XL18uhIaGCvfu3RMuX74svPzyy4KBgYFw69YtMXahxt5++23h5MmTQlxcnBAeHi707dtXsLGxEdLT0wVBKL+f4eHhgq6urvDJJ58IkZGRwtKlSwU9PT3hxo0bYu1CrZSWlgouLi7CokWLyr2m6cc0NzdXuHr1qnD16lUBgLB27Vrh6tWryiuE/ve//wkWFhbCb7/9Jvz555/C8OHDBXd3d+HJkyfKbbzwwgvCF198oXz+448/CjKZTNiyZYtw+/ZtYcaMGYKFhYWQmpra6Pv3T1Xta1FRkfDiiy8KzZs3F65du6byGZbL5cpt/Htfq/ssiKWqfc3NzRUWLlwonD9/XoiLixOOHj0qdOzYUfD09BQKCwuV29CU4yoI1f8dC4IgZGdnC0ZGRkJISEiF29CUY1uT75pZs2YJLi4uwvHjx4WIiAghICBACAgIUNlO69athd27dyuf1+Sz/iwYbjQUgAofmzdvVrbp1auXMHXqVOXz+fPnCy4uLoK+vr5gb28vDB48WLhy5UrjF19L48aNExwdHQV9fX2hWbNmwrhx44S7d+8qX//3fgqCIPz8889Cq1atBH19faFt27bCgQMHGrnqugsNDRUACFFRUeVe0/RjeuLEiQr/bp/uk0KhEN5//33B3t5ekMlkQp8+fcr9HlxdXYWlS5eqLPviiy+Uv4cuXboIFy5caKQ9qlxV+xoXF1fpZ/jEiRPKbfx7X6v7LIilqn0tKCgQ+vfvL9ja2gp6enqCq6urMH369HIhRVOOqyBU/3csCILwzTffCIaGhkJWVlaF29CUY1uT75onT54Ic+bMESwtLQUjIyNh5MiRQkpKSrnt/HOdmnzWn4XkrzclIiIi0gocc0NERERaheGGiIiItArDDREREWkVhhsiIiLSKgw3REREpFUYboiIiEirMNwQERGRVmG4ISICIJFIsHfvXrHLIKJ6wHBDRKKbNm0aJBJJucfAgQPFLo2INJCu2AUQEQHAwIEDsXnzZpVlMplMpGqISJOx54aI1IJMJoODg4PKw9LSEkDZKaOQkBAMGjQIhoaGaNGiBX755ReV9W/cuIEXXngBhoaGsLa2xowZM5CXl6fSZtOmTWjbti1kMhkcHR3xxhtvqLyekZGBkSNHwsjICJ6enti3b1/D7jQRNQiGGyLSCO+//z5Gjx6N69evY+LEiXj55ZcRGRkJAMjPz8eAAQNgaWmJS5cuYdeuXTh69KhKeAkJCUFQUBBmzJiBGzduYN++fWjZsqXKeyxfvhxjx47Fn3/+icGDB2PixIl4/Phxo+4nEdWDepuCk4iojqZOnSro6OgIxsbGKo///ve/giCUzSg8a9YslXW6du0qzJ49WxAEQdiwYYNgaWkp5OXlKV8/cOCAIJVKlbNPOzk5Ce+++26lNQAQ3nvvPeXzvLw8AYBw6NChettPImocHHNDRGqhd+/eCAkJUVlmZWWl/DkgIEDltYCAAFy7dg0AEBkZCV9fXxgbGytfDwwMhEKhQFRUFCQSCZKTk9GnT58qa2jfvr3yZ2NjY5iZmSE9Pb2uu0REImG4ISK1YGxsXO40UX0xNDSsUTs9PT2V5xKJBAqFoiFKIqIGxDE3RKQRLly4UO65t7c3AMDb2xvXr19Hfn6+8vXw8HBIpVK0bt0apqamcHNzw7Fjxxq1ZiISB3tuiEgtyOVypKamqizT1dWFjY0NAGDXrl3w9/dHjx498MMPP+DixYv47rvvAAATJ07E0qVLMXXqVCxbtgwPHz7E3LlzMXnyZNjb2wMAli1bhlmzZsHOzg6DBg1Cbm4uwsPDMXfu3MbdUSJqcAw3RKQWDh8+DEdHR5VlrVu3xp07dwCUXcn0448/Ys6cOXB0dMTOnTvRpk0bAICRkRFCQ0Px5ptvonPnzjAyMsLo0aOxdu1a5bamTp2KwsJCfPrpp1i4cCFsbGzw0ksvNd4OElGjkQiCIIhdBBFRVSQSCfbs2YMRI0aIXQoRaQCOuSEiIiKtwnBDREREWoVjbohI7fHsORHVBntuiIiISKsw3BAREZFWYbghIiIircJwQ0RERFqF4YaIiIi0CsMNERERaRWGGyIiItIqDDdERESkVRhuiIiISKv8P5PBXUt/myhOAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the learning curve\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(1, num_epochs + 1), loss_curve, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 512])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho_do_arquivo = 'efficientnet_SwaV_model.pth'\n",
    "torch.save(model, caminho_do_arquivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ImageDataLoader(\"../data/02_data_split/train_data/\")\n",
    "dataloader = DataLoader(data.dataset, batch_size=50, shuffle=False)\n",
    "\n",
    "# select embedding model training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# compute embeddings and save\n",
    "target = []\n",
    "paths = []\n",
    "labels = []\n",
    "feature_embeddings = np.empty((0, 1280))\n",
    "\n",
    "for i, (x, y, path, label) in enumerate(dataloader):\n",
    "    x = x.to(device=device)\n",
    "    with torch.no_grad():\n",
    "        batch_features = model(x)\n",
    "\n",
    "    batch_features_np = batch_features.view(batch_features.size(0), -1).cpu().numpy()\n",
    "    feature_embeddings = np.vstack((feature_embeddings, batch_features_np))\n",
    "    target.extend(list(y.cpu().detach().numpy()))\n",
    "    paths.extend(slice_image_paths(path))\n",
    "    labels.extend(label)\n",
    "\n",
    "\n",
    "data_dict = {\n",
    "    \"model\": 'efficientnet_SwaV',\n",
    "    \"embedding\":feature_embeddings,\n",
    "    \"target\":target,\n",
    "    \"paths\": paths,\n",
    "    \"classes\":labels\n",
    "}\n",
    "\n",
    "with open('./efficientnet_SwaV.pickle', 'wb') as pickle_file:\n",
    "    pickle.dump(data_dict, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "del loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13379"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "renal-pathology-retrieval-P_udDvkW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
