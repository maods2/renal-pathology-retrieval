{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_embeddings(pickle_file_path):\n",
    "    with open(pickle_file_path, 'rb') as pickle_file:\n",
    "        loaded_data_dict = pickle.load(pickle_file)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    data = loaded_data_dict[\"embedding\"]\n",
    "    labels = np.array(loaded_data_dict[\"target\"])\n",
    "    return data, labels, loaded_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "def concat_embeddings(\n",
    "        embedding,\n",
    "        semantic_att,\n",
    "        mode\n",
    "        ):\n",
    "    path_s = semantic_att['paths']\n",
    "    path_e = embedding['paths']\n",
    "\n",
    "    X_s = semantic_att['embedding']\n",
    "    X_e = embedding['embedding']\n",
    "\n",
    "    num_att = X_s.shape[1] + X_e.shape[1]\n",
    "    feature_embeddings = np.empty((0, num_att))\n",
    "\n",
    "\n",
    "    if mode == 'train':\n",
    "        for sem, emb, x_s, x_e in zip(path_s, path_e, X_s, X_e):\n",
    "            if sem != 'train_data/'+emb:\n",
    "                raise Exception(f'images are not the same: {sem} - train_data/{emb}')\n",
    "            \n",
    "            concatenated_array = np.concatenate((x_s, x_e))\n",
    "            feature_embeddings = np.vstack((feature_embeddings, concatenated_array))\n",
    "    else:\n",
    "        for sem, emb, x_s, x_e in zip(path_s, path_e, X_s, X_e):\n",
    "            if sem != emb:\n",
    "                raise Exception(f'images are not the same: {sem} - train_data/{emb}')\n",
    "            \n",
    "            concatenated_array = np.concatenate((x_s, x_e))\n",
    "            feature_embeddings = np.vstack((feature_embeddings, concatenated_array))\n",
    "\n",
    "    model_name = semantic_att['model']+'_'+embedding['model']\n",
    "    data_dict = {\n",
    "        \"model\": model_name,\n",
    "        \"embedding\":feature_embeddings,\n",
    "        \"target\":semantic_att['target'],\n",
    "        \"paths\": semantic_att['paths'],\n",
    "        \"classes\":semantic_att['classes']\n",
    "    }\n",
    "\n",
    "    with open(f'./{model_name}_{mode}.pickle', 'wb') as pickle_file:\n",
    "        pickle.dump(data_dict, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = '../evaluation/efficientnetb0_4096_autoencoder_test.pickle'\n",
    "path_train = '../evaluation/efficientnetb0_4096_autoencoder_train.pickle'\n",
    "_, _, encod_result_test = load_embeddings(path_test)\n",
    "_, _, encod_result_train = load_embeddings(path_train)\n",
    "\n",
    "path_test = '../evaluation/efficientnetb0_4096_pretrained_test.pickle'\n",
    "path_train = '../evaluation/efficientnetb0_4096_pretrained_train.pickle'\n",
    "_, _, triplet_result_test = load_embeddings(path_test)\n",
    "_, _, triplet_result_train = load_embeddings(path_train)\n",
    "\n",
    "path_test = '../evaluation/efficientnet_SwaV_test.pickle'\n",
    "path_train = '../evaluation/efficientnet_SwaV_train.pickle'\n",
    "_, _, swav_result_test = load_embeddings(path_test)\n",
    "_, _, swav_result_train = load_embeddings(path_train)\n",
    "\n",
    "path_test = '../semantic/semantic_test.pickle'\n",
    "path_train = '../semantic/semantic_train.pickle'\n",
    "_, _, semant_result_test = load_embeddings(path_test)\n",
    "_, _, semant_result_train = load_embeddings(path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_embeddings(\n",
    "        embedding=encod_result_train,\n",
    "        semantic_att=semant_result_train,\n",
    "        mode='train'\n",
    "        )\n",
    "concat_embeddings(\n",
    "        embedding=encod_result_test,\n",
    "        semantic_att=semant_result_test,\n",
    "        mode='test'\n",
    "        )\n",
    "\n",
    "concat_embeddings(\n",
    "        embedding=triplet_result_train,\n",
    "        semantic_att=semant_result_train,\n",
    "        mode='train'\n",
    "        )\n",
    "concat_embeddings(\n",
    "        embedding=triplet_result_test,\n",
    "        semantic_att=semant_result_test,\n",
    "        mode='test'\n",
    "        )\n",
    "\n",
    "concat_embeddings(\n",
    "        embedding=swav_result_train,\n",
    "        semantic_att=semant_result_train,\n",
    "        mode='train'\n",
    "        )\n",
    "concat_embeddings(\n",
    "        embedding=swav_result_test,\n",
    "        semantic_att=semant_result_test,\n",
    "        mode='test'\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "renal-pathology-retrieval-P_udDvkW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
